<html><head>
      <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
   <title>Chapter&nbsp;1.&nbsp;Apache HBase Operational Management</title><link rel="stylesheet" type="text/css" href="css/freebsd_docbook.css"><meta name="generator" content="DocBook XSL-NS Stylesheets V1.76.1"></head><body bgcolor="white" text="black" link="#0000FF" vlink="#840084" alink="#0000FF"><script type="text/javascript">
    var disqus_shortname = 'hbase'; // required: replace example with your forum shortname
    var disqus_url = 'http://hbase.apache.org/book/ops_mgt.html';
    </script><div class="chapter" title="Chapter&nbsp;1.&nbsp;Apache HBase Operational Management"><div class="titlepage"><div><div><h2 class="title"><a name="ops_mgt"></a>Chapter&nbsp;1.&nbsp;Apache HBase Operational Management</h2></div></div></div><div class="toc"><p><b>Table of Contents</b></p><dl><dt><span class="section"><a href="#tools">1.1. HBase Tools and Utilities</a></span></dt><dd><dl><dt><span class="section"><a href="#canary">1.1.1. Canary</a></span></dt><dt><span class="section"><a href="#health.check">1.1.2. Health Checker</a></span></dt><dt><span class="section"><a href="#driver">1.1.3. Driver</a></span></dt><dt><span class="section"><a href="#hbck">1.1.4. HBase <span class="application">hbck</span></a></span></dt><dt><span class="section"><a href="#hfile_tool2">1.1.5. HFile Tool</a></span></dt><dt><span class="section"><a href="#wal_tools">1.1.6. WAL Tools</a></span></dt><dt><span class="section"><a href="#compression.tool">1.1.7. Compression Tool</a></span></dt><dt><span class="section"><a href="#copytable">1.1.8. CopyTable</a></span></dt><dt><span class="section"><a href="#export">1.1.9. Export</a></span></dt><dt><span class="section"><a href="#import">1.1.10. Import</a></span></dt><dt><span class="section"><a href="#importtsv">1.1.11. ImportTsv</a></span></dt><dt><span class="section"><a href="#completebulkload">1.1.12. CompleteBulkLoad</a></span></dt><dt><span class="section"><a href="#walplayer">1.1.13. WALPlayer</a></span></dt><dt><span class="section"><a href="#rowcounter">1.1.14. RowCounter and CellCounter</a></span></dt><dt><span class="section"><a href="#mlockall">1.1.15. mlockall</a></span></dt><dt><span class="section"><a href="#compaction.tool">1.1.16. Offline Compaction Tool</a></span></dt><dt><span class="section"><a href="#d839e586">1.1.17. <span class="command"><strong>hbase clean</strong></span></a></span></dt><dt><span class="section"><a href="#d839e603">1.1.18. <span class="command"><strong>hbase pe</strong></span></a></span></dt><dt><span class="section"><a href="#d839e627">1.1.19. <span class="command"><strong>hbase ltt</strong></span></a></span></dt></dl></dd><dt><span class="section"><a href="#ops.regionmgt">1.2. Region Management</a></span></dt><dd><dl><dt><span class="section"><a href="#ops.regionmgt.majorcompact">1.2.1. Major Compaction</a></span></dt><dt><span class="section"><a href="#ops.regionmgt.merge">1.2.2. Merge</a></span></dt></dl></dd><dt><span class="section"><a href="#node.management">1.3. Node Management</a></span></dt><dd><dl><dt><span class="section"><a href="#decommission">1.3.1. Node Decommission</a></span></dt><dt><span class="section"><a href="#rolling">1.3.2. Rolling Restart</a></span></dt><dt><span class="section"><a href="#adding.new.node">1.3.3. Adding a New Node</a></span></dt></dl></dd><dt><span class="section"><a href="#hbase_metrics">1.4. HBase Metrics</a></span></dt><dd><dl><dt><span class="section"><a href="#metric_setup">1.4.1. Metric Setup</a></span></dt><dt><span class="section"><a href="#d839e1035">1.4.2. Disabling Metrics</a></span></dt><dt><span class="section"><a href="#discovering.available.metrics">1.4.3. Discovering Available Metrics</a></span></dt><dt><span class="section"><a href="#d839e1171">1.4.4. Units of Measure for Metrics</a></span></dt><dt><span class="section"><a href="#master_metrics">1.4.5. Most Important Master Metrics</a></span></dt><dt><span class="section"><a href="#rs_metrics">1.4.6. Most Important RegionServer Metrics</a></span></dt></dl></dd><dt><span class="section"><a href="#ops.monitoring">1.5. HBase Monitoring</a></span></dt><dd><dl><dt><span class="section"><a href="#ops.monitoring.overview">1.5.1. Overview</a></span></dt><dt><span class="section"><a href="#ops.slow.query">1.5.2. Slow Query Log</a></span></dt><dt><span class="section"><a href="#d839e1526">1.5.3. Block Cache Monitoring</a></span></dt></dl></dd><dt><span class="section"><a href="#cluster_replication">1.6. Cluster Replication</a></span></dt><dd><dl><dt><span class="section"><a href="#d839e1626">1.6.1. Life of a WAL Edit</a></span></dt><dt><span class="section"><a href="#d839e1716">1.6.2. Replication Internals</a></span></dt><dt><span class="section"><a href="#d839e1795">1.6.3. Replication Configuration Options</a></span></dt><dt><span class="section"><a href="#d839e1913">1.6.4. Replication Implementation Details</a></span></dt></dl></dd><dt><span class="section"><a href="#ops.backup">1.7. HBase Backup</a></span></dt><dd><dl><dt><span class="section"><a href="#ops.backup.fullshutdown">1.7.1. Full Shutdown Backup</a></span></dt><dt><span class="section"><a href="#ops.backup.live.replication">1.7.2. Live Cluster Backup - Replication</a></span></dt><dt><span class="section"><a href="#ops.backup.live.copytable">1.7.3. Live Cluster Backup - CopyTable</a></span></dt><dt><span class="section"><a href="#ops.backup.live.export">1.7.4. Live Cluster Backup - Export</a></span></dt></dl></dd><dt><span class="section"><a href="#ops.snapshots">1.8. HBase Snapshots</a></span></dt><dd><dl><dt><span class="section"><a href="#ops.snapshots.configuration">1.8.1. Configuration</a></span></dt><dt><span class="section"><a href="#ops.snapshots.takeasnapshot">1.8.2. Take a Snapshot</a></span></dt><dt><span class="section"><a href="#ops.snapshots.list">1.8.3. Listing Snapshots</a></span></dt><dt><span class="section"><a href="#ops.snapshots.delete">1.8.4. Deleting Snapshots</a></span></dt><dt><span class="section"><a href="#ops.snapshots.clone">1.8.5. Clone a table from snapshot</a></span></dt><dt><span class="section"><a href="#ops.snapshots.restore">1.8.6. Restore a snapshot</a></span></dt><dt><span class="section"><a href="#ops.snapshots.acls">1.8.7. Snapshots operations and ACLs</a></span></dt><dt><span class="section"><a href="#ops.snapshots.export">1.8.8. Export to another cluster</a></span></dt></dl></dd><dt><span class="section"><a href="#ops.capacity">1.9. Capacity Planning and Region Sizing</a></span></dt><dd><dl><dt><span class="section"><a href="#ops.capacity.nodes">1.9.1. Node count and hardware/VM configuration</a></span></dt><dt><span class="section"><a href="#ops.capacity.regions">1.9.2. Determining region count and size</a></span></dt><dt><span class="section"><a href="#ops.capacity.config">1.9.3. Initial configuration and tuning</a></span></dt></dl></dd><dt><span class="section"><a href="#table.rename">1.10. Table Rename</a></span></dt></dl></div><p> This chapter will cover operational tools and practices required of a running Apache HBase
    cluster. The subject of operations is related to the topics of <a class="xref" href="#">???</a>, <a class="xref" href="#">???</a>, and <a class="xref" href="#">???</a> but is a distinct topic in itself. </p><div class="section" title="1.1.&nbsp;HBase Tools and Utilities"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="tools"></a>1.1.&nbsp;HBase Tools and Utilities</h2></div></div></div><p>HBase provides several tools for administration, analysis, and debugging of your cluster.
      The entry-point to most of these tools is the <code class="filename">bin/hbase</code> command, though
      some tools are available in the <code class="filename">dev-support/</code> directory.</p><p>To see usage instructions for <code class="filename">bin/hbase</code> command, run it with no
      arguments, or with the <code class="option">-h</code> argument. These are the usage instructions for
      HBase 0.98.x. Some commands, such as <span class="command"><strong>version</strong></span>, <span class="command"><strong>pe</strong></span>,
        <span class="command"><strong>ltt</strong></span>, <span class="command"><strong>clean</strong></span>, are not available in previous
      versions.</p><pre class="screen">
$ <strong class="userinput"><code>bin/hbase</code></strong>
Usage: hbase [&lt;options&gt;] &lt;command&gt; [&lt;args&gt;]
Options:
  --config DIR    Configuration direction to use. Default: ./conf
  --hosts HOSTS   Override the list in 'regionservers' file

Commands:
Some commands take arguments. Pass no args or -h for usage.
  shell           Run the HBase shell
  hbck            Run the hbase 'fsck' tool
  hlog            Write-ahead-log analyzer
  hfile           Store file analyzer
  zkcli           Run the ZooKeeper shell
  upgrade         Upgrade hbase
  master          Run an HBase HMaster node
  regionserver    Run an HBase HRegionServer node
  zookeeper       Run a Zookeeper server
  rest            Run an HBase REST server
  thrift          Run the HBase Thrift server
  thrift2         Run the HBase Thrift2 server
  clean           Run the HBase clean up script
  classpath       Dump hbase CLASSPATH
  mapredcp        Dump CLASSPATH entries required by mapreduce
  pe              Run PerformanceEvaluation
  ltt             Run LoadTestTool
  version         Print the version
  CLASSNAME       Run the class named CLASSNAME      
    </pre><p>Some of the tools and utilities below are Java classes which are passed directly to the
        <code class="filename">bin/hbase</code> command, as referred to in the last line of the usage
      instructions. Others, such as <span class="command"><strong>hbase shell</strong></span> (<a class="xref" href="#">???</a>),
        <span class="command"><strong>hbase upgrade</strong></span> (<a class="xref" href="#">???</a>), and <span class="command"><strong>hbase
        thrift</strong></span> (<a class="xref" href="#">???</a>), are documented elsewhere in this guide.</p><div class="section" title="1.1.1.&nbsp;Canary"><div class="titlepage"><div><div><h3 class="title"><a name="canary"></a>1.1.1.&nbsp;Canary</h3></div></div></div><p> There is a Canary class can help users to canary-test the HBase cluster status, with
        every column-family for every regions or regionservers granularity. To see the usage, use
        the <code class="literal">--help</code> parameter. </p><pre class="screen">$ ${HBASE_HOME}/bin/hbase org.apache.hadoop.hbase.tool.Canary -help

Usage: bin/hbase org.apache.hadoop.hbase.tool.Canary [opts] [table1 [table2]...] | [regionserver1 [regionserver2]..]
 where [opts] are:
   -help          Show this help and exit.
   -regionserver  replace the table argument to regionserver,
      which means to enable regionserver mode
   -daemon        Continuous check at defined intervals.
   -interval &lt;N&gt;  Interval between checks (sec)
   -e             Use region/regionserver as regular expression
      which means the region/regionserver is regular expression pattern
   -f &lt;B&gt;         stop whole program if first error occurs, default is true
   -t &lt;N&gt;         timeout for a check, default is 600000 (milliseconds)</pre><p> This tool will return non zero error codes to user for collaborating with other
        monitoring tools, such as Nagios. The error code definitions are: </p><pre class="programlisting">private static final int USAGE_EXIT_CODE = 1;
private static final int INIT_ERROR_EXIT_CODE = 2;
private static final int TIMEOUT_ERROR_EXIT_CODE = 3;
private static final int ERROR_EXIT_CODE = 4;</pre><p> Here are some examples based on the following given case. There are two HTable called
        test-01 and test-02, they have two column family cf1 and cf2 respectively, and deployed on
        the 3 regionservers. see following table. </p><div class="informaltable"><table border="1"><colgroup><col align="center" class="regionserver"><col align="center" class="test-01"><col align="center" class="test-02"></colgroup><thead><tr><th align="center">RegionServer</th><th align="center">test-01</th><th align="center">test-02</th></tr></thead><tbody><tr><td align="center">rs1</td><td align="center">r1</td><td align="center">r2</td></tr><tr><td align="center">rs2</td><td align="center">r2</td><td align="center">&nbsp;</td></tr><tr><td align="center">rs3</td><td align="center">r2</td><td align="center">r1</td></tr></tbody></table></div><p> Following are some examples based on the previous given case. </p><div class="section" title="1.1.1.1.&nbsp;Canary test for every column family (store) of every region of every table"><div class="titlepage"><div><div><h4 class="title"><a name="d839e121"></a>1.1.1.1.&nbsp;Canary test for every column family (store) of every region of every table</h4></div></div></div><pre class="screen">$ ${HBASE_HOME}/bin/hbase org.apache.hadoop.hbase.tool.Canary
            
3/12/09 03:26:32 INFO tool.Canary: read from region test-01,,1386230156732.0e3c7d77ffb6361ea1b996ac1042ca9a. column family cf1 in 2ms
13/12/09 03:26:32 INFO tool.Canary: read from region test-01,,1386230156732.0e3c7d77ffb6361ea1b996ac1042ca9a. column family cf2 in 2ms
13/12/09 03:26:32 INFO tool.Canary: read from region test-01,0004883,1386230156732.87b55e03dfeade00f441125159f8ca87. column family cf1 in 4ms
13/12/09 03:26:32 INFO tool.Canary: read from region test-01,0004883,1386230156732.87b55e03dfeade00f441125159f8ca87. column family cf2 in 1ms
...
13/12/09 03:26:32 INFO tool.Canary: read from region test-02,,1386559511167.aa2951a86289281beee480f107bb36ee. column family cf1 in 5ms
13/12/09 03:26:32 INFO tool.Canary: read from region test-02,,1386559511167.aa2951a86289281beee480f107bb36ee. column family cf2 in 3ms
13/12/09 03:26:32 INFO tool.Canary: read from region test-02,0004883,1386559511167.cbda32d5e2e276520712d84eaaa29d84. column family cf1 in 31ms
13/12/09 03:26:32 INFO tool.Canary: read from region test-02,0004883,1386559511167.cbda32d5e2e276520712d84eaaa29d84. column family cf2 in 8ms
</pre><p> So you can see, table test-01 has two regions and two column families, so the Canary
          tool will pick 4 small piece of data from 4 (2 region * 2 store) different stores. This is
          a default behavior of the this tool does. </p></div><div class="section" title="1.1.1.2.&nbsp;Canary test for every column family (store) of every region of specific table(s)"><div class="titlepage"><div><div><h4 class="title"><a name="d839e128"></a>1.1.1.2.&nbsp;Canary test for every column family (store) of every region of specific
          table(s)</h4></div></div></div><p> You can also test one or more specific tables.</p><pre class="screen">$ ${HBASE_HOME}/bin/hbase orghapache.hadoop.hbase.tool.Canary test-01 test-02</pre></div><div class="section" title="1.1.1.3.&nbsp;Canary test with regionserver granularity"><div class="titlepage"><div><div><h4 class="title"><a name="d839e135"></a>1.1.1.3.&nbsp;Canary test with regionserver granularity</h4></div></div></div><p> This will pick one small piece of data from each regionserver, and can also put your
          resionserver name as input options for canary-test specific regionservers.</p><pre class="screen">$ ${HBASE_HOME}/bin/hbase org.apache.hadoop.hbase.tool.Canary -regionserver
            
13/12/09 06:05:17 INFO tool.Canary: Read from table:test-01 on region server:rs2 in 72ms
13/12/09 06:05:17 INFO tool.Canary: Read from table:test-02 on region server:rs3 in 34ms
13/12/09 06:05:17 INFO tool.Canary: Read from table:test-01 on region server:rs1 in 56ms</pre></div><div class="section" title="1.1.1.4.&nbsp;Canary test with regular expression pattern"><div class="titlepage"><div><div><h4 class="title"><a name="d839e142"></a>1.1.1.4.&nbsp;Canary test with regular expression pattern</h4></div></div></div><p> This will test both table test-01 and test-02.</p><pre class="screen">$ ${HBASE_HOME}/bin/hbase orghapache.hadoop.hbase.tool.Canary -e test-0[1-2]</pre></div><div class="section" title="1.1.1.5.&nbsp;Run canary test as daemon mode"><div class="titlepage"><div><div><h4 class="title"><a name="d839e149"></a>1.1.1.5.&nbsp;Run canary test as daemon mode</h4></div></div></div><p> Run repeatedly with interval defined in option -interval whose default value is 6
          seconds. This daemon will stop itself and return non-zero error code if any error occurs,
          due to the default value of option -f is true.</p><pre class="screen">$ ${HBASE_HOME}/bin/hbase orghapache.hadoop.hbase.tool.Canary -daemon</pre><p>Run repeatedly with internal 5 seconds and will not stop itself even error occurs in
          the test.</p><pre class="screen">$ ${HBASE_HOME}/bin/hbase orghapache.hadoop.hbase.tool.Canary -daemon -interval 50000 -f false</pre></div><div class="section" title="1.1.1.6.&nbsp;Force timeout if canary test stuck"><div class="titlepage"><div><div><h4 class="title"><a name="d839e160"></a>1.1.1.6.&nbsp;Force timeout if canary test stuck</h4></div></div></div><p>In some cases, we suffered the request stucked on the regionserver and not response
          back to the client. The regionserver in problem, would also not indicated to be dead by
          Master, which would bring the clients hung. So we provide the timeout option to kill the
          canary test forcefully and return non-zero error code as well. This run sets the timeout
          value to 60 seconds, the default value is 600 seconds.</p><pre class="screen">$ ${HBASE_HOME}/bin/hbase orghapache.hadoop.hbase.tool.Canary -t 600000</pre></div></div><div class="section" title="1.1.2.&nbsp;Health Checker"><div class="titlepage"><div><div><h3 class="title"><a name="health.check"></a>1.1.2.&nbsp;Health Checker</h3></div></div></div><p>You can configure HBase to run a script on a period and if it fails N times
        (configurable), have the server exit. See <a class="link" href="" target="_top">HBASE-7351 Periodic health check script</a> for configurations and
        detail. </p></div><div class="section" title="1.1.3.&nbsp;Driver"><div class="titlepage"><div><div><h3 class="title"><a name="driver"></a>1.1.3.&nbsp;Driver</h3></div></div></div><p>Several frequently-accessed utilities are provided as <code class="code">Driver</code> classes, and executed by
        the <code class="filename">bin/hbase</code> command. These utilities represent MapReduce jobs which
        run on your cluster. They are run in the following way, replacing
          <em class="replaceable"><code>UtilityName</code></em> with the utility you want to run. This command
        assumes you have set the environment variable <code class="literal">HBASE_HOME</code> to the directory
        where HBase is unpacked on your server.</p><pre class="screen">
${HBASE_HOME}/bin/hbase org.apache.hadoop.hbase.mapreduce.<em class="replaceable"><code>UtilityName</code></em>        
      </pre><p>The following utilities are available:</p><div class="variablelist"><dl><dt><span class="term"><span class="command"><strong>LoadIncrementalHFiles</strong></span></span></dt><dd><p>Complete a bulk data load.</p></dd><dt><span class="term"><span class="command"><strong>CopyTable</strong></span></span></dt><dd><p>Export a table from the local cluster to a peer cluster.</p></dd><dt><span class="term"><span class="command"><strong>Export</strong></span></span></dt><dd><p>Write table data to HDFS.</p></dd><dt><span class="term"><span class="command"><strong>Import</strong></span></span></dt><dd><p>Import data written by a previous <span class="command"><strong>Export</strong></span> operation.</p></dd><dt><span class="term"><span class="command"><strong>ImportTsv</strong></span></span></dt><dd><p>Import data in TSV format.</p></dd><dt><span class="term"><span class="command"><strong>RowCounter</strong></span></span></dt><dd><p>Count rows in an HBase table.</p></dd><dt><span class="term"><span class="command"><strong>replication.VerifyReplication</strong></span></span></dt><dd><p>Compare the data from tables in two different clusters. WARNING: It
            doesn't work for incrementColumnValues'd cells since the timestamp is changed. Note that
          this command is in a different package than the others.</p></dd></dl></div><p>Each command except <span class="command"><strong>RowCounter</strong></span> accepts a single
        <code class="literal">--help</code> argument to print usage instructions.</p></div><div class="section" title="1.1.4.&nbsp;HBase hbck"><div class="titlepage"><div><div><h3 class="title"><a name="hbck"></a>1.1.4.&nbsp;HBase <span class="application">hbck</span></h3></div><div><h4 class="subtitle">An <span class="command"><strong>fsck</strong></span> for your HBase install</h4></div></div></div><p>To run <span class="application">hbck</span> against your HBase cluster run <span class="command"><strong>$
          ./bin/hbase hbck</strong></span> At the end of the command's output it prints
          <code class="literal">OK</code> or <code class="literal">INCONSISTENCY</code>. If your cluster reports
        inconsistencies, pass <span class="command"><strong>-details</strong></span> to see more detail emitted. If
        inconsistencies, run <span class="command"><strong>hbck</strong></span> a few times because the inconsistency may be
        transient (e.g. cluster is starting up or a region is splitting). Passing
          <span class="command"><strong>-fix</strong></span> may correct the inconsistency (This latter is an experimental
        feature). </p><p>For more information, see <a class="xref" href="#">???</a>. </p></div><div class="section" title="1.1.5.&nbsp;HFile Tool"><div class="titlepage"><div><div><h3 class="title"><a name="hfile_tool2"></a>1.1.5.&nbsp;HFile Tool</h3></div></div></div><p>See <a class="xref" href="#">???</a>.</p></div><div class="section" title="1.1.6.&nbsp;WAL Tools"><div class="titlepage"><div><div><h3 class="title"><a name="wal_tools"></a>1.1.6.&nbsp;WAL Tools</h3></div></div></div><div class="section" title="1.1.6.1.&nbsp;FSHLog tool"><div class="titlepage"><div><div><h4 class="title"><a name="hlog_tool"></a>1.1.6.1.&nbsp;<code class="classname">FSHLog</code> tool</h4></div></div></div><p>The main method on <code class="classname">FSHLog</code> offers manual split and dump
          facilities. Pass it WALs or the product of a split, the content of the
            <code class="filename">recovered.edits</code>. directory.</p><p>You can get a textual dump of a WAL file content by doing the following:</p><pre class="screen"> $ ./bin/hbase org.apache.hadoop.hbase.regionserver.wal.FSHLog --dump hdfs://example.org:8020/hbase/.logs/example.org,60020,1283516293161/10.10.21.10%3A60020.1283973724012 </pre><p>The return code will be non-zero if issues with the file so you can test wholesomeness
          of file by redirecting <code class="varname">STDOUT</code> to <code class="code">/dev/null</code> and testing the
          program return.</p><p>Similarly you can force a split of a log file directory by doing:</p><pre class="screen"> $ ./bin/hbase org.apache.hadoop.hbase.regionserver.wal.FSHLog --split hdfs://example.org:8020/hbase/.logs/example.org,60020,1283516293161/</pre><div class="section" title="1.1.6.1.1.&nbsp;HLogPrettyPrinter"><div class="titlepage"><div><div><h5 class="title"><a name="hlog_tool.prettyprint"></a>1.1.6.1.1.&nbsp;<code class="classname">HLogPrettyPrinter</code></h5></div></div></div><p><code class="classname">HLogPrettyPrinter</code> is a tool with configurable options to
            print the contents of an HLog. </p></div></div></div><div class="section" title="1.1.7.&nbsp;Compression Tool"><div class="titlepage"><div><div><h3 class="title"><a name="compression.tool"></a>1.1.7.&nbsp;Compression Tool</h3></div></div></div><p>See <a class="xref" href="#">???</a>.</p></div><div class="section" title="1.1.8.&nbsp;CopyTable"><div class="titlepage"><div><div><h3 class="title"><a name="copytable"></a>1.1.8.&nbsp;CopyTable</h3></div></div></div><p> CopyTable is a utility that can copy part or of all of a table, either to the same
        cluster or another cluster. The target table must first exist. The usage is as
        follows:</p><pre class="screen">
$ <strong class="userinput"><code>./bin/hbase org.apache.hadoop.hbase.mapreduce.CopyTable --help </code></strong>       
/bin/hbase org.apache.hadoop.hbase.mapreduce.CopyTable --help
Usage: CopyTable [general options] [--starttime=X] [--endtime=Y] [--new.name=NEW] [--peer.adr=ADR] &lt;tablename&gt;

Options:
 rs.class     hbase.regionserver.class of the peer cluster, 
              specify if different from current cluster
 rs.impl      hbase.regionserver.impl of the peer cluster,
 startrow     the start row
 stoprow      the stop row
 starttime    beginning of the time range (unixtime in millis)
              without endtime means from starttime to forever
 endtime      end of the time range.  Ignored if no starttime specified.
 versions     number of cell versions to copy
 new.name     new table's name
 peer.adr     Address of the peer cluster given in the format
              hbase.zookeeer.quorum:hbase.zookeeper.client.port:zookeeper.znode.parent
 families     comma-separated list of families to copy
              To copy from cf1 to cf2, give sourceCfName:destCfName.
              To keep the same name, just give "cfName"
 all.cells    also copy delete markers and deleted cells

Args:
 tablename    Name of the table to copy

Examples:
 To copy 'TestTable' to a cluster that uses replication for a 1 hour window:
 $ bin/hbase org.apache.hadoop.hbase.mapreduce.CopyTable --starttime=1265875194289 --endtime=1265878794289 --peer.adr=server1,server2,server3:2181:/hbase --families=myOldCf:myNewCf,cf2,cf3 TestTable

For performance consider the following general options:
  It is recommended that you set the following to &gt;=100. A higher value uses more memory but
  decreases the round trip time to the server and may increase performance.
    -Dhbase.client.scanner.caching=100
  The following should always be set to false, to prevent writing data twice, which may produce
  inaccurate results.
    -Dmapred.map.tasks.speculative.execution=false       
      </pre><div class="note" title="Scanner Caching" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title">Scanner Caching</h3><p>Caching for the input Scan is configured via <code class="code">hbase.client.scanner.caching</code>
          in the job configuration. </p></div><div class="note" title="Versions" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title">Versions</h3><p>By default, CopyTable utility only copies the latest version of row cells unless
            <code class="code">--versions=n</code> is explicitly specified in the command. </p></div><p> See Jonathan Hsieh's <a class="link" href="http://www.cloudera.com/blog/2012/06/online-hbase-backups-with-copytable-2/" target="_top">Online
          HBase Backups with CopyTable</a> blog post for more on <span class="command"><strong>CopyTable</strong></span>.
      </p></div><div class="section" title="1.1.9.&nbsp;Export"><div class="titlepage"><div><div><h3 class="title"><a name="export"></a>1.1.9.&nbsp;Export</h3></div></div></div><p>Export is a utility that will dump the contents of table to HDFS in a sequence file.
        Invoke via:</p><pre class="screen">$ bin/hbase org.apache.hadoop.hbase.mapreduce.Export &lt;tablename&gt; &lt;outputdir&gt; [&lt;versions&gt; [&lt;starttime&gt; [&lt;endtime&gt;]]]
</pre><p>Note: caching for the input Scan is configured via
          <code class="code">hbase.client.scanner.caching</code> in the job configuration. </p></div><div class="section" title="1.1.10.&nbsp;Import"><div class="titlepage"><div><div><h3 class="title"><a name="import"></a>1.1.10.&nbsp;Import</h3></div></div></div><p>Import is a utility that will load data that has been exported back into HBase. Invoke
        via:</p><pre class="screen">$ bin/hbase org.apache.hadoop.hbase.mapreduce.Import &lt;tablename&gt; &lt;inputdir&gt;
</pre><p>To import 0.94 exported files in a 0.96 cluster or onwards, you need to set system
        property "hbase.import.version" when running the import command as below:</p><pre class="screen">$ bin/hbase -Dhbase.import.version=0.94 org.apache.hadoop.hbase.mapreduce.Import &lt;tablename&gt; &lt;inputdir&gt;
</pre></div><div class="section" title="1.1.11.&nbsp;ImportTsv"><div class="titlepage"><div><div><h3 class="title"><a name="importtsv"></a>1.1.11.&nbsp;ImportTsv</h3></div></div></div><p>ImportTsv is a utility that will load data in TSV format into HBase. It has two distinct
        usages: loading data from TSV format in HDFS into HBase via Puts, and preparing StoreFiles
        to be loaded via the <code class="code">completebulkload</code>. </p><p>To load data via Puts (i.e., non-bulk loading):</p><pre class="screen">$ bin/hbase org.apache.hadoop.hbase.mapreduce.ImportTsv -Dimporttsv.columns=a,b,c &lt;tablename&gt; &lt;hdfs-inputdir&gt;
</pre><p>To generate StoreFiles for bulk-loading:</p><pre class="programlisting">$ bin/hbase org.apache.hadoop.hbase.mapreduce.ImportTsv -Dimporttsv.columns=a,b,c -Dimporttsv.bulk.output=hdfs://storefile-outputdir &lt;tablename&gt; &lt;hdfs-data-inputdir&gt;
</pre><p>These generated StoreFiles can be loaded into HBase via <a class="xref" href="#completebulkload" title="1.1.12.&nbsp;CompleteBulkLoad">Section&nbsp;1.1.12, &#8220;CompleteBulkLoad&#8221;</a>. </p><div class="section" title="1.1.11.1.&nbsp;ImportTsv Options"><div class="titlepage"><div><div><h4 class="title"><a name="importtsv.options"></a>1.1.11.1.&nbsp;ImportTsv Options</h4></div></div></div><p>Running <span class="command"><strong>ImportTsv</strong></span> with no arguments prints brief usage
          information:</p><pre class="screen">
Usage: importtsv -Dimporttsv.columns=a,b,c &lt;tablename&gt; &lt;inputdir&gt;

Imports the given input directory of TSV data into the specified table.

The column names of the TSV data must be specified using the -Dimporttsv.columns
option. This option takes the form of comma-separated column names, where each
column name is either a simple column family, or a columnfamily:qualifier. The special
column name HBASE_ROW_KEY is used to designate that this column should be used
as the row key for each imported record. You must specify exactly one column
to be the row key, and you must specify a column name for every column that exists in the
input data.

By default importtsv will load data directly into HBase. To instead generate
HFiles of data to prepare for a bulk data load, pass the option:
  -Dimporttsv.bulk.output=/path/for/output
  Note: the target table will be created with default column family descriptors if it does not already exist.

Other options that may be specified with -D include:
  -Dimporttsv.skip.bad.lines=false - fail if encountering an invalid line
  '-Dimporttsv.separator=|' - eg separate on pipes instead of tabs
  -Dimporttsv.timestamp=currentTimeAsLong - use the specified timestamp for the import
  -Dimporttsv.mapper.class=my.Mapper - A user-defined Mapper to use instead of org.apache.hadoop.hbase.mapreduce.TsvImporterMapper
        </pre></div><div class="section" title="1.1.11.2.&nbsp;ImportTsv Example"><div class="titlepage"><div><div><h4 class="title"><a name="importtsv.example"></a>1.1.11.2.&nbsp;ImportTsv Example</h4></div></div></div><p>For example, assume that we are loading data into a table called 'datatsv' with a
          ColumnFamily called 'd' with two columns "c1" and "c2". </p><p>Assume that an input file exists as follows:
          </p><pre class="screen">
row1	c1	c2
row2	c1	c2
row3	c1	c2
row4	c1	c2
row5	c1	c2
row6	c1	c2
row7	c1	c2
row8	c1	c2
row9	c1	c2
row10	c1	c2
          </pre><p>
        </p><p>For ImportTsv to use this imput file, the command line needs to look like this:</p><pre class="screen">
 HADOOP_CLASSPATH=`${HBASE_HOME}/bin/hbase classpath` ${HADOOP_HOME}/bin/hadoop jar ${HBASE_HOME}/hbase-server-VERSION.jar importtsv -Dimporttsv.columns=HBASE_ROW_KEY,d:c1,d:c2 -Dimporttsv.bulk.output=hdfs://storefileoutput datatsv hdfs://inputfile
 </pre><p> ... and in this example the first column is the rowkey, which is why the
          HBASE_ROW_KEY is used. The second and third columns in the file will be imported as "d:c1"
          and "d:c2", respectively. </p></div><div class="section" title="1.1.11.3.&nbsp;ImportTsv Warning"><div class="titlepage"><div><div><h4 class="title"><a name="importtsv.warning"></a>1.1.11.3.&nbsp;ImportTsv Warning</h4></div></div></div><p>If you have preparing a lot of data for bulk loading, make sure the target HBase table
          is pre-split appropriately. </p></div><div class="section" title="1.1.11.4.&nbsp;See Also"><div class="titlepage"><div><div><h4 class="title"><a name="importtsv.also"></a>1.1.11.4.&nbsp;See Also</h4></div></div></div><p>For more information about bulk-loading HFiles into HBase, see <a class="xref" href="#">???</a></p></div></div><div class="section" title="1.1.12.&nbsp;CompleteBulkLoad"><div class="titlepage"><div><div><h3 class="title"><a name="completebulkload"></a>1.1.12.&nbsp;CompleteBulkLoad</h3></div></div></div><p>The <code class="code">completebulkload</code> utility will move generated StoreFiles into an HBase
        table. This utility is often used in conjunction with output from <a class="xref" href="#importtsv" title="1.1.11.&nbsp;ImportTsv">Section&nbsp;1.1.11, &#8220;ImportTsv&#8221;</a>. </p><p>There are two ways to invoke this utility, with explicit classname and via the
        driver:</p><pre class="screen">$ bin/hbase org.apache.hadoop.hbase.mapreduce.LoadIncrementalHFiles &lt;hdfs://storefileoutput&gt; &lt;tablename&gt;
</pre><p> .. and via the Driver..</p><pre class="screen">HADOOP_CLASSPATH=`${HBASE_HOME}/bin/hbase classpath` ${HADOOP_HOME}/bin/hadoop jar ${HBASE_HOME}/hbase-server-VERSION.jar completebulkload &lt;hdfs://storefileoutput&gt; &lt;tablename&gt;
</pre><div class="section" title="1.1.12.1.&nbsp;CompleteBulkLoad Warning"><div class="titlepage"><div><div><h4 class="title"><a name="completebulkload.warning"></a>1.1.12.1.&nbsp;CompleteBulkLoad Warning</h4></div></div></div><p>Data generated via MapReduce is often created with file permissions that are not
          compatible with the running HBase process. Assuming you're running HDFS with permissions
          enabled, those permissions will need to be updated before you run CompleteBulkLoad.</p><p>For more information about bulk-loading HFiles into HBase, see <a class="xref" href="#">???</a>. </p></div></div><div class="section" title="1.1.13.&nbsp;WALPlayer"><div class="titlepage"><div><div><h3 class="title"><a name="walplayer"></a>1.1.13.&nbsp;WALPlayer</h3></div></div></div><p>WALPlayer is a utility to replay WAL files into HBase. </p><p>The WAL can be replayed for a set of tables or all tables, and a timerange can be
        provided (in milliseconds). The WAL is filtered to this set of tables. The output can
        optionally be mapped to another set of tables. </p><p>WALPlayer can also generate HFiles for later bulk importing, in that case only a single
        table and no mapping can be specified. </p><p>Invoke via:</p><pre class="screen">$ bin/hbase org.apache.hadoop.hbase.mapreduce.WALPlayer [options] &lt;wal inputdir&gt; &lt;tables&gt; [&lt;tableMappings&gt;]&gt;
</pre><p>For example:</p><pre class="screen">$ bin/hbase org.apache.hadoop.hbase.mapreduce.WALPlayer /backuplogdir oldTable1,oldTable2 newTable1,newTable2
</pre><p> WALPlayer, by default, runs as a mapreduce job. To NOT run WALPlayer as a mapreduce job
        on your cluster, force it to run all in the local process by adding the flags
          <code class="code">-Dmapreduce.jobtracker.address=local</code> on the command line. </p></div><div class="section" title="1.1.14.&nbsp;RowCounter and CellCounter"><div class="titlepage"><div><div><h3 class="title"><a name="rowcounter"></a>1.1.14.&nbsp;RowCounter and CellCounter</h3></div></div></div><p><a class="link" href="http://hbase.apache.org/apidocs/org/apache/hadoop/hbase/mapreduce/RowCounter.html" target="_top">RowCounter</a>
        is a mapreduce job to count all the rows of a table. This is a good utility to use as a
        sanity check to ensure that HBase can read all the blocks of a table if there are any
        concerns of metadata inconsistency. It will run the mapreduce all in a single process but it
        will run faster if you have a MapReduce cluster in place for it to exploit.</p><pre class="screen">$ bin/hbase org.apache.hadoop.hbase.mapreduce.RowCounter &lt;tablename&gt; [&lt;column1&gt; &lt;column2&gt;...]
</pre><p>Note: caching for the input Scan is configured via
          <code class="code">hbase.client.scanner.caching</code> in the job configuration. </p><p>HBase ships another diagnostic mapreduce job called <a class="link" href="http://hbase.apache.org/apidocs/org/apache/hadoop/hbase/mapreduce/CellCounter.html" target="_top">CellCounter</a>.
        Like RowCounter, it gathers more fine-grained statistics about your table. The statistics
        gathered by RowCounter are more fine-grained and include: </p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem"><p>Total number of rows in the table.</p></li><li class="listitem"><p>Total number of CFs across all rows.</p></li><li class="listitem"><p>Total qualifiers across all rows.</p></li><li class="listitem"><p>Total occurrence of each CF.</p></li><li class="listitem"><p>Total occurrence of each qualifier.</p></li><li class="listitem"><p>Total number of versions of each qualifier.</p></li></ul></div><p>The program allows you to limit the scope of the run. Provide a row regex or prefix to
        limit the rows to analyze. Use <code class="code">hbase.mapreduce.scan.column.family</code> to specify
        scanning a single column family.</p><pre class="screen">$ bin/hbase org.apache.hadoop.hbase.mapreduce.CellCounter &lt;tablename&gt; &lt;outputDir&gt; [regex or prefix]</pre><p>Note: just like RowCounter, caching for the input Scan is configured via
          <code class="code">hbase.client.scanner.caching</code> in the job configuration. </p></div><div class="section" title="1.1.15.&nbsp;mlockall"><div class="titlepage"><div><div><h3 class="title"><a name="mlockall"></a>1.1.15.&nbsp;mlockall</h3></div></div></div><p>It is possible to optionally pin your servers in physical memory making them less likely
        to be swapped out in oversubscribed environments by having the servers call <a class="link" href="http://linux.die.net/man/2/mlockall" target="_top">mlockall</a> on startup. See <a class="link" href="https://issues.apache.org/jira/browse/HBASE-4391" target="_top">HBASE-4391 Add ability to
          start RS as root and call mlockall</a> for how to build the optional library and have
        it run on startup. </p></div><div class="section" title="1.1.16.&nbsp;Offline Compaction Tool"><div class="titlepage"><div><div><h3 class="title"><a name="compaction.tool"></a>1.1.16.&nbsp;Offline Compaction Tool</h3></div></div></div><p>See the usage for the <a class="link" href="http://hbase.apache.org/apidocs/org/apache/hadoop/hbase/regionserver/CompactionTool.html" target="_top">Compaction
          Tool</a>. Run it like this <span class="command"><strong>./bin/hbase
          org.apache.hadoop.hbase.regionserver.CompactionTool</strong></span>
      </p></div><div class="section" title="1.1.17.&nbsp;hbase clean"><div class="titlepage"><div><div><h3 class="title"><a name="d839e586"></a>1.1.17.&nbsp;<span class="command"><strong>hbase clean</strong></span></h3></div></div></div><p>The <span class="command"><strong>hbase clean</strong></span> command cleans HBase data from ZooKeeper, HDFS, or
        both. It is appropriate to use for testing. Run it with no options for usage instructions.
        The <span class="command"><strong>hbase clean</strong></span> command was introduced in HBase 0.98.</p><pre class="screen">
$ <strong class="userinput"><code>bin/hbase clean</code></strong>
Usage: hbase clean (--cleanZk|--cleanHdfs|--cleanAll)
Options:
        --cleanZk   cleans hbase related data from zookeeper.
        --cleanHdfs cleans hbase related data from hdfs.
        --cleanAll  cleans hbase related data from both zookeeper and hdfs.        
      </pre></div><div class="section" title="1.1.18.&nbsp;hbase pe"><div class="titlepage"><div><div><h3 class="title"><a name="d839e603"></a>1.1.18.&nbsp;<span class="command"><strong>hbase pe</strong></span></h3></div></div></div><p>The <span class="command"><strong>hbase pe</strong></span> command is a shortcut provided to run the
          <code class="code">org.apache.hadoop.hbase.PerformanceEvaluation</code> tool, which is used for
        testing. The <span class="command"><strong>hbase pe</strong></span> command was introduced in HBase 0.98.4.</p><p>The PerformanceEvaluation tool accepts many different options and commands. For usage
        instructions, run the command with no options.</p><p>To run PerformanceEvaluation prior to HBase 0.98.4, issue the command
          <span class="command"><strong>hbase org.apache.hadoop.hbase.PerformanceEvaluation</strong></span>.</p><p>The PerformanceEvaluation tool has received many updates in recent HBase releases,
        including support for namespaces, support for tags, cell-level ACLs and visibility labels,
        multiget support for RPC calls, increased sampling sizes, an option to randomly sleep during
        testing, and ability to "warm up" the cluster before testing starts.</p></div><div class="section" title="1.1.19.&nbsp;hbase ltt"><div class="titlepage"><div><div><h3 class="title"><a name="d839e627"></a>1.1.19.&nbsp;<span class="command"><strong>hbase ltt</strong></span></h3></div></div></div><p>The <span class="command"><strong>hbase ltt</strong></span> command is a shortcut provided to run the
        <code class="code">org.apache.hadoop.hbase.util.LoadTestTool</code> utility, which is used for
        testing. The <span class="command"><strong>hbase ltt</strong></span> command was introduced in HBase 0.98.4.</p><p>You must specify either <code class="option">-write</code> or <code class="option">-update-read</code> as the
        first option. For general usage instructions, pass the <code class="option">-h</code> option.</p><p>To run LoadTestTool prior to HBase 0.98.4, issue the command <span class="command"><strong>hbase
          org.apache.hadoop.hbase.util.LoadTestTool</strong></span>.</p><p>The LoadTestTool has received many updates in recent HBase releases, including support
        for namespaces, support for tags, cell-level ACLS and visibility labels, testing
        security-related features, ability to specify the number of regions per server, tests for
        multi-get RPC calls, and tests relating to replication.</p></div></div><div class="section" title="1.2.&nbsp;Region Management"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="ops.regionmgt"></a>1.2.&nbsp;Region Management</h2></div></div></div><div class="section" title="1.2.1.&nbsp;Major Compaction"><div class="titlepage"><div><div><h3 class="title"><a name="ops.regionmgt.majorcompact"></a>1.2.1.&nbsp;Major Compaction</h3></div></div></div><p>Major compactions can be requested via the HBase shell or <a class="link" href="http://hbase.apache.org/apidocs/org/apache/hadoop/hbase/client/HBaseAdmin.html#majorCompact%28java.lang.String%29" target="_top">HBaseAdmin.majorCompact</a>. </p><p>Note: major compactions do NOT do region merges. See <a class="xref" href="#">???</a> for more information about compactions. </p></div><div class="section" title="1.2.2.&nbsp;Merge"><div class="titlepage"><div><div><h3 class="title"><a name="ops.regionmgt.merge"></a>1.2.2.&nbsp;Merge</h3></div></div></div><p>Merge is a utility that can merge adjoining regions in the same table (see
        org.apache.hadoop.hbase.util.Merge).</p><pre class="programlisting">$ bin/hbase org.apache.hadoop.hbase.util.Merge &lt;tablename&gt; &lt;region1&gt; &lt;region2&gt;
</pre><p>If you feel you have too many regions and want to consolidate them, Merge is the utility
        you need. Merge must run be done when the cluster is down. See the <a class="link" href="http://ofps.oreilly.com/titles/9781449396107/performance.html" target="_top">O'Reilly HBase
          Book</a> for an example of usage. </p><p>You will need to pass 3 parameters to this application. The first one is the table name.
        The second one is the fully qualified name of the first region to merge, like
        "table_name,\x0A,1342956111995.7cef47f192318ba7ccc75b1bbf27a82b.". The third one is the
        fully qualified name for the second region to merge. </p><p>Additionally, there is a Ruby script attached to <a class="link" href="https://issues.apache.org/jira/browse/HBASE-1621" target="_top">HBASE-1621</a> for region
        merging. </p></div></div><div class="section" title="1.3.&nbsp;Node Management"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="node.management"></a>1.3.&nbsp;Node Management</h2></div></div></div><div class="section" title="1.3.1.&nbsp;Node Decommission"><div class="titlepage"><div><div><h3 class="title"><a name="decommission"></a>1.3.1.&nbsp;Node Decommission</h3></div></div></div><p>You can stop an individual RegionServer by running the following script in the HBase
        directory on the particular node:</p><pre class="screen">$ ./bin/hbase-daemon.sh stop regionserver</pre><p> The RegionServer will first close all regions and then shut itself down. On shutdown,
        the RegionServer's ephemeral node in ZooKeeper will expire. The master will notice the
        RegionServer gone and will treat it as a 'crashed' server; it will reassign the nodes the
        RegionServer was carrying. </p><div class="note" title="Disable the Load Balancer before Decommissioning a node" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title">Disable the Load Balancer before Decommissioning a node</h3><p>If the load balancer runs while a node is shutting down, then there could be
          contention between the Load Balancer and the Master's recovery of the just decommissioned
          RegionServer. Avoid any problems by disabling the balancer first. See <a class="xref" href="#lb" title="Load Balancer">Load Balancer</a> below. </p></div><div class="note" title="Kill Node Tool" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title">Kill Node Tool</h3><p>In hbase-2.0, in the bin directory, we added a script named
          <code class="filename">considerAsDead.sh</code> that can be used to kill a regionserver.
          Hardware issues could be detected by specialized monitoring tools before the 
          zookeeper timeout has expired. <code class="filename">considerAsDead.sh</code> is a
          simple function to mark a RegionServer as dead.  It deletes all the znodes
          of the server, starting the recovery process.  Plug in the script into
          your monitoring/fault detection tools to initiate faster failover. Be
          careful how you use this disruptive tool. Copy the script if you need to
          make use of it in a version of hbase previous to hbase-2.0.
        </p></div><p> A downside to the above stop of a RegionServer is that regions could be offline for a
        good period of time. Regions are closed in order. If many regions on the server, the first
        region to close may not be back online until all regions close and after the master notices
        the RegionServer's znode gone. In Apache HBase 0.90.2, we added facility for having a node
        gradually shed its load and then shutdown itself down. Apache HBase 0.90.2 added the
          <code class="filename">graceful_stop.sh</code> script. Here is its usage:</p><pre class="screen">$ ./bin/graceful_stop.sh
Usage: graceful_stop.sh [--config &amp;conf-dir&gt;] [--restart] [--reload] [--thrift] [--rest] &amp;hostname&gt;
 thrift      If we should stop/start thrift before/after the hbase stop/start
 rest        If we should stop/start rest before/after the hbase stop/start
 restart     If we should restart after graceful stop
 reload      Move offloaded regions back on to the stopped server
 debug       Move offloaded regions back on to the stopped server
 hostname    Hostname of server we are to stop</pre><p> To decommission a loaded RegionServer, run the following: <span class="command"><strong>$
          ./bin/graceful_stop.sh HOSTNAME</strong></span> where <code class="varname">HOSTNAME</code> is the host
        carrying the RegionServer you would decommission. </p><div class="note" title="On HOSTNAME" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title">On <code class="varname">HOSTNAME</code></h3><p>The <code class="varname">HOSTNAME</code> passed to <code class="filename">graceful_stop.sh</code> must
          match the hostname that hbase is using to identify RegionServers. Check the list of
          RegionServers in the master UI for how HBase is referring to servers. Its usually hostname
          but can also be FQDN. Whatever HBase is using, this is what you should pass the
            <code class="filename">graceful_stop.sh</code> decommission script. If you pass IPs, the script
          is not yet smart enough to make a hostname (or FQDN) of it and so it will fail when it
          checks if server is currently running; the graceful unloading of regions will not run.
        </p></div><p> The <code class="filename">graceful_stop.sh</code> script will move the regions off the
        decommissioned RegionServer one at a time to minimize region churn. It will verify the
        region deployed in the new location before it will moves the next region and so on until the
        decommissioned server is carrying zero regions. At this point, the
          <code class="filename">graceful_stop.sh</code> tells the RegionServer <span class="command"><strong>stop</strong></span>. The
        master will at this point notice the RegionServer gone but all regions will have already
        been redeployed and because the RegionServer went down cleanly, there will be no WAL logs to
        split. </p><div class="note" title="Load Balancer" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a name="lb"></a>Load Balancer</h3><p> It is assumed that the Region Load Balancer is disabled while the
            <span class="command"><strong>graceful_stop</strong></span> script runs (otherwise the balancer and the
          decommission script will end up fighting over region deployments). Use the shell to
          disable the balancer:</p><pre class="programlisting">hbase(main):001:0&gt; balance_switch false
true
0 row(s) in 0.3590 seconds</pre><p> This turns the balancer OFF. To reenable, do:</p><pre class="programlisting">hbase(main):001:0&gt; balance_switch true
false
0 row(s) in 0.3590 seconds</pre><p>The <span class="command"><strong>graceful_stop</strong></span> will check the balancer and if enabled, will turn
          it off before it goes to work. If it exits prematurely because of error, it will not have
          reset the balancer. Hence, it is better to manage the balancer apart from
            <span class="command"><strong>graceful_stop</strong></span> reenabling it after you are done w/ graceful_stop.
        </p></div><div class="section" title="1.3.1.1.&nbsp;Decommissioning several Regions Servers concurrently"><div class="titlepage"><div><div><h4 class="title"><a name="draining.servers"></a>1.3.1.1.&nbsp;Decommissioning several Regions Servers concurrently</h4></div></div></div><p>If you have a large cluster, you may want to decommission more than one machine at a
          time by gracefully stopping mutiple RegionServers concurrently. To gracefully drain
          multiple regionservers at the same time, RegionServers can be put into a "draining" state.
          This is done by marking a RegionServer as a draining node by creating an entry in
          ZooKeeper under the <code class="filename">hbase_root/draining</code> znode. This znode has format
            <code class="code">name,port,startcode</code> just like the regionserver entries under
            <code class="filename">hbase_root/rs</code> znode. </p><p>Without this facility, decommissioning mulitple nodes may be non-optimal because
          regions that are being drained from one region server may be moved to other regionservers
          that are also draining. Marking RegionServers to be in the draining state prevents this
          from happening. See this <a class="link" href="http://inchoate-clatter.blogspot.com/2012/03/hbase-ops-automation.html" target="_top">blog
            post</a> for more details.</p></div><div class="section" title="1.3.1.2.&nbsp;Bad or Failing Disk"><div class="titlepage"><div><div><h4 class="title"><a name="bad.disk"></a>1.3.1.2.&nbsp;Bad or Failing Disk</h4></div></div></div><p>It is good having <a class="xref" href="#">???</a> set if you have a decent number of
          disks per machine for the case where a disk plain dies. But usually disks do the "John
          Wayne" -- i.e. take a while to go down spewing errors in <code class="filename">dmesg</code> -- or
          for some reason, run much slower than their companions. In this case you want to
          decommission the disk. You have two options. You can <a class="link" href="http://wiki.apache.org/hadoop/FAQ#I_want_to_make_a_large_cluster_smaller_by_taking_out_a_bunch_of_nodes_simultaneously._How_can_this_be_done.3F" target="_top">decommission
            the datanode</a> or, less disruptive in that only the bad disks data will be
          rereplicated, can stop the datanode, unmount the bad volume (You can't umount a volume
          while the datanode is using it), and then restart the datanode (presuming you have set
          dfs.datanode.failed.volumes.tolerated &gt; 0). The regionserver will throw some errors in its
          logs as it recalibrates where to get its data from -- it will likely roll its WAL log too
          -- but in general but for some latency spikes, it should keep on chugging. </p><div class="note" title="Short Circuit Reads" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title">Short Circuit Reads</h3><p>If you are doing short-circuit reads, you will have to move the regions off the
            regionserver before you stop the datanode; when short-circuiting reading, though chmod'd
            so regionserver cannot have access, because it already has the files open, it will be
            able to keep reading the file blocks from the bad disk even though the datanode is down.
            Move the regions back after you restart the datanode.</p></div></div></div><div class="section" title="1.3.2.&nbsp;Rolling Restart"><div class="titlepage"><div><div><h3 class="title"><a name="rolling"></a>1.3.2.&nbsp;Rolling Restart</h3></div></div></div><p>Some cluster configuration changes require either the entire cluster, or the
          RegionServers, to be restarted in order to pick up the changes. In addition, rolling
          restarts are supported for upgrading to a minor or maintenance release, and to a major
          release if at all possible. See the release notes for release you want to upgrade to, to
          find out about limitations to the ability to perform a rolling upgrade.</p><p>There are multiple ways to restart your cluster nodes, depending on your situation.
        These methods are detailed below.</p><div class="section" title="1.3.2.1.&nbsp;Using the rolling-restart.sh Script"><div class="titlepage"><div><div><h4 class="title"><a name="d839e833"></a>1.3.2.1.&nbsp;Using the <span class="command"><strong>rolling-restart.sh</strong></span> Script</h4></div></div></div><p>HBase ships with a script, <code class="filename">bin/rolling-restart.sh</code>, that allows
          you to perform rolling restarts on the entire cluster, the master only, or the
          RegionServers only. The script is provided as a template for your own script, and is not
          explicitly tested. It requires password-less SSH login to be configured and assumes that
          you have deployed using a tarball. The script requires you to set some environment
          variables before running it. Examine the script and modify it to suit your needs.</p><div class="example"><a name="d839e844"></a><p class="title"><b>Example&nbsp;1.1.&nbsp;<code class="filename">rolling-restart.sh</code> General Usage</b></p><div class="example-contents"><pre class="screen">
$ <strong class="userinput"><code>./bin/rolling-restart.sh --help</code></strong>
Usage: rolling-restart.sh [--config &lt;hbase-confdir&gt;] [--rs-only] [--master-only] [--graceful] [--maxthreads xx]          
        </pre></div></div><br class="example-break"><div class="variablelist"><dl><dt><span class="term">Rolling Restart on RegionServers Only</span></dt><dd><p>To perform a rolling restart on the RegionServers only, use the
                  <code class="code">--rs-only</code> option. This might be necessary if you need to reboot the
                individual RegionServer or if you make a configuration change that only affects
                RegionServers and not the other HBase processes.</p><p>If you need to restart only a single RegionServer, or if you need to do extra
                actions during the restart, use the <code class="filename">bin/graceful_stop.sh</code>
                command instead. See <a class="xref" href="#rolling.restart.manual" title="1.3.2.2.&nbsp;Manual Rolling Restart">Section&nbsp;1.3.2.2, &#8220;Manual Rolling Restart&#8221;</a>.</p></dd><dt><span class="term">Rolling Restart on Masters Only</span></dt><dd><p>To perform a rolling restart on the active and backup Masters, use the
                  <code class="code">--master-only</code> option. You might use this if you know that your
                configuration change only affects the Master and not the RegionServers, or if you
                need to restart the server where the active Master is running.</p><p>If you are not running backup Masters, the Master is simply restarted. If you
                are running backup Masters, they are all stopped before any are restarted, to avoid
                a race condition in ZooKeeper to determine which is the new Master. First the main
                Master is restarted, then the backup Masters are restarted. Directly after restart,
                it checks for and cleans out any regions in transition before taking on its normal
                workload.</p></dd><dt><span class="term">Graceful Restart</span></dt><dd><p>If you specify the <code class="code">--graceful</code> option, RegionServers are restarted
                using the <code class="filename">bin/graceful_stop.sh</code> script, which moves regions off
                a RegionServer before restarting it. This is safer, but can delay the
                restart.</p></dd><dt><span class="term">Limiting the Number of Threads</span></dt><dd><p>To limit the rolling restart to using only a specific number of threads, use the
                  <code class="code">--maxthreads</code> option.</p></dd></dl></div></div><div class="section" title="1.3.2.2.&nbsp;Manual Rolling Restart"><div class="titlepage"><div><div><h4 class="title"><a name="rolling.restart.manual"></a>1.3.2.2.&nbsp;Manual Rolling Restart</h4></div></div></div><p>To retain more control over the process, you may wish to manually do a rolling restart
          across your cluster. This uses the <span class="command"><strong>graceful-stop.sh</strong></span> command <a class="xref" href="#decommission" title="1.3.1.&nbsp;Node Decommission">Section&nbsp;1.3.1, &#8220;Node Decommission&#8221;</a>. In this method, you can restart each RegionServer
          individually and then move its old regions back into place, retaining locality. If you
          also need to restart the Master, you need to do it separately, and restart the Master
          before restarting the RegionServers using this method. The following is an example of such
          a command. You may need to tailor it to your environment. This script does a rolling
          restart of RegionServers only. It disables the load balancer before moving the
          regions.</p><pre class="screen">
$ for i in `cat conf/regionservers|sort`; do ./bin/graceful_stop.sh --restart --reload --debug $i; done &amp;&gt; /tmp/log.txt &amp;;     
        </pre><p>Monitor the output of the <code class="filename">/tmp/log.txt</code> file to follow the
          progress of the script. </p></div><div class="section" title="1.3.2.3.&nbsp;Logic for Crafting Your Own Rolling Restart Script"><div class="titlepage"><div><div><h4 class="title"><a name="d839e920"></a>1.3.2.3.&nbsp;Logic for Crafting Your Own Rolling Restart Script</h4></div></div></div><p>Use the following guidelines if you want to create your own rolling restart script.</p><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem"><p>Extract the new release, verify its configuration, and synchronize it to all nodes
              of your cluster using <span class="command"><strong>rsync</strong></span>, <span class="command"><strong>scp</strong></span>, or another
              secure synchronization mechanism.</p></li><li class="listitem"><p>Use the hbck utility to ensure that the cluster is consistent.</p><pre class="screen">
$ ./bin/hbck            
          </pre><p>Perform repairs if required. See <a class="xref" href="#hbck" title="1.1.4.&nbsp;HBase hbck">Section&nbsp;1.1.4, &#8220;HBase <span class="application">hbck</span>&#8221;</a> for details.</p></li><li class="listitem"><p>Restart the master first. You may need to modify these commands if your
            new HBase directory is different from the old one, such as for an upgrade.</p><pre class="screen">
$ ./bin/hbase-daemon.sh stop master; ./bin/hbase-daemon.sh start master            
          </pre></li><li class="listitem"><p>Gracefully restart each RegionServer, using a script such as the
            following, from the Master.</p><pre class="screen">
$ for i in `cat conf/regionservers|sort`; do ./bin/graceful_stop.sh --restart --reload --debug $i; done &amp;&gt; /tmp/log.txt &amp;            
          </pre><p>If you are running Thrift or REST servers, pass the --thrift or --rest options.
              For other available options, run the <span class="command"><strong>bin/graceful-stop.sh --help</strong></span>
              command.</p><p>It is important to drain HBase regions slowly when restarting multiple
              RegionServers. Otherwise, multiple regions go offline simultaneously and must be
              reassigned to other nodes, which may also go offline soon. This can negatively affect
              performance. You can inject delays into the script above, for instance, by adding a
              Shell command such as <span class="command"><strong>sleep</strong></span>. To wait for 5 minutes between each
              RegionServer restart, modify the above script to the following:</p><pre class="screen">
$ for i in `cat conf/regionservers|sort`; do ./bin/graceful_stop.sh --restart --reload --debug $i &amp; sleep 5m; done &amp;&gt; /tmp/log.txt &amp;            
          </pre></li><li class="listitem"><p>Restart the Master again, to clear out the dead servers list and re-enable
          the load balancer.</p></li><li class="listitem"><p>Run the <span class="command"><strong>hbck</strong></span> utility again, to be sure the cluster is
            consistent.</p></li></ol></div></div></div><div class="section" title="1.3.3.&nbsp;Adding a New Node"><div class="titlepage"><div><div><h3 class="title"><a name="adding.new.node"></a>1.3.3.&nbsp;Adding a New Node</h3></div></div></div><p>Adding a new regionserver in HBase is essentially free, you simply start it like this:
          <span class="command"><strong>$ ./bin/hbase-daemon.sh start regionserver</strong></span> and it will register itself
        with the master. Ideally you also started a DataNode on the same machine so that the RS can
        eventually start to have local files. If you rely on ssh to start your daemons, don't forget
        to add the new hostname in <code class="filename">conf/regionservers</code> on the master. </p><p>At this point the region server isn't serving data because no regions have moved to it
        yet. If the balancer is enabled, it will start moving regions to the new RS. On a
        small/medium cluster this can have a very adverse effect on latency as a lot of regions will
        be offline at the same time. It is thus recommended to disable the balancer the same way
        it's done when decommissioning a node and move the regions manually (or even better, using a
        script that moves them one by one). </p><p>The moved regions will all have 0% locality and won't have any blocks in cache so the
        region server will have to use the network to serve requests. Apart from resulting in higher
        latency, it may also be able to use all of your network card's capacity. For practical
        purposes, consider that a standard 1GigE NIC won't be able to read much more than
          <span class="emphasis"><em>100MB/s</em></span>. In this case, or if you are in a OLAP environment and
        require having locality, then it is recommended to major compact the moved regions. </p></div></div><div class="section" title="1.4.&nbsp;HBase Metrics"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="hbase_metrics"></a>1.4.&nbsp;HBase Metrics</h2></div></div></div><p>HBase emits metrics which adhere to the <a class="link" href="http://hadoop.apache.org/core/docs/current/api/org/apache/hadoop/metrics/package-summary.html" target="_top">Hadoop metrics</a> API. Starting with HBase 0.95<sup>[<a name="d839e1002" href="#ftn.d839e1002" class="footnote">1</a>]</sup>,
      HBase is configured to emit a default
      set of metrics with a default sampling period of every 10 seconds. You can use HBase
      metrics in conjunction with Ganglia. You can also filter which metrics are emitted and extend
      the metrics framework to capture custom metrics appropriate for your environment.</p><div class="section" title="1.4.1.&nbsp;Metric Setup"><div class="titlepage"><div><div><h3 class="title"><a name="metric_setup"></a>1.4.1.&nbsp;Metric Setup</h3></div></div></div><p>For HBase 0.95 and newer, HBase ships with a default metrics configuration, or
          <em class="firstterm">sink</em>. This includes a wide variety of individual metrics, and emits
        them every 10 seconds by default. To configure metrics for a given region server, edit the
          <code class="filename">conf/hadoop-metrics2-hbase.properties</code> file. Restart the region server
        for the changes to take effect.</p><p>To change the sampling rate for the default sink, edit the line beginning with
          <code class="literal">*.period</code>. To filter which metrics are emitted or to extend the metrics
        framework, see <a class="link" href="http://hadoop.apache.org/docs/current/api/org/apache/hadoop/metrics2/package-summary.html" target="_top">http://hadoop.apache.org/docs/current/api/org/apache/hadoop/metrics2/package-summary.html</a>
      </p><div class="note" title="HBase Metrics and Ganglia" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a name="rs_metrics_ganglia"></a>HBase Metrics and Ganglia</h3><p>By default, HBase emits a large number of metrics per region server. Ganglia may have
          difficulty processing all these metrics. Consider increasing the capacity of the Ganglia
          server or reducing the number of metrics emitted by HBase. See <a class="link" href="http://hadoop.apache.org/docs/current/api/org/apache/hadoop/metrics2/package-summary.html#filtering" target="_top">Metrics Filtering</a>.</p></div></div><div class="section" title="1.4.2.&nbsp;Disabling Metrics"><div class="titlepage"><div><div><h3 class="title"><a name="d839e1035"></a>1.4.2.&nbsp;Disabling Metrics</h3></div></div></div><p>To disable metrics for a region server, edit the
          <code class="filename">conf/hadoop-metrics2-hbase.properties</code> file and comment out any
        uncommented lines. Restart the region server for the changes to take effect.</p></div><div class="section" title="1.4.3.&nbsp;Discovering Available Metrics"><div class="titlepage"><div><div><h3 class="title"><a name="discovering.available.metrics"></a>1.4.3.&nbsp;Discovering Available Metrics</h3></div></div></div><p>Rather than listing each metric which HBase emits by default, you can browse through the
        available metrics, either as a JSON output or via JMX. Different metrics are
        exposed for the Master process and each region server process.</p><div class="procedure" title="Procedure&nbsp;1.1.&nbsp;Access a JSON Output of Available Metrics"><a name="d839e1048"></a><p class="title"><b>Procedure&nbsp;1.1.&nbsp;Access a JSON Output of Available Metrics</b></p><ol class="procedure" type="1"><li class="step" title="Step 1"><p>After starting HBase, access the region server's web UI, at
              <code class="literal">http://REGIONSERVER_HOSTNAME:60030</code> by default (or port 16030 in HBase 1.0+).</p></li><li class="step" title="Step 2"><p>Click the <span class="guilabel">Metrics Dump</span> link near the top. The metrics for the region server are
            presented as a dump of the JMX bean in JSON format. This will dump out all metrics names and their
            values.
            To include metrics descriptions in the listing &#8212; this can be useful when you are exploring
            what is available &#8212; add a query string of
            <code class="literal">?description=true</code> so your URL becomes
            <code class="literal">http://REGIONSERVER_HOSTNAME:60030/jmx?description=true</code>.
            Not all beans and attributes have descriptions.
          </p></li><li class="step" title="Step 3"><p>To view metrics for the Master, connect to the Master's web UI instead (defaults to
              <code class="literal">http://localhost:60010</code> or port 16010 in HBase 1.0+) and click its <span class="guilabel">Metrics
                Dump</span> link.
            To include metrics descriptions in the listing &#8212; this can be useful when you are exploring
            what is available &#8212; add a query string of
            <code class="literal">?description=true</code> so your URL becomes
            <code class="literal">http://REGIONSERVER_HOSTNAME:60010/jmx?description=true</code>.
            Not all beans and attributes have descriptions.
            </p></li></ol></div><div class="procedure" title="Procedure&nbsp;1.2.&nbsp;Browse the JMX Output of Available Metrics"><a name="d839e1084"></a><p class="title"><b>Procedure&nbsp;1.2.&nbsp;Browse the JMX Output of Available Metrics</b></p><p>You can use many different tools to view JMX content by browsing MBeans. This
          procedure uses <span class="command"><strong>jvisualvm</strong></span>, which is an application usually available in the JDK.
            </p><ol class="procedure" type="1"><li class="step" title="Step 1"><p>Start HBase, if it is not already running.</p></li><li class="step" title="Step 2"><p>Run the command <span class="command"><strong>jvisualvm</strong></span> command on a host with a GUI display.
            You can launch it from the command line or another method appropriate for your operating
            system.</p></li><li class="step" title="Step 3"><p>Be sure the <span class="guilabel">VisualVM-MBeans</span> plugin is installed. Browse to <span class="guimenu">Tools</span> &#8594; <span class="guimenuitem">Plugins</span>. Click <span class="guilabel">Installed</span> and check whether the plugin is
            listed. If not, click <span class="guilabel">Available Plugins</span>, select it, and click
              <span class="guibutton">Install</span>. When finished, click
            <span class="guibutton">Close</span>.</p></li><li class="step" title="Step 4"><p>To view details for a given HBase process, double-click the process in the
              <span class="guilabel">Local</span> sub-tree in the left-hand panel. A detailed view opens in
            the right-hand panel. Click the <span class="guilabel">MBeans</span> tab which appears as a tab
            in the top of the right-hand panel.</p></li><li class="step" title="Step 5"><p>To access the HBase metrics, navigate to the appropriate sub-bean:</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem"><p>Master: <span class="guimenu">Hadoop</span> &#8594; <span class="guisubmenu">HBase</span> &#8594; <span class="guisubmenu">Master</span> &#8594; <span class="guisubmenu">Server</span></p></li><li class="listitem"><p>RegionServer: <span class="guimenu">Hadoop</span> &#8594; <span class="guisubmenu">HBase</span> &#8594; <span class="guisubmenu">RegionServer</span> &#8594; <span class="guisubmenu">Server</span></p></li></ul></div></li><li class="step" title="Step 6"><p>The name of each metric and its current value is displayed in the
              <span class="guilabel">Attributes</span> tab. For a view which includes more details, including
            the description of each attribute, click the <span class="guilabel">Metadata</span> tab.</p></li></ol></div></div><div class="section" title="1.4.4.&nbsp;Units of Measure for Metrics"><div class="titlepage"><div><div><h3 class="title"><a name="d839e1171"></a>1.4.4.&nbsp;Units of Measure for Metrics</h3></div></div></div><p>Different metrics are expressed in different units, as appropriate. Often, the unit of
        measure is in the name (as in the metric <code class="code">shippedKBs</code>). Otherwise, use the
        following guidelines. When in doubt, you may need to examine the source for a given
        metric.</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem"><p>Metrics that refer to a point in time are usually expressed as a timestamp.</p></li><li class="listitem"><p>Metrics that refer to an age (such as <code class="code">ageOfLastShippedOp</code>) are usually
            expressed in milliseconds.</p></li><li class="listitem"><p>Metrics that refer to memory sizes are in bytes.</p></li><li class="listitem"><p>Sizes of queues (such as <code class="code">sizeOfLogQueue</code>) are expressed as the number of
            items in the queue. Determine the size by multiplying by the block size (default is 64
            MB in HDFS).</p></li><li class="listitem"><p>Metrics that refer to things like the number of a given type of operations (such as
              <code class="code">logEditsRead</code>) are expressed as an integer.</p></li></ul></div></div><div class="section" title="1.4.5.&nbsp;Most Important Master Metrics"><div class="titlepage"><div><div><h3 class="title"><a name="master_metrics"></a>1.4.5.&nbsp;Most Important Master Metrics</h3></div></div></div><p>Note: Counts are usually over the last metrics reporting interval.</p><div class="variablelist"><dl><dt><span class="term">hbase.master.numRegionServers</span></dt><dd><p>Number of live regionservers</p></dd><dt><span class="term">hbase.master.numDeadRegionServers</span></dt><dd><p>Number of dead regionservers</p></dd><dt><span class="term">hbase.master.ritCount </span></dt><dd><p>The number of regions in transition</p></dd><dt><span class="term">hbase.master.ritCountOverThreshold</span></dt><dd><p>The number of regions that have been in transition longer than
            a threshold time (default: 60 seconds)</p></dd><dt><span class="term">hbase.master.ritOldestAge</span></dt><dd><p>The age of the longest region in transition, in milliseconds
            </p></dd></dl></div></div><div class="section" title="1.4.6.&nbsp;Most Important RegionServer Metrics"><div class="titlepage"><div><div><h3 class="title"><a name="rs_metrics"></a>1.4.6.&nbsp;Most Important RegionServer Metrics</h3></div></div></div><p>Note: Counts are usually over the last metrics reporting interval.</p><div class="variablelist"><dl><dt><span class="term">hbase.regionserver.regionCount</span></dt><dd><p>The number of regions hosted by the regionserver</p></dd><dt><span class="term">hbase.regionserver.storeFileCount</span></dt><dd><p>The number of store files on disk currently managed by the
            regionserver</p></dd><dt><span class="term">hbase.regionserver.storeFileSize</span></dt><dd><p>Aggregate size of the store files on disk</p></dd><dt><span class="term">hbase.regionserver.hlogFileCount</span></dt><dd><p>The number of write ahead logs not yet archived</p></dd><dt><span class="term">hbase.regionserver.totalRequestCount</span></dt><dd><p>The total number of requests received</p></dd><dt><span class="term">hbase.regionserver.readRequestCount</span></dt><dd><p>The number of read requests received</p></dd><dt><span class="term">hbase.regionserver.writeRequestCount</span></dt><dd><p>The number of write requests received</p></dd><dt><span class="term">hbase.regionserver.numOpenConnections</span></dt><dd><p>The number of open connections at the RPC layer</p></dd><dt><span class="term">hbase.regionserver.numActiveHandler</span></dt><dd><p>The number of RPC handlers actively servicing requests</p></dd><dt><span class="term">hbase.regionserver.numCallsInGeneralQueue</span></dt><dd><p>The number of currently enqueued user requests</p></dd><dt><span class="term">hbase.regionserver.numCallsInReplicationQueue</span></dt><dd><p>The number of currently enqueued operations received from
            replication</p></dd><dt><span class="term">hbase.regionserver.numCallsInPriorityQueue</span></dt><dd><p>The number of currently enqueued priority (internal housekeeping)
            requests</p></dd><dt><span class="term">hbase.regionserver.flushQueueLength</span></dt><dd><p>Current depth of the memstore flush queue. If increasing, we are falling
            behind with clearing memstores out to HDFS.</p></dd><dt><span class="term">hbase.regionserver.updatesBlockedTime</span></dt><dd><p>Number of milliseconds updates have been blocked so the memstore can be
            flushed</p></dd><dt><span class="term">hbase.regionserver.compactionQueueLength</span></dt><dd><p>Current depth of the compaction request queue. If increasing, we are
            falling behind with storefile compaction.</p></dd><dt><span class="term">hbase.regionserver.blockCacheHitCount</span></dt><dd><p>The number of block cache hits</p></dd><dt><span class="term">hbase.regionserver.blockCacheMissCount</span></dt><dd><p>The number of block cache misses</p></dd><dt><span class="term">hbase.regionserver.blockCacheExpressHitPercent </span></dt><dd><p>The percent of the time that requests with the cache turned on hit the
            cache</p></dd><dt><span class="term">hbase.regionserver.percentFilesLocal</span></dt><dd><p>Percent of store file data that can be read from the local DataNode,
            0-100</p></dd><dt><span class="term">hbase.regionserver.&lt;op&gt;_&lt;measure&gt;</span></dt><dd><p>Operation latencies, where &lt;op&gt; is one of Append, Delete, Mutate,
            Get, Replay, Increment; and where &lt;measure&gt; is one of min, max, mean, median,
            75th_percentile, 95th_percentile, 99th_percentile</p></dd><dt><span class="term">hbase.regionserver.slow&lt;op&gt;Count </span></dt><dd><p>The number of operations we thought were slow, where &lt;op&gt; is one
            of the list above</p></dd><dt><span class="term">hbase.regionserver.GcTimeMillis</span></dt><dd><p>Time spent in garbage collection, in milliseconds</p></dd><dt><span class="term">hbase.regionserver.GcTimeMillisParNew</span></dt><dd><p>Time spent in garbage collection of the young generation, in
            milliseconds</p></dd><dt><span class="term">hbase.regionserver.GcTimeMillisConcurrentMarkSweep</span></dt><dd><p>Time spent in garbage collection of the old generation, in
            milliseconds</p></dd><dt><span class="term">hbase.regionserver.authenticationSuccesses</span></dt><dd><p>Number of client connections where authentication succeeded</p></dd><dt><span class="term">hbase.regionserver.authenticationFailures</span></dt><dd><p>Number of client connection authentication failures</p></dd><dt><span class="term">hbase.regionserver.mutationsWithoutWALCount </span></dt><dd><p>Count of writes submitted with a flag indicating they should bypass the
            write ahead log</p></dd></dl></div></div></div><div class="section" title="1.5.&nbsp;HBase Monitoring"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="ops.monitoring"></a>1.5.&nbsp;HBase Monitoring</h2></div></div></div><div class="section" title="1.5.1.&nbsp;Overview"><div class="titlepage"><div><div><h3 class="title"><a name="ops.monitoring.overview"></a>1.5.1.&nbsp;Overview</h3></div></div></div><p>The following metrics are arguably the most important to monitor for each RegionServer
        for "macro monitoring", preferably with a system like <a class="link" href="http://opentsdb.net/" target="_top">OpenTSDB</a>. If your cluster is having performance
        issues it's likely that you'll see something unusual with this group. </p><div class="itemizedlist" title="HBase:"><p class="title"><b>HBase:</b></p><ul class="itemizedlist" type="disc"><li class="listitem"><p>See <a class="xref" href="#rs_metrics" title="1.4.6.&nbsp;Most Important RegionServer Metrics">Section&nbsp;1.4.6, &#8220;Most Important RegionServer Metrics&#8221;</a></p></li></ul></div><div class="itemizedlist" title="OS:"><p class="title"><b>OS:</b></p><ul class="itemizedlist" type="disc"><li class="listitem"><p>IO Wait</p></li><li class="listitem"><p>User CPU</p></li></ul></div><div class="itemizedlist" title="Java:"><p class="title"><b>Java:</b></p><ul class="itemizedlist" type="disc"><li class="listitem"><p>GC</p></li></ul></div><p> For more information on HBase metrics, see <a class="xref" href="#hbase_metrics" title="1.4.&nbsp;HBase Metrics">Section&nbsp;1.4, &#8220;HBase Metrics&#8221;</a>. </p></div><div class="section" title="1.5.2.&nbsp;Slow Query Log"><div class="titlepage"><div><div><h3 class="title"><a name="ops.slow.query"></a>1.5.2.&nbsp;Slow Query Log</h3></div></div></div><p>The HBase slow query log consists of parseable JSON structures describing the properties
        of those client operations (Gets, Puts, Deletes, etc.) that either took too long to run, or
        produced too much output. The thresholds for "too long to run" and "too much output" are
        configurable, as described below. The output is produced inline in the main region server
        logs so that it is easy to discover further details from context with other logged events.
        It is also prepended with identifying tags <code class="constant">(responseTooSlow)</code>,
          <code class="constant">(responseTooLarge)</code>, <code class="constant">(operationTooSlow)</code>, and
          <code class="constant">(operationTooLarge)</code> in order to enable easy filtering with grep, in
        case the user desires to see only slow queries. </p><div class="section" title="1.5.2.1.&nbsp;Configuration"><div class="titlepage"><div><div><h4 class="title"><a name="d839e1462"></a>1.5.2.1.&nbsp;Configuration</h4></div></div></div><p>There are two configuration knobs that can be used to adjust the thresholds for when
          queries are logged. </p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem"><p><code class="varname">hbase.ipc.warn.response.time</code> Maximum number of milliseconds
              that a query can be run without being logged. Defaults to 10000, or 10 seconds. Can be
              set to -1 to disable logging by time. </p></li><li class="listitem"><p><code class="varname">hbase.ipc.warn.response.size</code> Maximum byte size of response that
              a query can return without being logged. Defaults to 100 megabytes. Can be set to -1
              to disable logging by size. </p></li></ul></div></div><div class="section" title="1.5.2.2.&nbsp;Metrics"><div class="titlepage"><div><div><h4 class="title"><a name="d839e1478"></a>1.5.2.2.&nbsp;Metrics</h4></div></div></div><p>The slow query log exposes to metrics to JMX.</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem"><p><code class="varname">hadoop.regionserver_rpc_slowResponse</code> a global metric reflecting
              the durations of all responses that triggered logging.</p></li><li class="listitem"><p><code class="varname">hadoop.regionserver_rpc_methodName.aboveOneSec</code> A metric
              reflecting the durations of all responses that lasted for more than one second.</p></li></ul></div></div><div class="section" title="1.5.2.3.&nbsp;Output"><div class="titlepage"><div><div><h4 class="title"><a name="d839e1494"></a>1.5.2.3.&nbsp;Output</h4></div></div></div><p>The output is tagged with operation e.g. <code class="constant">(operationTooSlow)</code> if
          the call was a client operation, such as a Put, Get, or Delete, which we expose detailed
          fingerprint information for. If not, it is tagged <code class="constant">(responseTooSlow)</code>
          and still produces parseable JSON output, but with less verbose information solely
          regarding its duration and size in the RPC itself. <code class="constant">TooLarge</code> is
          substituted for <code class="constant">TooSlow</code> if the response size triggered the logging,
          with <code class="constant">TooLarge</code> appearing even in the case that both size and duration
          triggered logging. </p></div><div class="section" title="1.5.2.4.&nbsp;Example"><div class="titlepage"><div><div><h4 class="title"><a name="d839e1514"></a>1.5.2.4.&nbsp;Example</h4></div></div></div><p>
          </p><pre class="programlisting">2011-09-08 10:01:25,824 WARN org.apache.hadoop.ipc.HBaseServer: (operationTooSlow): {"tables":{"riley2":{"puts":[{"totalColumns":11,"families":{"actions":[{"timestamp":1315501284459,"qualifier":"0","vlen":9667580},{"timestamp":1315501284459,"qualifier":"1","vlen":10122412},{"timestamp":1315501284459,"qualifier":"2","vlen":11104617},{"timestamp":1315501284459,"qualifier":"3","vlen":13430635}]},"row":"cfcd208495d565ef66e7dff9f98764da:0"}],"families":["actions"]}},"processingtimems":956,"client":"10.47.34.63:33623","starttimems":1315501284456,"queuetimems":0,"totalPuts":1,"class":"HRegionServer","responsesize":0,"method":"multiPut"}</pre><p>
        </p><p>Note that everything inside the "tables" structure is output produced by MultiPut's
          fingerprint, while the rest of the information is RPC-specific, such as processing time
          and client IP/port. Other client operations follow the same pattern and the same general
          structure, with necessary differences due to the nature of the individual operations. In
          the case that the call is not a client operation, that detailed fingerprint information
          will be completely absent. </p><p>This particular example, for example, would indicate that the likely cause of slowness
          is simply a very large (on the order of 100MB) multiput, as we can tell by the "vlen," or
          value length, fields of each put in the multiPut. </p></div></div><div class="section" title="1.5.3.&nbsp;Block Cache Monitoring"><div class="titlepage"><div><div><h3 class="title"><a name="d839e1526"></a>1.5.3.&nbsp;Block Cache Monitoring</h3></div></div></div><p>Starting with HBase 0.98, the HBase Web UI includes the ability to monitor and report on
        the performance of the block cache. To view the block cache reports, click <span class="guimenu">Tasks</span> &#8594; <span class="guisubmenu">Show Non-RPC Tasks</span> &#8594; <span class="guimenuitem">Block Cache</span>. Following are a few examples of the reporting capabilities.</p><div class="figure"><a name="d839e1539"></a><p class="title"><b>Figure&nbsp;1.1.&nbsp;Basic Info</b></p><div class="figure-contents"><div class="mediaobject"><table border="0" summary="manufactured viewport for HTML img" cellspacing="0" cellpadding="0" width="100%"><tr><td><img src="images/bc_basic.png" width="100%" alt="Basic Info"></td></tr></table><div class="caption"><p>Shows the cache implementation</p></div></div></div></div><br class="figure-break"><div class="figure"><a name="d839e1548"></a><p class="title"><b>Figure&nbsp;1.2.&nbsp;Config</b></p><div class="figure-contents"><div class="mediaobject"><table border="0" summary="manufactured viewport for HTML img" cellspacing="0" cellpadding="0" width="100%"><tr><td><img src="images/bc_config.png" width="100%" alt="Config"></td></tr></table><div class="caption"><p>Shows all cache configuration options.</p></div></div></div></div><br class="figure-break"><div class="figure"><a name="d839e1557"></a><p class="title"><b>Figure&nbsp;1.3.&nbsp;Stats</b></p><div class="figure-contents"><div class="mediaobject"><table border="0" summary="manufactured viewport for HTML img" cellspacing="0" cellpadding="0" width="100%"><tr><td><img src="images/bc_stats.png" width="100%" alt="Stats"></td></tr></table><div class="caption"><p>Shows statistics about the performance of the cache.</p></div></div></div></div><br class="figure-break"><div class="figure"><a name="d839e1566"></a><p class="title"><b>Figure&nbsp;1.4.&nbsp;L1 and L2</b></p><div class="figure-contents"><div class="mediaobject"><table border="0" summary="manufactured viewport for HTML img" cellspacing="0" cellpadding="0" width="100%"><tr><td><img src="images/bc_l1.png" width="100%" alt="L1 and L2"></td></tr></table><div class="caption"><p>Shows information about the L1 and L2 caches.</p></div></div></div></div><br class="figure-break"><p>This is not an exhaustive list of all the screens and reports available. Have a look in
        the Web UI.</p></div></div><div class="section" title="1.6.&nbsp;Cluster Replication"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="cluster_replication"></a>1.6.&nbsp;Cluster Replication</h2></div></div></div><div class="note" title="Note" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title">Note</h3><p>This information was previously available at <a class="link" href="http://hbase.apache.org/replication.html" target="_top">Cluster Replication</a>. </p></div><p>HBase provides a replication mechanism to copy data between HBase
      clusters. Replication can be used as a disaster recovery solution and as a mechanism for high
      availability. You can also use replication to separate web-facing operations from back-end
      jobs such as MapReduce.</p><p>In terms of architecture, HBase replication is master-push. This takes advantage of the
      fact that each region server has its own write-ahead log (WAL). One master cluster can
      replicate to any number of slave clusters, and each region server replicates its own stream of
      edits. For more information on the different properties of master/slave replication and other
      types of replication, see the article <a class="link" href="http://highscalability.com/blog/2009/8/24/how-google-serves-data-from-multiple-datacenters.html" target="_top">How
        Google Serves Data From Multiple Datacenters</a>.</p><p>Replication is asynchronous, allowing clusters to be geographically distant or to have
      some gaps in availability. This also means that data between master and slave clusters will
      not be instantly consistent. Rows inserted on the master are not immediately available or
      consistent with rows on the slave clusters. rows inserted on the master cluster won&#8217;t be
      available at the same time on the slave clusters. The goal is eventual consistency. </p><p>The replication format used in this design is conceptually the same as the <em class="firstterm"><a class="link" href="http://dev.mysql.com/doc/refman/5.1/en/replication-formats.html" target="_top">statement-based
          replication</a></em> design used by MySQL. Instead of SQL statements, entire
      WALEdits (consisting of multiple cell inserts coming from Put and Delete operations on the
      clients) are replicated in order to maintain atomicity. </p><p>The WALs for each region server must be kept in HDFS as long as they are needed to
      replicate data to any slave cluster. Each region server reads from the oldest log it needs to
      replicate and keeps track of the current position inside ZooKeeper to simplify failure
      recovery. That position, as well as the queue of WALs to process, may be different for every
      slave cluster.</p><p>The clusters participating in replication can be of different sizes. The master
      cluster relies on randomization to attempt to balance the stream of replication on the slave clusters</p><p>HBase supports master/master and cyclic replication as well as replication to multiple
      slaves.</p><div class="figure"><a name="d839e1609"></a><p class="title"><b>Figure&nbsp;1.5.&nbsp;Replication Architecture Overview</b></p><div class="figure-contents"><div class="mediaobject"><img src="images/replication_overview.png" alt="Replication Architecture Overview"><div class="caption"><p>Illustration of the replication architecture in HBase, as described in the prior
            text.</p></div></div></div></div><br class="figure-break"><p title="Enabling and Configuring Replication"><b>Enabling and Configuring Replication.&nbsp;</b>See the <a class="link" href="http://hbase.apache.org/apidocs/org/apache/hadoop/hbase/replication/package-summary.html#requirements" target="_top">
          API documentation for replication</a> for information on enabling and configuring
        replication.</p><div class="section" title="1.6.1.&nbsp;Life of a WAL Edit"><div class="titlepage"><div><div><h3 class="title"><a name="d839e1626"></a>1.6.1.&nbsp;Life of a WAL Edit</h3></div></div></div><p>A single WAL edit goes through several steps in order to be replicated to a slave
        cluster.</p><div class="orderedlist" title="When the slave responds correctly:"><p class="title"><b>When the slave responds correctly:</b></p><ol class="orderedlist" type="1"><li class="listitem"><p>A HBase client uses a Put or Delete operation to manipulate data in HBase.</p></li><li class="listitem"><p>The region server writes the request to the WAL in a way that would allow it to be
            replayed if it were not written successfully.</p></li><li class="listitem"><p>If the changed cell corresponds to a column family that is scoped for replication,
            the edit is added to the queue for replication.</p></li><li class="listitem"><p>In a separate thread, the edit is read from the log, as part of a batch process.
            Only the KeyValues that are eligible for replication are kept. Replicable KeyValues are
            part of a column family whose schema is scoped GLOBAL, are not part of a catalog such as
              <code class="code">hbase:meta</code>, and did not originate from the target slave cluster, in the
            case of cyclic replication.</p></li><li class="listitem"><p>The edit is tagged with the master's UUID and added to a buffer. When the buffer is
            filled, or the reader reaches the end of the file, the buffer is sent to a random region
            server on the slave cluster.</p></li><li class="listitem"><p>The region server reads the edits sequentially and separates them into buffers, one
            buffer per table. After all edits are read, each buffer is flushed using <a class="link" href="http://hbase.apache.org/apidocs/org/apache/hadoop/hbase/client/HTable.html" target="_top">HTable</a>, HBase's normal client. The master's UUID is preserved in the edits
            they are applied, in order to allow for cyclic replication.</p></li><li class="listitem"><p>In the master, the offset for the WAL that is currently being replicated is
            registered in ZooKeeper.</p></li></ol></div><div class="orderedlist" title="When the slave does not respond:"><p class="title"><b>When the slave does not respond:</b></p><ol class="orderedlist" type="1"><li class="listitem"><p>The first three steps, where the edit is inserted, are identical.</p></li><li class="listitem"><p>Again in a separate thread, the region server reads, filters, and edits the log
            edits in the same way as above. The slave region server does not answer the RPC
            call.</p></li><li class="listitem"><p>The master sleeps and tries again a configurable number of times.</p></li><li class="listitem"><p>If the slave region server is still not available, the master selects a new subset
            of region server to replicate to, and tries again to send the buffer of edits.</p></li><li class="listitem"><p>Meanwhile, the WALs are rolled and stored in a queue in ZooKeeper. Logs that are
              <em class="firstterm">archived</em> by their region server, by moving them from the region
            server's log directory to a central log directory, will update their paths in the
            in-memory queue of the replicating thread.</p></li><li class="listitem"><p>When the slave cluster is finally available, the buffer is applied in the same way
            as during normal processing. The master region server will then replicate the backlog of
            logs that accumulated during the outage.</p></li></ol></div><div class="note" title="Spreading Queue Failover Load" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a name="cluster.replication.spreading.load"></a>Spreading Queue Failover Load</h3><p>When replication is active, a subset of RegionServers in the source cluster are
          responsible for shipping edits to the sink. This function must be failed over like all
          other RegionServer functions should a process or node crash. The following configuration
          settings are recommended for maintaining an even distribution of replication activity
          over the remaining live servers in the source cluster: Set
            <code class="code">replication.source.maxretriesmultiplier</code> to
            <code class="literal">300</code> (5 minutes), and
            <code class="code">replication.sleep.before.failover</code> to
            <code class="literal">30000</code> (30 seconds) in the source cluster site configuration.
        </p></div><div class="note" title="Preserving Tags During Replication" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a name="cluster.replication.preserving.tags"></a>Preserving Tags During Replication</h3><p>By default, the codec used for replication between clusters strips tags, such as
          cell-level ACLs, from cells. To prevent the tags from being stripped, you can use a
          different codec which does not strip them. Configure
            <code class="code">hbase.replication.rpc.codec</code> to use
            <code class="literal">org.apache.hadoop.hbase.codec.KeyValueCodecWithTags</code>, on both the
          source and sink RegionServers involved in the replication. This option was introduced in
            <a class="link" href="https://issues.apache.org/jira/browse/HBASE-10322" target="_top">HBASE-10322</a>.</p></div></div><div class="section" title="1.6.2.&nbsp;Replication Internals"><div class="titlepage"><div><div><h3 class="title"><a name="d839e1716"></a>1.6.2.&nbsp;Replication Internals</h3></div></div></div><div class="variablelist"><dl><dt><span class="term">Replication State in ZooKeeper</span></dt><dd><p>HBase replication maintains its state in ZooKeeper. By default, the state is
              contained in the base node <code class="filename">/hbase/replication</code>. This node contains
              two child nodes, the <code class="code">Peers</code> znode and the <code class="code">RS</code> znode.</p><div class="warning" title="Warning" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title">Warning</h3><p>Replication may be disrupted and data loss may occur if you delete the
                replication tree (<code class="filename">/hbase/replication/</code>) from ZooKeeper. This is
                despite the information about invariants at <a class="xref" href="#">???</a>. Follow progress on this issue at <a class="link" href="https://issues.apache.org/jira/browse/HBASE-10295" target="_top">HBASE-10295</a>.</p></div></dd><dt><span class="term">The <code class="code">Peers</code> Znode</span></dt><dd><p>The <code class="code">peers</code> znode is stored in
                <code class="filename">/hbase/replication/peers</code> by default. It consists of a list of
              all peer replication clusters, along with the status of each of them. The value of
              each peer is its cluster key, which is provided in the HBase Shell. The cluster key
              contains a list of ZooKeeper nodes in the cluster's quorum, the client port for the
              ZooKeeper quorum, and the base znode for HBase in HDFS on that cluster.</p><pre class="screen">
/hbase/replication/peers
  /1 [Value: zk1.host.com,zk2.host.com,zk3.host.com:2181:/hbase]
  /2 [Value: zk5.host.com,zk6.host.com,zk7.host.com:2181:/hbase]            
          </pre><p>Each peer has a child znode which indicates whether or not replication is enabled
              on that cluster. These peer-state znodes do not contain any child znodes, but only
              contain a Boolean value. This value is read and maintained by the
                R<code class="code">eplicationPeer.PeerStateTracker</code> class.</p><pre class="screen">
/hbase/replication/peers
  /1/peer-state [Value: ENABLED]
  /2/peer-state [Value: DISABLED]
          </pre></dd><dt><span class="term">The <code class="code">RS</code> Znode</span></dt><dd><p>The <code class="code">rs</code> znode contains a list of WAL logs which need to be replicated.
              This list is divided into a set of queues organized by region server and the peer
              cluster the region server is shipping the logs to. The rs znode has one child znode
              for each region server in the cluster. The child znode name is the region server's
              hostname, client port, and start code. This list includes both live and dead region
              servers.</p><pre class="screen">
/hbase/replication/rs
  /hostname.example.org,6020,1234
  /hostname2.example.org,6020,2856            
          </pre><p>Each <code class="code">rs</code> znode contains a list of WAL replication queues, one queue
              for each peer cluster it replicates to. These queues are represented by child znodes
              named by the cluster ID of the peer cluster they represent.</p><pre class="screen">
/hbase/replication/rs
  /hostname.example.org,6020,1234
    /1
    /2            
          </pre><p>Each queue has one child znode for each WAL log that still needs to be replicated.
              the value of these child znodes is the last position that was replicated. This
              position is updated each time a WAL log is replicated.</p><pre class="screen">
/hbase/replication/rs
  /hostname.example.org,6020,1234
    /1
      23522342.23422 [VALUE: 254]
      12340993.22342 [VALUE: 0]            
          </pre></dd></dl></div></div><div class="section" title="1.6.3.&nbsp;Replication Configuration Options"><div class="titlepage"><div><div><h3 class="title"><a name="d839e1795"></a>1.6.3.&nbsp;Replication Configuration Options</h3></div></div></div><div class="informaltable"><table border="1"><colgroup><col><col><col></colgroup><thead><tr><th>Option</th><th>Description</th><th>Default</th></tr></thead><tbody><tr><td><p><code class="code">zookeeper.znode.parent</code></p></td><td><p>The name of the base ZooKeeper znode used for HBase</p></td><td><p><code class="literal">/hbase</code></p></td></tr><tr><td><p><code class="code">zookeeper.znode.replication</code></p></td><td><p>The name of the base znode used for replication</p></td><td><p><code class="literal">replication</code></p></td></tr><tr><td><p><code class="code">zookeeper.znode.replication.peers</code></p></td><td><p>The name of the <code class="code">peer</code> znode</p></td><td><p><code class="literal">peers</code></p></td></tr><tr><td><p><code class="code">zookeeper.znode.replication.peers.state</code></p></td><td><p>The name of <code class="code">peer-state</code> znode</p></td><td><p><code class="literal">peer-state</code></p></td></tr><tr><td><p><code class="code">zookeeper.znode.replication.rs</code></p></td><td><p>The name of the <code class="code">rs</code> znode</p></td><td><p><code class="literal">rs</code></p></td></tr><tr><td><p><code class="code">hbase.replication</code></p></td><td><p>Whether replication is enabled or disabled on a given cluster</p></td><td><p><code class="literal">false</code></p></td></tr><tr><td><p><code class="code">eplication.sleep.before.failover</code></p></td><td><p>How many milliseconds a worker should sleep before attempting to replicate
              a dead region server's WAL queues.</p></td><td><p><code class="literal"></code></p></td></tr><tr><td><p><code class="code">replication.executor.workers</code></p></td><td><p>The number of region servers a given region server should attempt to
                failover simultaneously.</p></td><td><p><code class="literal">1</code></p></td></tr></tbody></table></div></div><div class="section" title="1.6.4.&nbsp;Replication Implementation Details"><div class="titlepage"><div><div><h3 class="title"><a name="d839e1913"></a>1.6.4.&nbsp;Replication Implementation Details</h3></div></div></div><p title="Choosing Region Servers to Replicate To"><b>Choosing Region Servers to Replicate To.&nbsp;</b>When a master cluster region server initiates a replication source to a slave cluster,
          it first connects to the slave's ZooKeeper ensemble using the provided cluster key . It
          then scans the <code class="filename">rs/</code> directory to discover all the available sinks
          (region servers that are accepting incoming streams of edits to replicate) and randomly
          chooses a subset of them using a configured ratio which has a default value of 10%. For
          example, if a slave cluster has 150 machines, 15 will be chosen as potential recipient for
          edits that this master cluster region server sends. Because this selection is performed by
          each master region server, the probability that all slave region servers are used is very
          high, and this method works for clusters of any size. For example, a master cluster of 10
          machines replicating to a slave cluster of 5 machines with a ratio of 10% causes the
          master cluster region servers to choose one machine each at random.</p><p>A ZooKeeper watcher is placed on the
            <code class="filename">${<em class="replaceable"><code>zookeeper.znode.parent</code></em>}/rs</code> node of the
        slave cluster by each of the master cluster's region servers. This watch is used to monitor
        changes in the composition of the slave cluster. When nodes are removed from the slave
        cluster, or if nodes go down or come back up, the master cluster's region servers will
        respond by selecting a new pool of slave region servers to replicate to.</p><p title="Keeping Track of Logs"><b>Keeping Track of Logs.&nbsp;</b>Each master cluster region server has its own znode in the replication znodes
          hierarchy. It contains one znode per peer cluster (if 5 slave clusters, 5 znodes are
          created), and each of these contain a queue of WALs to process. Each of these queues will
          track the WALs created by that region server, but they can differ in size. For example, if
          one slave cluster becomes unavailable for some time, the WALs should not be deleted, so
          they need to stay in the queue while the others are processed. See <a class="xref" href="#rs.failover.details" title="Region Server Failover">Region Server Failover</a> for an example.</p><p>When a source is instantiated, it contains the current WAL that the region server is
        writing to. During log rolling, the new file is added to the queue of each slave cluster's
        znode just before it is made available. This ensures that all the sources are aware that a
        new log exists before the region server is able to append edits into it, but this operations
        is now more expensive. The queue items are discarded when the replication thread cannot read
        more entries from a file (because it reached the end of the last block) and there are other
        files in the queue. This means that if a source is up to date and replicates from the log
        that the region server writes to, reading up to the "end" of the current file will not
        delete the item in the queue.</p><p>A log can be archived if it is no longer used or if the number of logs exceeds
          <code class="code">hbase.regionserver.maxlogs</code> because the insertion rate is faster than regions
        are flushed. When a log is archived, the source threads are notified that the path for that
        log changed. If a particular source has already finished with an archived log, it will just
        ignore the message. If the log is in the queue, the path will be updated in memory. If the
        log is currently being replicated, the change will be done atomically so that the reader
        doesn't attempt to open the file when has already been moved. Because moving a file is a
        NameNode operation , if the reader is currently reading the log, it won't generate any
        exception.</p><p title="Reading, Filtering and Sending Edits"><b>Reading, Filtering and Sending Edits.&nbsp;</b>By default, a source attempts to read from a WAL and ship log entries to a sink as
          quickly as possible. Speed is limited by the filtering of log entries Only KeyValues that
          are scoped GLOBAL and that do not belong to catalog tables will be retained. Speed is also
          limited by total size of the list of edits to replicate per slave, which is limited to 64
          MB by default. With this configuration, a master cluster region server with three slaves
          would use at most 192 MB to store data to replicate. This does not account for the data
          which was filtered but not garbage collected.</p><p>Once the maximum size of edits has been buffered or the reader reaces the end of the
        WAL, the source thread stops reading and chooses at random a sink to replicate to (from the
        list that was generated by keeping only a subset of slave region servers). It directly
        issues a RPC to the chosen region server and waits for the method to return. If the RPC was
        successful, the source determines whether the current file has been emptied or it contains
        more data which needs to be read. If the file has been emptied, the source deletes the znode
        in the queue. Otherwise, it registers the new offset in the log's znode. If the RPC threw an
        exception, the source will retry 10 times before trying to find a different sink.</p><p title="Cleaning Logs"><b>Cleaning Logs.&nbsp;</b>If replication is not enabled, the master's log-cleaning thread deletes old logs using
          a configured TTL. This TTL-based method does not work well with replication, because
          archived logs which have exceeded their TTL may still be in a queue. The default behavior
          is augmented so that if a log is past its TTL, the cleaning thread looks up every queue
          until it finds the log, while caching queues it has found. If the log is not found in any
          queues, the log will be deleted. The next time the cleaning process needs to look for a
          log, it starts by using its cached list.</p><p title="Region Server Failover"><a name="rs.failover.details"></a><b>Region Server Failover.&nbsp;</b>When no region servers are failing, keeping track of the logs in ZooKeeper adds no
          value. Unfortunately, region servers do fail, and since ZooKeeper is highly available, it
          is useful for managing the transfer of the queues in the event of a failure.</p><p>Each of the master cluster region servers keeps a watcher on every other region server,
        in order to be notified when one dies (just as the master does). When a failure happens,
        they all race to create a znode called <code class="literal">lock</code> inside the dead region
        server's znode that contains its queues. The region server that creates it successfully then
        transfers all the queues to its own znode, one at a time since ZooKeeper does not support
        renaming queues. After queues are all transferred, they are deleted from the old location.
        The znodes that were recovered are renamed with the ID of the slave cluster appended with
        the name of the dead server.</p><p>Next, the master cluster region server creates one new source thread per copied queue,
        and each of the source threads follows the read/filter/ship pattern. The main difference is
        that those queues will never receive new data, since they do not belong to their new region
        server. When the reader hits the end of the last log, the queue's znode is deleted and the
        master cluster region server closes that replication source.</p><p>Given a master cluster with 3 region servers replicating to a single slave with id
          <code class="literal">2</code>, the following hierarchy represents what the znodes layout could be
        at some point in time. The region servers' znodes all contain a <code class="literal">peers</code>
        znode which contains a single queue. The znode names in the queues represent the actual file
        names on HDFS in the form
            <code class="literal"><em class="replaceable"><code>address</code></em>,<em class="replaceable"><code>port</code></em>.<em class="replaceable"><code>timestamp</code></em></code>.</p><pre class="screen">
/hbase/replication/rs/
  1.1.1.1,60020,123456780/
    2/
      1.1.1.1,60020.1234  (Contains a position)
      1.1.1.1,60020.1265
  1.1.1.2,60020,123456790/
    2/
      1.1.1.2,60020.1214  (Contains a position)
      1.1.1.2,60020.1248
      1.1.1.2,60020.1312
  1.1.1.3,60020,    123456630/
    2/
      1.1.1.3,60020.1280  (Contains a position)            
          </pre><p>Assume that 1.1.1.2 loses its ZooKeeper session. The survivors will race to create a
        lock, and, arbitrarily, 1.1.1.3 wins. It will then start transferring all the queues to its
        local peers znode by appending the name of the dead server. Right before 1.1.1.3 is able to
        clean up the old znodes, the layout will look like the following:</p><pre class="screen">
/hbase/replication/rs/
  1.1.1.1,60020,123456780/
    2/
      1.1.1.1,60020.1234  (Contains a position)
      1.1.1.1,60020.1265
  1.1.1.2,60020,123456790/
    lock
    2/
      1.1.1.2,60020.1214  (Contains a position)
      1.1.1.2,60020.1248
      1.1.1.2,60020.1312
  1.1.1.3,60020,123456630/
    2/
      1.1.1.3,60020.1280  (Contains a position)

    2-1.1.1.2,60020,123456790/
      1.1.1.2,60020.1214  (Contains a position)
      1.1.1.2,60020.1248
      1.1.1.2,60020.1312            
          </pre><p>Some time later, but before 1.1.1.3 is able to finish replicating the last WAL from
        1.1.1.2, it dies too. Some new logs were also created in the normal queues. The last region
        server will then try to lock 1.1.1.3's znode and will begin transferring all the queues. The
        new layout will be:</p><pre class="screen">
/hbase/replication/rs/
  1.1.1.1,60020,123456780/
    2/
      1.1.1.1,60020.1378  (Contains a position)

    2-1.1.1.3,60020,123456630/
      1.1.1.3,60020.1325  (Contains a position)
      1.1.1.3,60020.1401

    2-1.1.1.2,60020,123456790-1.1.1.3,60020,123456630/
      1.1.1.2,60020.1312  (Contains a position)
  1.1.1.3,60020,123456630/
    lock
    2/
      1.1.1.3,60020.1325  (Contains a position)
      1.1.1.3,60020.1401

    2-1.1.1.2,60020,123456790/
      1.1.1.2,60020.1312  (Contains a position)            
          </pre><p title="Replication Metrics"><b>Replication Metrics.&nbsp;</b>The following metrics are exposed at the global region server level and (since HBase
          0.95) at the peer level:</p><div class="variablelist"><dl><dt><span class="term"><code class="code">source.sizeOfLogQueue</code></span></dt><dd><p>number of WALs to process (excludes the one which is being processed) at the
              Replication source</p></dd><dt><span class="term"><code class="code">source.shippedOps</code></span></dt><dd><p>number of mutations shipped</p></dd><dt><span class="term"><code class="code">source.logEditsRead</code></span></dt><dd><p>number of mutations read from HLogs at the replication source</p></dd><dt><span class="term"><code class="code">source.ageOfLastShippedOp</code></span></dt><dd><p>age of last batch that was shipped by the replication source</p></dd></dl></div></div></div><div class="section" title="1.7.&nbsp;HBase Backup"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="ops.backup"></a>1.7.&nbsp;HBase Backup</h2></div></div></div><p>There are two broad strategies for performing HBase backups: backing up with a full
      cluster shutdown, and backing up on a live cluster. Each approach has pros and cons. </p><p>For additional information, see <a class="link" href="http://blog.sematext.com/2011/03/11/hbase-backup-options/" target="_top">HBase Backup
        Options</a> over on the Sematext Blog. </p><div class="section" title="1.7.1.&nbsp;Full Shutdown Backup"><div class="titlepage"><div><div><h3 class="title"><a name="ops.backup.fullshutdown"></a>1.7.1.&nbsp;Full Shutdown Backup</h3></div></div></div><p>Some environments can tolerate a periodic full shutdown of their HBase cluster, for
        example if it is being used a back-end analytic capacity and not serving front-end
        web-pages. The benefits are that the NameNode/Master are RegionServers are down, so there is
        no chance of missing any in-flight changes to either StoreFiles or metadata. The obvious con
        is that the cluster is down. The steps include: </p><div class="section" title="1.7.1.1.&nbsp;Stop HBase"><div class="titlepage"><div><div><h4 class="title"><a name="ops.backup.fullshutdown.stop"></a>1.7.1.1.&nbsp;Stop HBase</h4></div></div></div><p> </p></div><div class="section" title="1.7.1.2.&nbsp;Distcp"><div class="titlepage"><div><div><h4 class="title"><a name="ops.backup.fullshutdown.distcp"></a>1.7.1.2.&nbsp;Distcp</h4></div></div></div><p>Distcp could be used to either copy the contents of the HBase directory in HDFS to
          either the same cluster in another directory, or to a different cluster. </p><p>Note: Distcp works in this situation because the cluster is down and there are no
          in-flight edits to files. Distcp-ing of files in the HBase directory is not generally
          recommended on a live cluster. </p></div><div class="section" title="1.7.1.3.&nbsp;Restore (if needed)"><div class="titlepage"><div><div><h4 class="title"><a name="ops.backup.fullshutdown.restore"></a>1.7.1.3.&nbsp;Restore (if needed)</h4></div></div></div><p>The backup of the hbase directory from HDFS is copied onto the 'real' hbase directory
          via distcp. The act of copying these files creates new HDFS metadata, which is why a
          restore of the NameNode edits from the time of the HBase backup isn't required for this
          kind of restore, because it's a restore (via distcp) of a specific HDFS directory (i.e.,
          the HBase part) not the entire HDFS file-system. </p></div></div><div class="section" title="1.7.2.&nbsp;Live Cluster Backup - Replication"><div class="titlepage"><div><div><h3 class="title"><a name="ops.backup.live.replication"></a>1.7.2.&nbsp;Live Cluster Backup - Replication</h3></div></div></div><p>This approach assumes that there is a second cluster. See the HBase page on <a class="link" href="http://hbase.apache.org/replication.html" target="_top">replication</a> for more
        information. </p></div><div class="section" title="1.7.3.&nbsp;Live Cluster Backup - CopyTable"><div class="titlepage"><div><div><h3 class="title"><a name="ops.backup.live.copytable"></a>1.7.3.&nbsp;Live Cluster Backup - CopyTable</h3></div></div></div><p>The <a class="xref" href="#copytable" title="1.1.8.&nbsp;CopyTable">Section&nbsp;1.1.8, &#8220;CopyTable&#8221;</a> utility could either be used to copy data from one table to another
        on the same cluster, or to copy data to another table on another cluster. </p><p>Since the cluster is up, there is a risk that edits could be missed in the copy process.
      </p></div><div class="section" title="1.7.4.&nbsp;Live Cluster Backup - Export"><div class="titlepage"><div><div><h3 class="title"><a name="ops.backup.live.export"></a>1.7.4.&nbsp;Live Cluster Backup - Export</h3></div></div></div><p>The <a class="xref" href="#export" title="1.1.9.&nbsp;Export">Section&nbsp;1.1.9, &#8220;Export&#8221;</a> approach dumps the content of a table to HDFS on the same cluster. To
        restore the data, the <a class="xref" href="#import" title="1.1.10.&nbsp;Import">Section&nbsp;1.1.10, &#8220;Import&#8221;</a> utility would be used. </p><p>Since the cluster is up, there is a risk that edits could be missed in the export
        process. </p></div></div><div class="section" title="1.8.&nbsp;HBase Snapshots"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="ops.snapshots"></a>1.8.&nbsp;HBase Snapshots</h2></div></div></div><p>HBase Snapshots allow you to take a snapshot of a table without too much impact on Region
      Servers. Snapshot, Clone and restore operations don't involve data copying. Also, Exporting
      the snapshot to another cluster doesn't have impact on the Region Servers. </p><p>Prior to version 0.94.6, the only way to backup or to clone a table is to use
      CopyTable/ExportTable, or to copy all the hfiles in HDFS after disabling the table. The
      disadvantages of these methods are that you can degrade region server performance (Copy/Export
      Table) or you need to disable the table, that means no reads or writes; and this is usually
      unacceptable. </p><div class="section" title="1.8.1.&nbsp;Configuration"><div class="titlepage"><div><div><h3 class="title"><a name="ops.snapshots.configuration"></a>1.8.1.&nbsp;Configuration</h3></div></div></div><p>To turn on the snapshot support just set the <code class="varname">hbase.snapshot.enabled</code>
        property to true. (Snapshots are enabled by default in 0.95+ and off by default in
        0.94.6+)</p><pre class="programlisting">
  &lt;property&gt;
    &lt;name&gt;hbase.snapshot.enabled&lt;/name&gt;
    &lt;value&gt;true&lt;/value&gt;
  &lt;/property&gt;
        </pre></div><div class="section" title="1.8.2.&nbsp;Take a Snapshot"><div class="titlepage"><div><div><h3 class="title"><a name="ops.snapshots.takeasnapshot"></a>1.8.2.&nbsp;Take a Snapshot</h3></div></div></div><p>You can take a snapshot of a table regardless of whether it is enabled or disabled. The
        snapshot operation doesn't involve any data copying.</p><pre class="screen">
$ ./bin/hbase shell
hbase&gt; snapshot 'myTable', 'myTableSnapshot-122112'
        </pre><p title="Take a Snapshot Without Flushing"><b>Take a Snapshot Without Flushing.&nbsp;</b>The default behavior is to perform a flush of data in memory before the snapshot is
          taken. This means that data in memory is included in the snapshot. In most cases, this is
          the desired behavior. However, if your set-up can tolerate data in memory being excluded
          from the snapshot, you can use the <code class="option">SKIP_FLUSH</code> option of the
            <span class="command"><strong>snapshot</strong></span> command to disable and flushing while taking the
          snapshot.</p><pre class="screen">hbase&gt; snapshot 'mytable', 'snapshot123', {SKIP_FLUSH =&gt; true}</pre><div class="warning" title="Warning" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title">Warning</h3><p>There is no way to determine or predict whether a very concurrent insert or update
          will be included in a given snapshot, whether flushing is enabled or disabled. A snapshot
          is only a representation of a table during a window of time. The amount of time the
          snapshot operation will take to reach each Region Server may vary from a few seconds to a
          minute, depending on the resource load and speed of the hardware or network, among other
          factors. There is also no way to know whether a given insert or update is in memory or has
          been flushed.</p></div></div><div class="section" title="1.8.3.&nbsp;Listing Snapshots"><div class="titlepage"><div><div><h3 class="title"><a name="ops.snapshots.list"></a>1.8.3.&nbsp;Listing Snapshots</h3></div></div></div><p>List all snapshots taken (by printing the names and relative information).</p><pre class="screen">
$ ./bin/hbase shell
hbase&gt; list_snapshots
        </pre></div><div class="section" title="1.8.4.&nbsp;Deleting Snapshots"><div class="titlepage"><div><div><h3 class="title"><a name="ops.snapshots.delete"></a>1.8.4.&nbsp;Deleting Snapshots</h3></div></div></div><p>You can remove a snapshot, and the files retained for that snapshot will be removed if
        no longer needed.</p><pre class="screen">
$ ./bin/hbase shell
hbase&gt; delete_snapshot 'myTableSnapshot-122112'
        </pre></div><div class="section" title="1.8.5.&nbsp;Clone a table from snapshot"><div class="titlepage"><div><div><h3 class="title"><a name="ops.snapshots.clone"></a>1.8.5.&nbsp;Clone a table from snapshot</h3></div></div></div><p>From a snapshot you can create a new table (clone operation) with the same data that you
        had when the snapshot was taken. The clone operation, doesn't involve data copies, and a
        change to the cloned table doesn't impact the snapshot or the original table.</p><pre class="screen">
$ ./bin/hbase shell
hbase&gt; clone_snapshot 'myTableSnapshot-122112', 'myNewTestTable'
        </pre></div><div class="section" title="1.8.6.&nbsp;Restore a snapshot"><div class="titlepage"><div><div><h3 class="title"><a name="ops.snapshots.restore"></a>1.8.6.&nbsp;Restore a snapshot</h3></div></div></div><p>The restore operation requires the table to be disabled, and the table will be restored
        to the state at the time when the snapshot was taken, changing both data and schema if
        required.</p><pre class="screen">
$ ./bin/hbase shell
hbase&gt; disable 'myTable'
hbase&gt; restore_snapshot 'myTableSnapshot-122112'
        </pre><div class="note" title="Note" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title">Note</h3><p>Since Replication works at log level and snapshots at file-system level, after a
          restore, the replicas will be in a different state from the master. If you want to use
          restore, you need to stop replication and redo the bootstrap. </p></div><p>In case of partial data-loss due to misbehaving client, instead of a full restore that
        requires the table to be disabled, you can clone the table from the snapshot and use a
        Map-Reduce job to copy the data that you need, from the clone to the main one. </p></div><div class="section" title="1.8.7.&nbsp;Snapshots operations and ACLs"><div class="titlepage"><div><div><h3 class="title"><a name="ops.snapshots.acls"></a>1.8.7.&nbsp;Snapshots operations and ACLs</h3></div></div></div><p>If you are using security with the AccessController Coprocessor (See <a class="xref" href="#">???</a>), only a global administrator can take,
        clone, or restore a snapshot, and these actions do not capture the ACL rights. This means
        that restoring a table preserves the ACL rights of the existing table, while cloning a table
        creates a new table that has no ACL rights until the administrator adds them.</p></div><div class="section" title="1.8.8.&nbsp;Export to another cluster"><div class="titlepage"><div><div><h3 class="title"><a name="ops.snapshots.export"></a>1.8.8.&nbsp;Export to another cluster</h3></div></div></div><p>The ExportSnapshot tool copies all the data related to a snapshot (hfiles, logs,
        snapshot metadata) to another cluster. The tool executes a Map-Reduce job, similar to
        distcp, to copy files between the two clusters, and since it works at file-system level the
        hbase cluster does not have to be online.</p><p>To copy a snapshot called MySnapshot to an HBase cluster srv2 (hdfs:///srv2:8082/hbase)
        using 16 mappers:</p><pre class="programlisting">$ bin/hbase class org.apache.hadoop.hbase.snapshot.ExportSnapshot -snapshot MySnapshot -copy-to hdfs://srv2:8082/hbase -mappers 16</pre><p title="Limiting Bandwidth Consumption"><b>Limiting Bandwidth Consumption.&nbsp;</b>You can limit the bandwidth consumption when exporting a snapshot, by specifying the
            <code class="code">-bandwidth</code> parameter, which expects an integer representing megabytes per
          second. The following example limits the above example to 200 MB/sec.</p><pre class="programlisting">$ bin/hbase class org.apache.hadoop.hbase.snapshot.ExportSnapshot -snapshot MySnapshot -copy-to hdfs://srv2:8082/hbase -mappers 16 -bandwidth 200</pre></div></div><div class="section" title="1.9.&nbsp;Capacity Planning and Region Sizing"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="ops.capacity"></a>1.9.&nbsp;Capacity Planning and Region Sizing</h2></div></div></div><p>There are several considerations when planning the capacity for an HBase cluster and
      performing the initial configuration. Start with a solid understanding of how HBase handles
      data internally.</p><div class="section" title="1.9.1.&nbsp;Node count and hardware/VM configuration"><div class="titlepage"><div><div><h3 class="title"><a name="ops.capacity.nodes"></a>1.9.1.&nbsp;Node count and hardware/VM configuration</h3></div></div></div><div class="section" title="1.9.1.1.&nbsp;Physical data size"><div class="titlepage"><div><div><h4 class="title"><a name="ops.capacity.nodes.datasize"></a>1.9.1.1.&nbsp;Physical data size</h4></div></div></div><p>Physical data size on disk is distinct from logical size of your data and is affected
          by the following: </p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem"><p>Increased by HBase overhead</p><div class="itemizedlist"><ul class="itemizedlist" type="circle"><li class="listitem"><p>See <a class="xref" href="#">???</a> and <a class="xref" href="#">???</a>. At least 24 bytes per key-value (cell), can be more. Small
                  keys/values means more relative overhead.</p></li><li class="listitem"><p>KeyValue instances are aggregated into blocks, which are indexed. Indexes also
                  have to be stored. Blocksize is configurable on a per-ColumnFamily basis. See <a class="xref" href="#">???</a>.</p></li></ul></div></li><li class="listitem"><p>Decreased by <a class="xref" href="#">???</a> and data block encoding, depending on data. See
              also <a class="link" href="http://search-hadoop.com/m/lL12B1PFVhp1" target="_top">this thread</a>. You might
              want to test what compression and encoding (if any) make sense for your data.</p></li><li class="listitem"><p>Increased by size of region server <a class="xref" href="#">???</a> (usually fixed and negligible - less than half of RS
              memory size, per RS).</p></li><li class="listitem"><p>Increased by HDFS replication - usually x3.</p></li></ul></div><p>Aside from the disk space necessary to store the data, one RS may not be able to serve
          arbitrarily large amounts of data due to some practical limits on region count and size
          (see <a class="xref" href="#ops.capacity.regions" title="1.9.2.&nbsp;Determining region count and size">below</a>).</p></div><div class="section" title="1.9.1.2.&nbsp;Read/Write throughput"><div class="titlepage"><div><div><h4 class="title"><a name="ops.capacity.nodes.throughput"></a>1.9.1.2.&nbsp;Read/Write throughput</h4></div></div></div><p>Number of nodes can also be driven by required thoughput for reads and/or writes. The
          throughput one can get per node depends a lot on data (esp. key/value sizes) and request
          patterns, as well as node and system configuration. Planning should be done for peak load
          if it is likely that the load would be the main driver of the increase of the node count.
          PerformanceEvaluation and <a class="xref" href="#">???</a> tools can be used to test single node or a test
          cluster.</p><p>For write, usually 5-15Mb/s per RS can be expected, since every region server has only
          one active WAL. There's no good estimate for reads, as it depends vastly on data,
          requests, and cache hit rate. <a class="xref" href="#">???</a> might be helpful.</p></div><div class="section" title="1.9.1.3.&nbsp;JVM GC limitations"><div class="titlepage"><div><div><h4 class="title"><a name="ops.capacity.nodes.gc"></a>1.9.1.3.&nbsp;JVM GC limitations</h4></div></div></div><p>RS cannot currently utilize very large heap due to cost of GC. There's also no good
          way of running multiple RS-es per server (other than running several VMs per machine).
          Thus, ~20-24Gb or less memory dedicated to one RS is recommended. GC tuning is required
          for large heap sizes. See <a class="xref" href="#">???</a>, <a class="xref" href="#">???</a> and elsewhere (TODO: where?)</p></div></div><div class="section" title="1.9.2.&nbsp;Determining region count and size"><div class="titlepage"><div><div><h3 class="title"><a name="ops.capacity.regions"></a>1.9.2.&nbsp;Determining region count and size</h3></div></div></div><p>Generally less regions makes for a smoother running cluster (you can always manually
        split the big regions later (if necessary) to spread the data, or request load, over the
        cluster); 20-200 regions per RS is a reasonable range. The number of regions cannot be
        configured directly (unless you go for fully <a class="xref" href="#">???</a>); adjust the region size to achieve the target
        region size given table size.</p><p>When configuring regions for multiple tables, note that most region settings can be set
        on a per-table basis via <a class="link" href="http://hbase.apache.org/apidocs/org/apache/hadoop/hbase/HTableDescriptor.html" target="_top">HTableDescriptor</a>,
        as well as shell commands. These settings will override the ones in
          <code class="varname">hbase-site.xml</code>. That is useful if your tables have different
        workloads/use cases.</p><p>Also note that in the discussion of region sizes here, <span class="bold"><strong>HDFS replication factor is not (and should not be) taken into account, whereas
          other factors <a class="xref" href="#ops.capacity.nodes.datasize" title="1.9.1.1.&nbsp;Physical data size">above</a> should be.</strong></span> So, if your data is compressed and
        replicated 3 ways by HDFS, "9 Gb region" means 9 Gb of compressed data. HDFS replication
        factor only affects your disk usage and is invisible to most HBase code.</p><div class="section" title="1.9.2.1.&nbsp;Viewing the Current Number of Regions"><div class="titlepage"><div><div><h4 class="title"><a name="d839e2289"></a>1.9.2.1.&nbsp;Viewing the Current Number of Regions</h4></div></div></div><p>You can view the current number of regions for a given table using the HMaster UI. In
          the <span class="guilabel">Tables</span> section, the number of online regions for each table is
          listed in the <span class="guilabel">Online Regions</span> column. This total only includes the
          in-memory state and does not include disabled or offline regions. If you do not want to
          use the HMaster UI, you can determine the number of regions by counting the number of
          subdirectories of the /hbase/&lt;table&gt;/ subdirectories in HDFS, or by running the
            <span class="command"><strong>bin/hbase hbck</strong></span> command. Each of these methods may return a slightly
          different number, depending on the status of each region.</p></div><div class="section" title="1.9.2.2.&nbsp;Number of regions per RS - upper bound"><div class="titlepage"><div><div><h4 class="title"><a name="ops.capacity.regions.count"></a>1.9.2.2.&nbsp;Number of regions per RS - upper bound</h4></div></div></div><p>In production scenarios, where you have a lot of data, you are normally concerned with
          the maximum number of regions you can have per server. <a class="xref" href="#">???</a>
          has technical discussion on the subject. Basically, the maximum number of regions is
          mostly determined by memstore memory usage. Each region has its own memstores; these grow
          up to a configurable size; usually in 128-256 MB range, see <a class="xref" href="#">???</a>. One memstore exists per column family (so
          there's only one per region if there's one CF in the table). The RS dedicates some
          fraction of total memory to its memstores (see <a class="xref" href="#">???</a>). If this memory is exceeded (too
          much memstore usage), it can cause undesirable consequences such as unresponsive server or
          compaction storms. A good starting point for the number of regions per RS (assuming one
          table) is:</p><pre class="programlisting">((RS memory) * (total memstore fraction)) / ((memstore size)*(# column families))</pre><p>This formula is pseudo-code. Here are two formulas using the actual tunable
          parameters, first for HBase 0.98+ and second for HBase 0.94.x.</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem"><p>HBase 0.98.x:<code class="code">((RS Xmx) * hbase.regionserver.global.memstore.size) /
                (hbase.hregion.memstore.flush.size * (# column families))</code></p></li><li class="listitem"><p>HBase 0.94.x:<code class="code">((RS Xmx) * hbase.regionserver.global.memstore.upperLimit) /
                (hbase.hregion.memstore.flush.size * (# column families))</code></p></li></ul></div><p>If a given RegionServer has 16 GB of RAM, with default settings, the formula works out
          to 16384*0.4/128 ~ 51 regions per RS is a starting point. The formula can be extended to
          multiple tables; if they all have the same configuration, just use the total number of
          families.</p><p>This number can be adjusted; the formula above assumes all your regions are filled at
          approximately the same rate. If only a fraction of your regions are going to be actively
          written to, you can divide the result by that fraction to get a larger region count. Then,
          even if all regions are written to, all region memstores are not filled evenly, and
          eventually jitter appears even if they are (due to limited number of concurrent flushes).
          Thus, one can have as many as 2-3 times more regions than the starting point; however,
          increased numbers carry increased risk.</p><p>For write-heavy workload, memstore fraction can be increased in configuration at the
          expense of block cache; this will also allow one to have more regions.</p></div><div class="section" title="1.9.2.3.&nbsp;Number of regions per RS - lower bound"><div class="titlepage"><div><div><h4 class="title"><a name="ops.capacity.regions.mincount"></a>1.9.2.3.&nbsp;Number of regions per RS - lower bound</h4></div></div></div><p>HBase scales by having regions across many servers. Thus if you have 2 regions for
          16GB data, on a 20 node machine your data will be concentrated on just a few machines -
          nearly the entire cluster will be idle. This really can't be stressed enough, since a
          common problem is loading 200MB data into HBase and then wondering why your awesome 10
          node cluster isn't doing anything.</p><p>On the other hand, if you have a very large amount of data, you may also want to go
          for a larger number of regions to avoid having regions that are too large.</p></div><div class="section" title="1.9.2.4.&nbsp;Maximum region size"><div class="titlepage"><div><div><h4 class="title"><a name="ops.capacity.regions.size"></a>1.9.2.4.&nbsp;Maximum region size</h4></div></div></div><p>For large tables in production scenarios, maximum region size is mostly limited by
          compactions - very large compactions, esp. major, can degrade cluster performance.
          Currently, the recommended maximum region size is 10-20Gb, and 5-10Gb is optimal. For
          older 0.90.x codebase, the upper-bound of regionsize is about 4Gb, with a default of
          256Mb.</p><p>The size at which the region is split into two is generally configured via <a class="xref" href="#">???</a>; for details, see <a class="xref" href="#">???</a>.</p><p>If you cannot estimate the size of your tables well, when starting off, it's probably
          best to stick to the default region size, perhaps going smaller for hot tables (or
          manually split hot regions to spread the load over the cluster), or go with larger region
          sizes if your cell sizes tend to be largish (100k and up).</p><p>In HBase 0.98, experimental stripe compactions feature was added that would allow for
          larger regions, especially for log data. See <a class="xref" href="#">???</a>.</p></div><div class="section" title="1.9.2.5.&nbsp;Total data size per region server"><div class="titlepage"><div><div><h4 class="title"><a name="ops.capacity.regions.total"></a>1.9.2.5.&nbsp;Total data size per region server</h4></div></div></div><p>According to above numbers for region size and number of regions per region server, in
          an optimistic estimate 10 GB x 100 regions per RS will give up to 1TB served per region
          server, which is in line with some of the reported multi-PB use cases. However, it is
          important to think about the data vs cache size ratio at the RS level. With 1TB of data
          per server and 10 GB block cache, only 1% of the data will be cached, which may barely
          cover all block indices.</p></div></div><div class="section" title="1.9.3.&nbsp;Initial configuration and tuning"><div class="titlepage"><div><div><h3 class="title"><a name="ops.capacity.config"></a>1.9.3.&nbsp;Initial configuration and tuning</h3></div></div></div><p>First, see <a class="xref" href="#">???</a>. Note that some configurations, more than others,
        depend on specific scenarios. Pay special attention to:</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem"><p><a class="xref" href="#">???</a> - request handler thread count, vital
            for high-throughput workloads.</p></li><li class="listitem"><p><a class="xref" href="#">???</a> - the blocking number of WAL files depends on your memstore
            configuration and should be set accordingly to prevent potential blocking when doing
            high volume of writes.</p></li></ul></div><p>Then, there are some considerations when setting up your cluster and tables.</p><div class="section" title="1.9.3.1.&nbsp;Compactions"><div class="titlepage"><div><div><h4 class="title"><a name="ops.capacity.config.compactions"></a>1.9.3.1.&nbsp;Compactions</h4></div></div></div><p>Depending on read/write volume and latency requirements, optimal compaction settings
          may be different. See <a class="xref" href="#">???</a> for some details.</p><p>When provisioning for large data sizes, however, it's good to keep in mind that
          compactions can affect write throughput. Thus, for write-intensive workloads, you may opt
          for less frequent compactions and more store files per regions. Minimum number of files
          for compactions (<code class="varname">hbase.hstore.compaction.min</code>) can be set to higher
          value; <a class="xref" href="#">???</a> should also be increased, as more files
          might accumulate in such case. You may also consider manually managing compactions: <a class="xref" href="#">???</a></p></div><div class="section" title="1.9.3.2.&nbsp;Pre-splitting the table"><div class="titlepage"><div><div><h4 class="title"><a name="ops.capacity.config.presplit"></a>1.9.3.2.&nbsp;Pre-splitting the table</h4></div></div></div><p>Based on the target number of the regions per RS (see <a class="xref" href="#ops.capacity.regions.count" title="1.9.2.2.&nbsp;Number of regions per RS - upper bound">above</a>) and number of RSes, one can pre-split the table at
          creation time. This would both avoid some costly splitting as the table starts to fill up,
          and ensure that the table starts out already distributed across many servers.</p><p>If the table is expected to grow large enough to justify that, at least one region per
          RS should be created. It is not recommended to split immediately into the full target
          number of regions (e.g. 50 * number of RSes), but a low intermediate value can be chosen.
          For multiple tables, it is recommended to be conservative with presplitting (e.g.
          pre-split 1 region per RS at most), especially if you don't know how much each table will
          grow. If you split too much, you may end up with too many regions, with some tables having
          too many small regions.</p><p>For pre-splitting howto, see <a class="xref" href="#">???</a> and
            <a class="xref" href="#">???</a>.</p></div></div></div><div class="section" title="1.10.&nbsp;Table Rename"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="table.rename"></a>1.10.&nbsp;Table Rename</h2></div></div></div><p>In versions 0.90.x of hbase and earlier, we had a simple script that would rename the hdfs
      table directory and then do an edit of the hbase:meta table replacing all mentions of the old
      table name with the new. The script was called <span class="command"><strong>./bin/rename_table.rb</strong></span>. The
      script was deprecated and removed mostly because it was unmaintained and the operation
      performed by the script was brutal. </p><p> As of hbase 0.94.x, you can use the snapshot facility renaming a table. Here is how you
      would do it using the hbase shell:</p><pre class="screen">hbase shell&gt; disable 'tableName'
hbase shell&gt; snapshot 'tableName', 'tableSnapshot'
hbase shell&gt; clone_snapshot 'tableSnapshot', 'newTableName'
hbase shell&gt; delete_snapshot 'tableSnapshot'
hbase shell&gt; drop 'tableName'</pre><p>or in code it would be as follows:</p><pre class="programlisting">void rename(HBaseAdmin admin, String oldTableName, String newTableName) {
  String snapshotName = randomName();
  admin.disableTable(oldTableName);
  admin.snapshot(snapshotName, oldTableName);
  admin.cloneSnapshot(snapshotName, newTableName);
  admin.deleteSnapshot(snapshotName);
  admin.deleteTable(oldTableName);
}</pre></div><div class="footnotes"><br><hr width="100" align="left"><div class="footnote"><p><sup>[<a id="ftn.d839e1002" href="#d839e1002" class="para">1</a>] </sup>The Metrics system was redone in
          HBase 0.96. See <a class="link" href="https://blogs.apache.org/hbase/entry/migration_to_the_new_metrics" target="_top">Migration
            to the New Metrics Hotness &#8211; Metrics2</a> by Elliot Clark for detail</p></div></div></div><div id="disqus_thread"></div><script type="text/javascript">
    /* * * DON'T EDIT BELOW THIS LINE * * */
    (function() {
        var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
        dsq.src = 'http://' + disqus_shortname + '.disqus.com/embed.js';
        (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
    })();
</script><noscript>Please enable JavaScript to view the <a href="http://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript><a href="http://disqus.com" class="dsq-brlink">comments powered by <span class="logo-disqus">Disqus</span></a></body></html>