<html><head>
      <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
   <title>1.9.&nbsp;Reading from HBase</title><link rel="stylesheet" type="text/css" href="../css/freebsd_docbook.css"><meta name="generator" content="DocBook XSL-NS Stylesheets V1.76.1"><link rel="home" href="performance.html" title="Chapter&nbsp;1.&nbsp;Apache HBase Performance Tuning"><link rel="up" href="performance.html" title="Chapter&nbsp;1.&nbsp;Apache HBase Performance Tuning"><link rel="prev" href="perf.writing.html" title="1.8.&nbsp;Writing to HBase"><link rel="next" href="perf.deleting.html" title="1.10.&nbsp;Deleting from HBase"></head><body bgcolor="white" text="black" link="#0000FF" vlink="#840084" alink="#0000FF"><div class="navheader"><table width="100%" summary="Navigation header"><tr><th colspan="3" align="center">1.9.&nbsp;Reading from HBase</th></tr><tr><td width="20%" align="left"><a accesskey="p" href="perf.writing.html">Prev</a>&nbsp;</td><th width="60%" align="center">&nbsp;</th><td width="20%" align="right">&nbsp;<a accesskey="n" href="perf.deleting.html">Next</a></td></tr></table><hr></div><script type="text/javascript">
    var disqus_shortname = 'hbase'; // required: replace example with your forum shortname
    var disqus_url = 'http://hbase.apache.org/book/perf.reading.html';
    </script><div class="section" title="1.9.&nbsp;Reading from HBase"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="perf.reading"></a>1.9.&nbsp;Reading from HBase</h2></div></div></div><p>The mailing list can help if you are having performance issues. For example, here is a
      good general thread on what to look at addressing read-time issues: <a class="link" href="http://search-hadoop.com/m/qOo2yyHtCC1" target="_top">HBase Random Read latency &gt;
      100ms</a></p><div class="section" title="1.9.1.&nbsp;Scan Caching"><div class="titlepage"><div><div><h3 class="title"><a name="perf.hbase.client.caching"></a>1.9.1.&nbsp;Scan Caching</h3></div></div></div><p>If HBase is used as an input source for a MapReduce job, for example, make sure that the
        input <a class="link" href="http://hbase.apache.org/apidocs/org/apache/hadoop/hbase/client/Scan.html" target="_top">Scan</a>
        instance to the MapReduce job has <code class="methodname">setCaching</code> set to something
        greater than the default (which is 1). Using the default value means that the map-task will
        make call back to the region-server for every record processed. Setting this value to 500,
        for example, will transfer 500 rows at a time to the client to be processed. There is a
        cost/benefit to have the cache value be large because it costs more in memory for both
        client and RegionServer, so bigger isn't always better.</p><div class="section" title="1.9.1.1.&nbsp;Scan Caching in MapReduce Jobs"><div class="titlepage"><div><div><h4 class="title"><a name="perf.hbase.client.caching.mr"></a>1.9.1.1.&nbsp;Scan Caching in MapReduce Jobs</h4></div></div></div><p>Scan settings in MapReduce jobs deserve special attention. Timeouts can result (e.g.,
          UnknownScannerException) in Map tasks if it takes longer to process a batch of records
          before the client goes back to the RegionServer for the next set of data. This problem can
          occur because there is non-trivial processing occuring per row. If you process rows
          quickly, set caching higher. If you process rows more slowly (e.g., lots of
          transformations per row, writes), then set caching lower. </p><p>Timeouts can also happen in a non-MapReduce use case (i.e., single threaded HBase
          client doing a Scan), but the processing that is often performed in MapReduce jobs tends
          to exacerbate this issue. </p></div></div><div class="section" title="1.9.2.&nbsp;Scan Attribute Selection"><div class="titlepage"><div><div><h3 class="title"><a name="perf.hbase.client.selection"></a>1.9.2.&nbsp;Scan Attribute Selection</h3></div></div></div><p>Whenever a Scan is used to process large numbers of rows (and especially when used as a
        MapReduce source), be aware of which attributes are selected. If <code class="code">scan.addFamily</code>
        is called then <span class="emphasis"><em>all</em></span> of the attributes in the specified ColumnFamily will
        be returned to the client. If only a small number of the available attributes are to be
        processed, then only those attributes should be specified in the input scan because
        attribute over-selection is a non-trivial performance penalty over large datasets. </p></div><div class="section" title="1.9.3.&nbsp;Avoid scan seeks"><div class="titlepage"><div><div><h3 class="title"><a name="perf.hbase.client.seek"></a>1.9.3.&nbsp;Avoid scan seeks</h3></div></div></div><p>When columns are selected explicitly with <code class="code">scan.addColumn</code>, HBase will
        schedule seek operations to seek between the selected columns. When rows have few columns
        and each column has only a few versions this can be inefficient. A seek operation is
        generally slower if does not seek at least past 5-10 columns/versions or 512-1024
        bytes.</p><p>In order to opportunistically look ahead a few columns/versions to see if the next
        column/version can be found that way before a seek operation is scheduled, a new attribute
          <code class="code">Scan.HINT_LOOKAHEAD</code> can be set the on Scan object. The following code
        instructs the RegionServer to attempt two iterations of next before a seek is
        scheduled:</p><pre class="programlisting">
Scan scan = new Scan();
scan.addColumn(...);
scan.setAttribute(Scan.HINT_LOOKAHEAD, Bytes.toBytes(2));
table.getScanner(scan);
      </pre></div><div class="section" title="1.9.4.&nbsp;MapReduce - Input Splits"><div class="titlepage"><div><div><h3 class="title"><a name="perf.hbase.mr.input"></a>1.9.4.&nbsp;MapReduce - Input Splits</h3></div></div></div><p>For MapReduce jobs that use HBase tables as a source, if there a pattern where the
        "slow" map tasks seem to have the same Input Split (i.e., the RegionServer serving the
        data), see the Troubleshooting Case Study in <a class="xref" href="">???</a>. </p></div><div class="section" title="1.9.5.&nbsp;Close ResultScanners"><div class="titlepage"><div><div><h3 class="title"><a name="perf.hbase.client.scannerclose"></a>1.9.5.&nbsp;Close ResultScanners</h3></div></div></div><p>This isn't so much about improving performance but rather <span class="emphasis"><em>avoiding</em></span>
        performance problems. If you forget to close <a class="link" href="http://hbase.apache.org/apidocs/org/apache/hadoop/hbase/client/ResultScanner.html" target="_top">ResultScanners</a>
        you can cause problems on the RegionServers. Always have ResultScanner processing enclosed
        in try/catch blocks...</p><pre class="programlisting">
Scan scan = new Scan();
// set attrs...
ResultScanner rs = htable.getScanner(scan);
try {
  for (Result r = rs.next(); r != null; r = rs.next()) {
  // process result...
} finally {
  rs.close();  // always close the ResultScanner!
}
htable.close();
      </pre></div><div class="section" title="1.9.6.&nbsp;Block Cache"><div class="titlepage"><div><div><h3 class="title"><a name="perf.hbase.client.blockcache"></a>1.9.6.&nbsp;Block Cache</h3></div></div></div><p><a class="link" href="http://hbase.apache.org/apidocs/org/apache/hadoop/hbase/client/Scan.html" target="_top">Scan</a>
        instances can be set to use the block cache in the RegionServer via the
          <code class="methodname">setCacheBlocks</code> method. For input Scans to MapReduce jobs, this
        should be <code class="varname">false</code>. For frequently accessed rows, it is advisable to use the
        block cache.</p><p>Cache more data by moving your Block Cache offheap.  See <a class="xref" href="">???</a></p></div><div class="section" title="1.9.7.&nbsp;Optimal Loading of Row Keys"><div class="titlepage"><div><div><h3 class="title"><a name="perf.hbase.client.rowkeyonly"></a>1.9.7.&nbsp;Optimal Loading of Row Keys</h3></div></div></div><p>When performing a table <a class="link" href="http://hbase.apache.org/apidocs/org/apache/hadoop/hbase/client/Scan.html" target="_top">scan</a>
        where only the row keys are needed (no families, qualifiers, values or timestamps), add a
        FilterList with a <code class="varname">MUST_PASS_ALL</code> operator to the scanner using
          <code class="methodname">setFilter</code>. The filter list should include both a <a class="link" href="http://hbase.apache.org/apidocs/org/apache/hadoop/hbase/filter/FirstKeyOnlyFilter.html" target="_top">FirstKeyOnlyFilter</a>
        and a <a class="link" href="http://hbase.apache.org/apidocs/org/apache/hadoop/hbase/filter/KeyOnlyFilter.html" target="_top">KeyOnlyFilter</a>.
        Using this filter combination will result in a worst case scenario of a RegionServer reading
        a single value from disk and minimal network traffic to the client for a single row. </p></div><div class="section" title="1.9.8.&nbsp;Concurrency: Monitor Data Spread"><div class="titlepage"><div><div><h3 class="title"><a name="perf.hbase.read.dist"></a>1.9.8.&nbsp;Concurrency: Monitor Data Spread</h3></div></div></div><p>When performing a high number of concurrent reads, monitor the data spread of the target
        tables. If the target table(s) have too few regions then the reads could likely be served
        from too few nodes. </p><p>See <a class="xref" href="perf.writing.html#precreate.regions" title="1.8.2.&nbsp; Table Creation: Pre-Creating Regions">Section&nbsp;1.8.2, &#8220; Table Creation: Pre-Creating Regions &#8221;</a>, as well as <a class="xref" href="perf.configurations.html" title="1.4.&nbsp;HBase Configurations">Section&nbsp;1.4, &#8220;HBase Configurations&#8221;</a>
      </p></div><div class="section" title="1.9.9.&nbsp;Bloom Filters"><div class="titlepage"><div><div><h3 class="title"><a name="blooms"></a>1.9.9.&nbsp;Bloom Filters</h3></div></div></div><p>Enabling Bloom Filters can save your having to go to disk and can help improve read
        latencies.</p><p><a class="link" href="http://en.wikipedia.org/wiki/Bloom_filter" target="_top">Bloom filters</a> were developed
        over in <a class="link" href="https://issues.apache.org/jira/browse/HBASE-1200" target="_top">HBase-1200 Add
          bloomfilters</a>. For description of the development process -- why static blooms rather than dynamic
            -- and for an overview of the unique properties that pertain to blooms in HBase, as well
            as possible future directions, see the <span class="emphasis"><em>Development Process</em></span> section
            of the document <a class="link" href="https://issues.apache.org/jira/secure/attachment/12444007/Bloom_Filters_in_HBase.pdf" target="_top">BloomFilters
              in HBase</a> attached to <a class="link" href="https://issues.apache.org/jira/browse/HBASE-1200" target="_top">HBase-1200</a>.  The bloom filters described here are actually version two of blooms in HBase. In
            versions up to 0.19.x, HBase had a dynamic bloom option based on work done by the <a class="link" href="http://www.one-lab.org" target="_top">European Commission One-Lab Project 034819</a>.
            The core of the HBase bloom work was later pulled up into Hadoop to implement
            org.apache.hadoop.io.BloomMapFile. Version 1 of HBase blooms never worked that well.
            Version 2 is a rewrite from scratch though again it starts with the one-lab work.</p><p>See also <a class="xref" href="perf.schema.html#schema.bloom" title="1.6.4.&nbsp;Bloom Filters">Section&nbsp;1.6.4, &#8220;Bloom Filters&#8221;</a>. </p><div class="section" title="1.9.9.1.&nbsp;Bloom StoreFile footprint"><div class="titlepage"><div><div><h4 class="title"><a name="bloom_footprint"></a>1.9.9.1.&nbsp;Bloom StoreFile footprint</h4></div></div></div><p>Bloom filters add an entry to the <code class="classname">StoreFile</code> general
            <code class="classname">FileInfo</code> data structure and then two extra entries to the
            <code class="classname">StoreFile</code> metadata section.</p><div class="section" title="1.9.9.1.1.&nbsp;BloomFilter in the StoreFile FileInfo data structure"><div class="titlepage"><div><div><h5 class="title"><a name="d9542e1053"></a>1.9.9.1.1.&nbsp;BloomFilter in the <code class="classname">StoreFile</code>
            <code class="classname">FileInfo</code> data structure</h5></div></div></div><p><code class="classname">FileInfo</code> has a <code class="varname">BLOOM_FILTER_TYPE</code> entry
            which is set to <code class="varname">NONE</code>, <code class="varname">ROW</code> or
              <code class="varname">ROWCOL.</code></p></div><div class="section" title="1.9.9.1.2.&nbsp;BloomFilter entries in StoreFile metadata"><div class="titlepage"><div><div><h5 class="title"><a name="d9542e1077"></a>1.9.9.1.2.&nbsp;BloomFilter entries in <code class="classname">StoreFile</code> metadata</h5></div></div></div><p><code class="varname">BLOOM_FILTER_META</code> holds Bloom Size, Hash Function used, etc. Its
            small in size and is cached on <code class="classname">StoreFile.Reader</code> load</p><p><code class="varname">BLOOM_FILTER_DATA</code> is the actual bloomfilter data. Obtained
            on-demand. Stored in the LRU cache, if it is enabled (Its enabled by default).</p></div></div><div class="section" title="1.9.9.2.&nbsp;Bloom Filter Configuration"><div class="titlepage"><div><div><h4 class="title"><a name="config.bloom"></a>1.9.9.2.&nbsp;Bloom Filter Configuration</h4></div></div></div><div class="section" title="1.9.9.2.1.&nbsp;io.hfile.bloom.enabled global kill switch"><div class="titlepage"><div><div><h5 class="title"><a name="d9542e1097"></a>1.9.9.2.1.&nbsp;<code class="varname">io.hfile.bloom.enabled</code> global kill switch</h5></div></div></div><p><code class="code">io.hfile.bloom.enabled</code> in <code class="classname">Configuration</code> serves
            as the kill switch in case something goes wrong. Default =
            <code class="varname">true</code>.</p></div><div class="section" title="1.9.9.2.2.&nbsp;io.hfile.bloom.error.rate"><div class="titlepage"><div><div><h5 class="title"><a name="d9542e1112"></a>1.9.9.2.2.&nbsp;<code class="varname">io.hfile.bloom.error.rate</code></h5></div></div></div><p><code class="varname">io.hfile.bloom.error.rate</code> = average false positive rate. Default
            = 1%. Decrease rate by &frac12; (e.g. to .5%) == +1 bit per bloom entry.</p></div><div class="section" title="1.9.9.2.3.&nbsp;io.hfile.bloom.max.fold"><div class="titlepage"><div><div><h5 class="title"><a name="d9542e1120"></a>1.9.9.2.3.&nbsp;<code class="varname">io.hfile.bloom.max.fold</code></h5></div></div></div><p><code class="varname">io.hfile.bloom.max.fold</code> = guaranteed minimum fold rate. Most
            people should leave this alone. Default = 7, or can collapse to at least 1/128th of
            original size. See the <span class="emphasis"><em>Development Process</em></span> section of the document <a class="link" href="https://issues.apache.org/jira/secure/attachment/12444007/Bloom_Filters_in_HBase.pdf" target="_top">BloomFilters
              in HBase</a> for more on what this option means.</p></div></div></div><div class="section" title="1.9.10.&nbsp;Hedged Reads"><div class="titlepage"><div><div><h3 class="title"><a name="d9542e1135"></a>1.9.10.&nbsp;Hedged Reads</h3></div></div></div><p>Hedged reads are a feature of HDFS, introduced in <a class="link" href="https://issues.apache.org/jira/browse/HDFS-5776" target="_top">HDFS-5776</a>. Normally, a
        single thread is spawned for each read request. However, if hedged reads are enabled, the
        client waits some configurable amount of time, and if the read does not return, the client
        spawns a second read request, against a different block replica of the same data. Whichever
        read returns first is used, and the other read request is discarded. Hedged reads can be
        helpful for times where a rare slow read is caused by a transient error such as a failing
        disk or flaky network connection.</p><p> Because a HBase RegionServer is a HDFS client, you can enable hedged reads in HBase, by
        adding the following properties to the RegionServer's hbase-site.xml and tuning the values
        to suit your environment.</p><div class="itemizedlist" title="Configuration for Hedged Reads"><p class="title"><b>Configuration for Hedged Reads</b></p><ul class="itemizedlist" type="disc"><li class="listitem"><p><code class="code">dfs.client.hedged.read.threadpool.size</code> - the number of threads
            dedicated to servicing hedged reads. If this is set to 0 (the default), hedged reads are
            disabled.</p></li><li class="listitem"><p><code class="code">dfs.client.hedged.read.threshold.millis</code> - the number of milliseconds to
            wait before spawning a second read thread.</p></li></ul></div><div class="example"><a name="d9542e1158"></a><p class="title"><b>Example&nbsp;1.3.&nbsp;Hedged Reads Configuration Example</b></p><div class="example-contents"><pre class="screen">&lt;property&gt;
  &lt;name&gt;dfs.client.hedged.read.threadpool.size&lt;/name&gt;
  &lt;value&gt;20&lt;/value&gt;  &lt;!-- 20 threads --&gt;
&lt;/property&gt;
&lt;property&gt;
  &lt;name&gt;dfs.client.hedged.read.threshold.millis&lt;/name&gt;
  &lt;value&gt;10&lt;/value&gt;  &lt;!-- 10 milliseconds --&gt;
&lt;/property&gt;</pre></div></div><br class="example-break"><p>Use the following metrics to tune the settings for hedged reads on
        your cluster. See <a class="xref" href="">???</a>  for more information.</p><div class="itemizedlist" title="Metrics for Hedged Reads"><p class="title"><b>Metrics for Hedged Reads</b></p><ul class="itemizedlist" type="disc"><li class="listitem"><p>hedgedReadOps - the number of times hedged read threads have been triggered. This
            could indicate that read requests are often slow, or that hedged reads are triggered too
            quickly.</p></li><li class="listitem"><p>hedgeReadOpsWin - the number of times the hedged read thread was faster than the
            original thread. This could indicate that a given RegionServer is having trouble
            servicing requests.</p></li></ul></div></div></div><div id="disqus_thread"></div><script type="text/javascript">
    /* * * DON'T EDIT BELOW THIS LINE * * */
    (function() {
        var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
        dsq.src = 'http://' + disqus_shortname + '.disqus.com/embed.js';
        (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
    })();
</script><noscript>Please enable JavaScript to view the <a href="http://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript><a href="http://disqus.com" class="dsq-brlink">comments powered by <span class="logo-disqus">Disqus</span></a><div class="navfooter"><hr><table width="100%" summary="Navigation footer"><tr><td width="40%" align="left"><a accesskey="p" href="perf.writing.html">Prev</a>&nbsp;</td><td width="20%" align="center">&nbsp;</td><td width="40%" align="right">&nbsp;<a accesskey="n" href="perf.deleting.html">Next</a></td></tr><tr><td width="40%" align="left" valign="top">1.8.&nbsp;Writing to HBase&nbsp;</td><td width="20%" align="center"><a accesskey="h" href="performance.html">Home</a></td><td width="40%" align="right" valign="top">&nbsp;1.10.&nbsp;Deleting from HBase</td></tr></table></div></body></html>