<html><head>
      <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
   <title>1.3.&nbsp;Node Management</title><link rel="stylesheet" type="text/css" href="../css/freebsd_docbook.css"><meta name="generator" content="DocBook XSL-NS Stylesheets V1.76.1"><link rel="home" href="ops_mgt.html" title="Chapter&nbsp;1.&nbsp;Apache HBase Operational Management"><link rel="up" href="ops_mgt.html" title="Chapter&nbsp;1.&nbsp;Apache HBase Operational Management"><link rel="prev" href="ops.regionmgt.html" title="1.2.&nbsp;Region Management"><link rel="next" href="hbase_metrics.html" title="1.4.&nbsp;HBase Metrics"></head><body bgcolor="white" text="black" link="#0000FF" vlink="#840084" alink="#0000FF"><div class="navheader"><table width="100%" summary="Navigation header"><tr><th colspan="3" align="center">1.3.&nbsp;Node Management</th></tr><tr><td width="20%" align="left"><a accesskey="p" href="ops.regionmgt.html">Prev</a>&nbsp;</td><th width="60%" align="center">&nbsp;</th><td width="20%" align="right">&nbsp;<a accesskey="n" href="hbase_metrics.html">Next</a></td></tr></table><hr></div><script type="text/javascript">
    var disqus_shortname = 'hbase'; // required: replace example with your forum shortname
    var disqus_url = 'http://hbase.apache.org/book/node.management.html';
    </script><div class="section" title="1.3.&nbsp;Node Management"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="node.management"></a>1.3.&nbsp;Node Management</h2></div></div></div><div class="section" title="1.3.1.&nbsp;Node Decommission"><div class="titlepage"><div><div><h3 class="title"><a name="decommission"></a>1.3.1.&nbsp;Node Decommission</h3></div></div></div><p>You can stop an individual RegionServer by running the following script in the HBase
        directory on the particular node:</p><pre class="screen">$ ./bin/hbase-daemon.sh stop regionserver</pre><p> The RegionServer will first close all regions and then shut itself down. On shutdown,
        the RegionServer's ephemeral node in ZooKeeper will expire. The master will notice the
        RegionServer gone and will treat it as a 'crashed' server; it will reassign the nodes the
        RegionServer was carrying. </p><div class="note" title="Disable the Load Balancer before Decommissioning a node" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title">Disable the Load Balancer before Decommissioning a node</h3><p>If the load balancer runs while a node is shutting down, then there could be
          contention between the Load Balancer and the Master's recovery of the just decommissioned
          RegionServer. Avoid any problems by disabling the balancer first. See <a class="xref" href="node.management.html#lb" title="Load Balancer">Load Balancer</a> below. </p></div><div class="note" title="Kill Node Tool" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title">Kill Node Tool</h3><p>In hbase-2.0, in the bin directory, we added a script named
          <code class="filename">considerAsDead.sh</code> that can be used to kill a regionserver.
          Hardware issues could be detected by specialized monitoring tools before the 
          zookeeper timeout has expired. <code class="filename">considerAsDead.sh</code> is a
          simple function to mark a RegionServer as dead.  It deletes all the znodes
          of the server, starting the recovery process.  Plug in the script into
          your monitoring/fault detection tools to initiate faster failover. Be
          careful how you use this disruptive tool. Copy the script if you need to
          make use of it in a version of hbase previous to hbase-2.0.
        </p></div><p> A downside to the above stop of a RegionServer is that regions could be offline for a
        good period of time. Regions are closed in order. If many regions on the server, the first
        region to close may not be back online until all regions close and after the master notices
        the RegionServer's znode gone. In Apache HBase 0.90.2, we added facility for having a node
        gradually shed its load and then shutdown itself down. Apache HBase 0.90.2 added the
          <code class="filename">graceful_stop.sh</code> script. Here is its usage:</p><pre class="screen">$ ./bin/graceful_stop.sh
Usage: graceful_stop.sh [--config &amp;conf-dir&gt;] [--restart] [--reload] [--thrift] [--rest] &amp;hostname&gt;
 thrift      If we should stop/start thrift before/after the hbase stop/start
 rest        If we should stop/start rest before/after the hbase stop/start
 restart     If we should restart after graceful stop
 reload      Move offloaded regions back on to the stopped server
 debug       Move offloaded regions back on to the stopped server
 hostname    Hostname of server we are to stop</pre><p> To decommission a loaded RegionServer, run the following: <span class="command"><strong>$
          ./bin/graceful_stop.sh HOSTNAME</strong></span> where <code class="varname">HOSTNAME</code> is the host
        carrying the RegionServer you would decommission. </p><div class="note" title="On HOSTNAME" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title">On <code class="varname">HOSTNAME</code></h3><p>The <code class="varname">HOSTNAME</code> passed to <code class="filename">graceful_stop.sh</code> must
          match the hostname that hbase is using to identify RegionServers. Check the list of
          RegionServers in the master UI for how HBase is referring to servers. Its usually hostname
          but can also be FQDN. Whatever HBase is using, this is what you should pass the
            <code class="filename">graceful_stop.sh</code> decommission script. If you pass IPs, the script
          is not yet smart enough to make a hostname (or FQDN) of it and so it will fail when it
          checks if server is currently running; the graceful unloading of regions will not run.
        </p></div><p> The <code class="filename">graceful_stop.sh</code> script will move the regions off the
        decommissioned RegionServer one at a time to minimize region churn. It will verify the
        region deployed in the new location before it will moves the next region and so on until the
        decommissioned server is carrying zero regions. At this point, the
          <code class="filename">graceful_stop.sh</code> tells the RegionServer <span class="command"><strong>stop</strong></span>. The
        master will at this point notice the RegionServer gone but all regions will have already
        been redeployed and because the RegionServer went down cleanly, there will be no WAL logs to
        split. </p><div class="note" title="Load Balancer" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a name="lb"></a>Load Balancer</h3><p> It is assumed that the Region Load Balancer is disabled while the
            <span class="command"><strong>graceful_stop</strong></span> script runs (otherwise the balancer and the
          decommission script will end up fighting over region deployments). Use the shell to
          disable the balancer:</p><pre class="programlisting">hbase(main):001:0&gt; balance_switch false
true
0 row(s) in 0.3590 seconds</pre><p> This turns the balancer OFF. To reenable, do:</p><pre class="programlisting">hbase(main):001:0&gt; balance_switch true
false
0 row(s) in 0.3590 seconds</pre><p>The <span class="command"><strong>graceful_stop</strong></span> will check the balancer and if enabled, will turn
          it off before it goes to work. If it exits prematurely because of error, it will not have
          reset the balancer. Hence, it is better to manage the balancer apart from
            <span class="command"><strong>graceful_stop</strong></span> reenabling it after you are done w/ graceful_stop.
        </p></div><div class="section" title="1.3.1.1.&nbsp;Decommissioning several Regions Servers concurrently"><div class="titlepage"><div><div><h4 class="title"><a name="draining.servers"></a>1.3.1.1.&nbsp;Decommissioning several Regions Servers concurrently</h4></div></div></div><p>If you have a large cluster, you may want to decommission more than one machine at a
          time by gracefully stopping mutiple RegionServers concurrently. To gracefully drain
          multiple regionservers at the same time, RegionServers can be put into a "draining" state.
          This is done by marking a RegionServer as a draining node by creating an entry in
          ZooKeeper under the <code class="filename">hbase_root/draining</code> znode. This znode has format
            <code class="code">name,port,startcode</code> just like the regionserver entries under
            <code class="filename">hbase_root/rs</code> znode. </p><p>Without this facility, decommissioning mulitple nodes may be non-optimal because
          regions that are being drained from one region server may be moved to other regionservers
          that are also draining. Marking RegionServers to be in the draining state prevents this
          from happening. See this <a class="link" href="http://inchoate-clatter.blogspot.com/2012/03/hbase-ops-automation.html" target="_top">blog
            post</a> for more details.</p></div><div class="section" title="1.3.1.2.&nbsp;Bad or Failing Disk"><div class="titlepage"><div><div><h4 class="title"><a name="bad.disk"></a>1.3.1.2.&nbsp;Bad or Failing Disk</h4></div></div></div><p>It is good having <a class="xref" href="">???</a> set if you have a decent number of
          disks per machine for the case where a disk plain dies. But usually disks do the "John
          Wayne" -- i.e. take a while to go down spewing errors in <code class="filename">dmesg</code> -- or
          for some reason, run much slower than their companions. In this case you want to
          decommission the disk. You have two options. You can <a class="link" href="http://wiki.apache.org/hadoop/FAQ#I_want_to_make_a_large_cluster_smaller_by_taking_out_a_bunch_of_nodes_simultaneously._How_can_this_be_done.3F" target="_top">decommission
            the datanode</a> or, less disruptive in that only the bad disks data will be
          rereplicated, can stop the datanode, unmount the bad volume (You can't umount a volume
          while the datanode is using it), and then restart the datanode (presuming you have set
          dfs.datanode.failed.volumes.tolerated &gt; 0). The regionserver will throw some errors in its
          logs as it recalibrates where to get its data from -- it will likely roll its WAL log too
          -- but in general but for some latency spikes, it should keep on chugging. </p><div class="note" title="Short Circuit Reads" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title">Short Circuit Reads</h3><p>If you are doing short-circuit reads, you will have to move the regions off the
            regionserver before you stop the datanode; when short-circuiting reading, though chmod'd
            so regionserver cannot have access, because it already has the files open, it will be
            able to keep reading the file blocks from the bad disk even though the datanode is down.
            Move the regions back after you restart the datanode.</p></div></div></div><div class="section" title="1.3.2.&nbsp;Rolling Restart"><div class="titlepage"><div><div><h3 class="title"><a name="rolling"></a>1.3.2.&nbsp;Rolling Restart</h3></div></div></div><p>Some cluster configuration changes require either the entire cluster, or the
          RegionServers, to be restarted in order to pick up the changes. In addition, rolling
          restarts are supported for upgrading to a minor or maintenance release, and to a major
          release if at all possible. See the release notes for release you want to upgrade to, to
          find out about limitations to the ability to perform a rolling upgrade.</p><p>There are multiple ways to restart your cluster nodes, depending on your situation.
        These methods are detailed below.</p><div class="section" title="1.3.2.1.&nbsp;Using the rolling-restart.sh Script"><div class="titlepage"><div><div><h4 class="title"><a name="d839e833"></a>1.3.2.1.&nbsp;Using the <span class="command"><strong>rolling-restart.sh</strong></span> Script</h4></div></div></div><p>HBase ships with a script, <code class="filename">bin/rolling-restart.sh</code>, that allows
          you to perform rolling restarts on the entire cluster, the master only, or the
          RegionServers only. The script is provided as a template for your own script, and is not
          explicitly tested. It requires password-less SSH login to be configured and assumes that
          you have deployed using a tarball. The script requires you to set some environment
          variables before running it. Examine the script and modify it to suit your needs.</p><div class="example"><a name="d839e844"></a><p class="title"><b>Example&nbsp;1.1.&nbsp;<code class="filename">rolling-restart.sh</code> General Usage</b></p><div class="example-contents"><pre class="screen">
$ <strong class="userinput"><code>./bin/rolling-restart.sh --help</code></strong>
Usage: rolling-restart.sh [--config &lt;hbase-confdir&gt;] [--rs-only] [--master-only] [--graceful] [--maxthreads xx]          
        </pre></div></div><br class="example-break"><div class="variablelist"><dl><dt><span class="term">Rolling Restart on RegionServers Only</span></dt><dd><p>To perform a rolling restart on the RegionServers only, use the
                  <code class="code">--rs-only</code> option. This might be necessary if you need to reboot the
                individual RegionServer or if you make a configuration change that only affects
                RegionServers and not the other HBase processes.</p><p>If you need to restart only a single RegionServer, or if you need to do extra
                actions during the restart, use the <code class="filename">bin/graceful_stop.sh</code>
                command instead. See <a class="xref" href="node.management.html#rolling.restart.manual" title="1.3.2.2.&nbsp;Manual Rolling Restart">Section&nbsp;1.3.2.2, &#8220;Manual Rolling Restart&#8221;</a>.</p></dd><dt><span class="term">Rolling Restart on Masters Only</span></dt><dd><p>To perform a rolling restart on the active and backup Masters, use the
                  <code class="code">--master-only</code> option. You might use this if you know that your
                configuration change only affects the Master and not the RegionServers, or if you
                need to restart the server where the active Master is running.</p><p>If you are not running backup Masters, the Master is simply restarted. If you
                are running backup Masters, they are all stopped before any are restarted, to avoid
                a race condition in ZooKeeper to determine which is the new Master. First the main
                Master is restarted, then the backup Masters are restarted. Directly after restart,
                it checks for and cleans out any regions in transition before taking on its normal
                workload.</p></dd><dt><span class="term">Graceful Restart</span></dt><dd><p>If you specify the <code class="code">--graceful</code> option, RegionServers are restarted
                using the <code class="filename">bin/graceful_stop.sh</code> script, which moves regions off
                a RegionServer before restarting it. This is safer, but can delay the
                restart.</p></dd><dt><span class="term">Limiting the Number of Threads</span></dt><dd><p>To limit the rolling restart to using only a specific number of threads, use the
                  <code class="code">--maxthreads</code> option.</p></dd></dl></div></div><div class="section" title="1.3.2.2.&nbsp;Manual Rolling Restart"><div class="titlepage"><div><div><h4 class="title"><a name="rolling.restart.manual"></a>1.3.2.2.&nbsp;Manual Rolling Restart</h4></div></div></div><p>To retain more control over the process, you may wish to manually do a rolling restart
          across your cluster. This uses the <span class="command"><strong>graceful-stop.sh</strong></span> command <a class="xref" href="node.management.html#decommission" title="1.3.1.&nbsp;Node Decommission">Section&nbsp;1.3.1, &#8220;Node Decommission&#8221;</a>. In this method, you can restart each RegionServer
          individually and then move its old regions back into place, retaining locality. If you
          also need to restart the Master, you need to do it separately, and restart the Master
          before restarting the RegionServers using this method. The following is an example of such
          a command. You may need to tailor it to your environment. This script does a rolling
          restart of RegionServers only. It disables the load balancer before moving the
          regions.</p><pre class="screen">
$ for i in `cat conf/regionservers|sort`; do ./bin/graceful_stop.sh --restart --reload --debug $i; done &amp;&gt; /tmp/log.txt &amp;;     
        </pre><p>Monitor the output of the <code class="filename">/tmp/log.txt</code> file to follow the
          progress of the script. </p></div><div class="section" title="1.3.2.3.&nbsp;Logic for Crafting Your Own Rolling Restart Script"><div class="titlepage"><div><div><h4 class="title"><a name="d839e920"></a>1.3.2.3.&nbsp;Logic for Crafting Your Own Rolling Restart Script</h4></div></div></div><p>Use the following guidelines if you want to create your own rolling restart script.</p><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem"><p>Extract the new release, verify its configuration, and synchronize it to all nodes
              of your cluster using <span class="command"><strong>rsync</strong></span>, <span class="command"><strong>scp</strong></span>, or another
              secure synchronization mechanism.</p></li><li class="listitem"><p>Use the hbck utility to ensure that the cluster is consistent.</p><pre class="screen">
$ ./bin/hbck            
          </pre><p>Perform repairs if required. See <a class="xref" href="ops_mgt.html#hbck" title="1.1.4.&nbsp;HBase hbck">Section&nbsp;1.1.4, &#8220;HBase <span class="application">hbck</span>&#8221;</a> for details.</p></li><li class="listitem"><p>Restart the master first. You may need to modify these commands if your
            new HBase directory is different from the old one, such as for an upgrade.</p><pre class="screen">
$ ./bin/hbase-daemon.sh stop master; ./bin/hbase-daemon.sh start master            
          </pre></li><li class="listitem"><p>Gracefully restart each RegionServer, using a script such as the
            following, from the Master.</p><pre class="screen">
$ for i in `cat conf/regionservers|sort`; do ./bin/graceful_stop.sh --restart --reload --debug $i; done &amp;&gt; /tmp/log.txt &amp;            
          </pre><p>If you are running Thrift or REST servers, pass the --thrift or --rest options.
              For other available options, run the <span class="command"><strong>bin/graceful-stop.sh --help</strong></span>
              command.</p><p>It is important to drain HBase regions slowly when restarting multiple
              RegionServers. Otherwise, multiple regions go offline simultaneously and must be
              reassigned to other nodes, which may also go offline soon. This can negatively affect
              performance. You can inject delays into the script above, for instance, by adding a
              Shell command such as <span class="command"><strong>sleep</strong></span>. To wait for 5 minutes between each
              RegionServer restart, modify the above script to the following:</p><pre class="screen">
$ for i in `cat conf/regionservers|sort`; do ./bin/graceful_stop.sh --restart --reload --debug $i &amp; sleep 5m; done &amp;&gt; /tmp/log.txt &amp;            
          </pre></li><li class="listitem"><p>Restart the Master again, to clear out the dead servers list and re-enable
          the load balancer.</p></li><li class="listitem"><p>Run the <span class="command"><strong>hbck</strong></span> utility again, to be sure the cluster is
            consistent.</p></li></ol></div></div></div><div class="section" title="1.3.3.&nbsp;Adding a New Node"><div class="titlepage"><div><div><h3 class="title"><a name="adding.new.node"></a>1.3.3.&nbsp;Adding a New Node</h3></div></div></div><p>Adding a new regionserver in HBase is essentially free, you simply start it like this:
          <span class="command"><strong>$ ./bin/hbase-daemon.sh start regionserver</strong></span> and it will register itself
        with the master. Ideally you also started a DataNode on the same machine so that the RS can
        eventually start to have local files. If you rely on ssh to start your daemons, don't forget
        to add the new hostname in <code class="filename">conf/regionservers</code> on the master. </p><p>At this point the region server isn't serving data because no regions have moved to it
        yet. If the balancer is enabled, it will start moving regions to the new RS. On a
        small/medium cluster this can have a very adverse effect on latency as a lot of regions will
        be offline at the same time. It is thus recommended to disable the balancer the same way
        it's done when decommissioning a node and move the regions manually (or even better, using a
        script that moves them one by one). </p><p>The moved regions will all have 0% locality and won't have any blocks in cache so the
        region server will have to use the network to serve requests. Apart from resulting in higher
        latency, it may also be able to use all of your network card's capacity. For practical
        purposes, consider that a standard 1GigE NIC won't be able to read much more than
          <span class="emphasis"><em>100MB/s</em></span>. In this case, or if you are in a OLAP environment and
        require having locality, then it is recommended to major compact the moved regions. </p></div></div><div id="disqus_thread"></div><script type="text/javascript">
    /* * * DON'T EDIT BELOW THIS LINE * * */
    (function() {
        var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
        dsq.src = 'http://' + disqus_shortname + '.disqus.com/embed.js';
        (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
    })();
</script><noscript>Please enable JavaScript to view the <a href="http://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript><a href="http://disqus.com" class="dsq-brlink">comments powered by <span class="logo-disqus">Disqus</span></a><div class="navfooter"><hr><table width="100%" summary="Navigation footer"><tr><td width="40%" align="left"><a accesskey="p" href="ops.regionmgt.html">Prev</a>&nbsp;</td><td width="20%" align="center">&nbsp;</td><td width="40%" align="right">&nbsp;<a accesskey="n" href="hbase_metrics.html">Next</a></td></tr><tr><td width="40%" align="left" valign="top">1.2.&nbsp;Region Management&nbsp;</td><td width="20%" align="center"><a accesskey="h" href="ops_mgt.html">Home</a></td><td width="40%" align="right" valign="top">&nbsp;1.4.&nbsp;HBase Metrics</td></tr></table></div></body></html>