<html><head>
      <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
   <title>Chapter&nbsp;1.&nbsp;Troubleshooting and Debugging Apache HBase</title><link rel="stylesheet" type="text/css" href="css/freebsd_docbook.css"><meta name="generator" content="DocBook XSL-NS Stylesheets V1.76.1"></head><body bgcolor="white" text="black" link="#0000FF" vlink="#840084" alink="#0000FF"><script type="text/javascript">
    var disqus_shortname = 'hbase'; // required: replace example with your forum shortname
    var disqus_url = 'http://hbase.apache.org/book/trouble.html';
    </script><div class="chapter" title="Chapter&nbsp;1.&nbsp;Troubleshooting and Debugging Apache HBase"><div class="titlepage"><div><div><h2 class="title"><a name="trouble"></a>Chapter&nbsp;1.&nbsp;Troubleshooting and Debugging Apache HBase</h2></div></div></div><div class="toc"><p><b>Table of Contents</b></p><dl><dt><span class="section"><a href="#trouble.general">1.1. General Guidelines</a></span></dt><dt><span class="section"><a href="#trouble.log">1.2. Logs</a></span></dt><dd><dl><dt><span class="section"><a href="#trouble.log.locations">1.2.1. Log Locations</a></span></dt><dt><span class="section"><a href="#trouble.log.levels">1.2.2. Log Levels</a></span></dt><dt><span class="section"><a href="#trouble.log.gc">1.2.3. JVM Garbage Collection Logs</a></span></dt></dl></dd><dt><span class="section"><a href="#trouble.resources">1.3. Resources</a></span></dt><dd><dl><dt><span class="section"><a href="#trouble.resources.searchhadoop">1.3.1. search-hadoop.com</a></span></dt><dt><span class="section"><a href="#trouble.resources.lists">1.3.2. Mailing Lists</a></span></dt><dt><span class="section"><a href="#trouble.resources.irc">1.3.3. IRC</a></span></dt><dt><span class="section"><a href="#trouble.resources.jira">1.3.4. JIRA</a></span></dt></dl></dd><dt><span class="section"><a href="#trouble.tools">1.4. Tools</a></span></dt><dd><dl><dt><span class="section"><a href="#trouble.tools.builtin">1.4.1. Builtin Tools</a></span></dt><dt><span class="section"><a href="#trouble.tools.external">1.4.2. External Tools</a></span></dt></dl></dd><dt><span class="section"><a href="#trouble.client">1.5. Client</a></span></dt><dd><dl><dt><span class="section"><a href="#trouble.client.scantimeout">1.5.1. ScannerTimeoutException or UnknownScannerException</a></span></dt><dt><span class="section"><a href="#d0e396">1.5.2. Performance Differences in Thrift and Java APIs</a></span></dt><dt><span class="section"><a href="#trouble.client.lease.exception">1.5.3. <code class="classname">LeaseException</code> when calling
        <code class="classname">Scanner.next</code></a></span></dt><dt><span class="section"><a href="#trouble.client.scarylogs">1.5.4. Shell or client application throws lots of scary exceptions during normal
        operation</a></span></dt><dt><span class="section"><a href="#trouble.client.longpauseswithcompression">1.5.5. Long Client Pauses With Compression</a></span></dt><dt><span class="section"><a href="#trouble.client.security.rpc.krb">1.5.6. Secure Client Connect ([Caused by GSSException: No valid credentials provided...])</a></span></dt><dt><span class="section"><a href="#trouble.client.zookeeper">1.5.7. ZooKeeper Client Connection Errors</a></span></dt><dt><span class="section"><a href="#trouble.client.oome.directmemory.leak">1.5.8. Client running out of memory though heap size seems to be stable (but the
        off-heap/direct heap keeps growing)</a></span></dt><dt><span class="section"><a href="#trouble.client.slowdown.admin">1.5.9. Client Slowdown When Calling Admin Methods (flush, compact, etc.)</a></span></dt><dt><span class="section"><a href="#trouble.client.security.rpc">1.5.10. Secure Client Cannot Connect ([Caused by GSSException: No valid credentials provided
        (Mechanism level: Failed to find any Kerberos tgt)])</a></span></dt></dl></dd><dt><span class="section"><a href="#trouble.mapreduce">1.6. MapReduce</a></span></dt><dd><dl><dt><span class="section"><a href="#trouble.mapreduce.local">1.6.1. You Think You're On The Cluster, But You're Actually Local</a></span></dt><dt><span class="section"><a href="#trouble.hbasezerocopybytestring">1.6.2. Launching a job, you get java.lang.IllegalAccessError: com/google/protobuf/HBaseZeroCopyByteString or class com.google.protobuf.ZeroCopyLiteralByteString cannot access its superclass com.google.protobuf.LiteralByteString</a></span></dt></dl></dd><dt><span class="section"><a href="#trouble.namenode">1.7. NameNode</a></span></dt><dd><dl><dt><span class="section"><a href="#trouble.namenode.disk">1.7.1. HDFS Utilization of Tables and Regions</a></span></dt><dt><span class="section"><a href="#trouble.namenode.hbase.objects">1.7.2. Browsing HDFS for HBase Objects</a></span></dt></dl></dd><dt><span class="section"><a href="#trouble.network">1.8. Network</a></span></dt><dd><dl><dt><span class="section"><a href="#trouble.network.spikes">1.8.1. Network Spikes</a></span></dt><dt><span class="section"><a href="#trouble.network.loopback">1.8.2. Loopback IP</a></span></dt><dt><span class="section"><a href="#trouble.network.ints">1.8.3. Network Interfaces</a></span></dt></dl></dd><dt><span class="section"><a href="#trouble.rs">1.9. RegionServer</a></span></dt><dd><dl><dt><span class="section"><a href="#trouble.rs.startup">1.9.1. Startup Errors</a></span></dt><dt><span class="section"><a href="#trouble.rs.runtime">1.9.2. Runtime Errors</a></span></dt><dt><span class="section"><a href="#d0e942">1.9.3. Snapshot Errors Due to Reverse DNS</a></span></dt><dt><span class="section"><a href="#trouble.rs.shutdown">1.9.4. Shutdown Errors</a></span></dt></dl></dd><dt><span class="section"><a href="#trouble.master">1.10. Master</a></span></dt><dd><dl><dt><span class="section"><a href="#trouble.master.startup">1.10.1. Startup Errors</a></span></dt><dt><span class="section"><a href="#trouble.master.shutdown">1.10.2. Shutdown Errors</a></span></dt></dl></dd><dt><span class="section"><a href="#trouble.zookeeper">1.11. ZooKeeper</a></span></dt><dd><dl><dt><span class="section"><a href="#trouble.zookeeper.startup">1.11.1. Startup Errors</a></span></dt><dt><span class="section"><a href="#trouble.zookeeper.general">1.11.2. ZooKeeper, The Cluster Canary</a></span></dt></dl></dd><dt><span class="section"><a href="#trouble.ec2">1.12. Amazon EC2</a></span></dt><dd><dl><dt><span class="section"><a href="#trouble.ec2.zookeeper">1.12.1. ZooKeeper does not seem to work on Amazon EC2</a></span></dt><dt><span class="section"><a href="#trouble.ec2.instability">1.12.2. Instability on Amazon EC2</a></span></dt><dt><span class="section"><a href="#trouble.ec2.connection">1.12.3. Remote Java Connection into EC2 Cluster Not Working</a></span></dt></dl></dd><dt><span class="section"><a href="#trouble.versions">1.13. HBase and Hadoop version issues</a></span></dt><dd><dl><dt><span class="section"><a href="#trouble.versions.205">1.13.1. <code class="code">NoClassDefFoundError</code> when trying to run 0.90.x on hadoop-0.20.205.x (or hadoop-1.0.x)</a></span></dt><dt><span class="section"><a href="#trouble.wrong.version">1.13.2. ...cannot communicate with client version...</a></span></dt></dl></dd><dt><span class="section"><a href="#d0e1094">1.14. IPC Configuration Conflicts with Hadoop</a></span></dt><dt><span class="section"><a href="#d0e1251">1.15. HBase and HDFS</a></span></dt><dt><span class="section"><a href="#trouble.tests">1.16. Running unit or integration tests</a></span></dt><dd><dl><dt><span class="section"><a href="#trouble.HDFS-2556">1.16.1. Runtime exceptions from MiniDFSCluster when running tests</a></span></dt></dl></dd><dt><span class="section"><a href="#trouble.casestudy">1.17. Case Studies</a></span></dt><dt><span class="section"><a href="#trouble.crypto">1.18. Cryptographic Features</a></span></dt><dd><dl><dt><span class="section"><a href="#trouble.crypto.HBASE-10132">1.18.1. sun.security.pkcs11.wrapper.PKCS11Exception: CKR_ARGUMENTS_BAD</a></span></dt></dl></dd><dt><span class="section"><a href="#d0e1501">1.19. Operating System Specific Issues</a></span></dt><dd><dl><dt><span class="section"><a href="#d0e1504">1.19.1. Page Allocation Failure</a></span></dt></dl></dd><dt><span class="section"><a href="#d0e1543">1.20. JDK Issues</a></span></dt><dd><dl><dt><span class="section"><a href="#d0e1546">1.20.1. NoSuchMethodError: java.util.concurrent.ConcurrentHashMap.keySet</a></span></dt></dl></dd></dl></div><div class="section" title="1.1.&nbsp;General Guidelines"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="trouble.general"></a>1.1.&nbsp;General Guidelines</h2></div></div></div><p> Always start with the master log (TODO: Which lines?). Normally it&#8217;s just printing the
      same lines over and over again. If not, then there&#8217;s an issue. Google or <a class="link" href="http://search-hadoop.com" target="_top">search-hadoop.com</a> should return some hits for
      those exceptions you&#8217;re seeing. </p><p> An error rarely comes alone in Apache HBase, usually when something gets screwed up what
      will follow may be hundreds of exceptions and stack traces coming from all over the place. The
      best way to approach this type of problem is to walk the log up to where it all began, for
      example one trick with RegionServers is that they will print some metrics when aborting so
      grepping for <span class="emphasis"><em>Dump</em></span> should get you around the start of the problem. </p><p> RegionServer suicides are &#8220;normal&#8221;, as this is what they do when something goes wrong.
      For example, if ulimit and max transfer threads (the two most important initial settings, see
      <a class="xref" href="#">???</a> and <a class="xref" href="#">???</a>) aren&#8217;t
      changed, it will make it impossible at some point for DataNodes
      to create new threads that from the HBase point of view is seen as if HDFS was gone. Think
      about what would happen if your MySQL database was suddenly unable to access files on your
      local file system, well it&#8217;s the same with HBase and HDFS. Another very common reason to see
      RegionServers committing seppuku is when they enter prolonged garbage collection pauses that
      last longer than the default ZooKeeper session timeout. For more information on GC pauses, see
      the <a class="link" href="http://www.cloudera.com/blog/2011/02/avoiding-full-gcs-in-hbase-with-memstore-local-allocation-buffers-part-1/" target="_top">3
        part blog post</a> by Todd Lipcon and <a class="xref" href="#">???</a> above. </p></div><div class="section" title="1.2.&nbsp;Logs"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="trouble.log"></a>1.2.&nbsp;Logs</h2></div></div></div><p> The key process logs are as follows... (replace &lt;user&gt; with the user that started
      the service, and &lt;hostname&gt; for the machine name) </p><p> NameNode:
        <code class="filename">$HADOOP_HOME/logs/hadoop-&lt;user&gt;-namenode-&lt;hostname&gt;.log</code>
    </p><p> DataNode:
        <code class="filename">$HADOOP_HOME/logs/hadoop-&lt;user&gt;-datanode-&lt;hostname&gt;.log</code>
    </p><p> JobTracker:
        <code class="filename">$HADOOP_HOME/logs/hadoop-&lt;user&gt;-jobtracker-&lt;hostname&gt;.log</code>
    </p><p> TaskTracker:
        <code class="filename">$HADOOP_HOME/logs/hadoop-&lt;user&gt;-tasktracker-&lt;hostname&gt;.log</code>
    </p><p> HMaster:
        <code class="filename">$HBASE_HOME/logs/hbase-&lt;user&gt;-master-&lt;hostname&gt;.log</code>
    </p><p> RegionServer:
        <code class="filename">$HBASE_HOME/logs/hbase-&lt;user&gt;-regionserver-&lt;hostname&gt;.log</code>
    </p><p> ZooKeeper: <code class="filename">TODO</code>
    </p><div class="section" title="1.2.1.&nbsp;Log Locations"><div class="titlepage"><div><div><h3 class="title"><a name="trouble.log.locations"></a>1.2.1.&nbsp;Log Locations</h3></div></div></div><p>For stand-alone deployments the logs are obviously going to be on a single machine,
        however this is a development configuration only. Production deployments need to run on a
        cluster.</p><div class="section" title="1.2.1.1.&nbsp;NameNode"><div class="titlepage"><div><div><h4 class="title"><a name="trouble.log.locations.namenode"></a>1.2.1.1.&nbsp;NameNode</h4></div></div></div><p>The NameNode log is on the NameNode server. The HBase Master is typically run on the
          NameNode server, and well as ZooKeeper.</p><p>For smaller clusters the JobTracker is typically run on the NameNode server as
          well.</p></div><div class="section" title="1.2.1.2.&nbsp;DataNode"><div class="titlepage"><div><div><h4 class="title"><a name="trouble.log.locations.datanode"></a>1.2.1.2.&nbsp;DataNode</h4></div></div></div><p>Each DataNode server will have a DataNode log for HDFS, as well as a RegionServer log
          for HBase.</p><p>Additionally, each DataNode server will also have a TaskTracker log for MapReduce task
          execution.</p></div></div><div class="section" title="1.2.2.&nbsp;Log Levels"><div class="titlepage"><div><div><h3 class="title"><a name="trouble.log.levels"></a>1.2.2.&nbsp;Log Levels</h3></div></div></div><div class="section" title="1.2.2.1.&nbsp;Enabling RPC-level logging"><div class="titlepage"><div><div><h4 class="title"><a name="rpc.logging"></a>1.2.2.1.&nbsp;Enabling RPC-level logging</h4></div></div></div><p>Enabling the RPC-level logging on a RegionServer can often given insight on timings at
          the server. Once enabled, the amount of log spewed is voluminous. It is not recommended
          that you leave this logging on for more than short bursts of time. To enable RPC-level
          logging, browse to the RegionServer UI and click on <span class="emphasis"><em>Log Level</em></span>. Set
          the log level to <code class="varname">DEBUG</code> for the package
            <code class="classname">org.apache.hadoop.ipc</code> (Thats right, for
            <code class="classname">hadoop.ipc</code>, NOT, <code class="classname">hbase.ipc</code>). Then tail the
          RegionServers log. Analyze.</p><p>To disable, set the logging level back to <code class="varname">INFO</code> level. </p></div></div><div class="section" title="1.2.3.&nbsp;JVM Garbage Collection Logs"><div class="titlepage"><div><div><h3 class="title"><a name="trouble.log.gc"></a>1.2.3.&nbsp;JVM Garbage Collection Logs</h3></div></div></div><p>HBase is memory intensive, and using the default GC you can see long pauses in all
        threads including the <span class="emphasis"><em>Juliet Pause</em></span> aka "GC of Death". To help debug
        this or confirm this is happening GC logging can be turned on in the Java virtual machine. </p><p> To enable, in <code class="filename">hbase-env.sh</code>, uncomment one of the below lines
        :</p><pre class="programlisting">
# This enables basic gc logging to the .out file.
# export SERVER_GC_OPTS="-verbose:gc -XX:+PrintGCDetails -XX:+PrintGCDateStamps"

# This enables basic gc logging to its own file.
# export SERVER_GC_OPTS="-verbose:gc -XX:+PrintGCDetails -XX:+PrintGCDateStamps -Xloggc:&lt;FILE-PATH&gt;"

# This enables basic GC logging to its own file with automatic log rolling. Only applies to jdk 1.6.0_34+ and 1.7.0_2+.
# export SERVER_GC_OPTS="-verbose:gc -XX:+PrintGCDetails -XX:+PrintGCDateStamps -Xloggc:&lt;FILE-PATH&gt; -XX:+UseGCLogFileRotation -XX:NumberOfGCLogFiles=1 -XX:GCLogFileSize=512M"

# If &lt;FILE-PATH&gt; is not replaced, the log file(.gc) would be generated in the HBASE_LOG_DIR.
          </pre><p> At this point you should see logs like so:</p><pre class="programlisting">
64898.952: [GC [1 CMS-initial-mark: 2811538K(3055704K)] 2812179K(3061272K), 0.0007360 secs] [Times: user=0.00 sys=0.00, real=0.00 secs]
64898.953: [CMS-concurrent-mark-start]
64898.971: [GC 64898.971: [ParNew: 5567K-&gt;576K(5568K), 0.0101110 secs] 2817105K-&gt;2812715K(3061272K), 0.0102200 secs] [Times: user=0.07 sys=0.00, real=0.01 secs]
          </pre><p> In this section, the first line indicates a 0.0007360 second pause for the CMS to
        initially mark. This pauses the entire VM, all threads for that period of time. </p><p> The third line indicates a "minor GC", which pauses the VM for 0.0101110 seconds - aka
        10 milliseconds. It has reduced the "ParNew" from about 5.5m to 576k. Later on in this cycle
        we see:</p><pre class="programlisting">
64901.445: [CMS-concurrent-mark: 1.542/2.492 secs] [Times: user=10.49 sys=0.33, real=2.49 secs]
64901.445: [CMS-concurrent-preclean-start]
64901.453: [GC 64901.453: [ParNew: 5505K-&gt;573K(5568K), 0.0062440 secs] 2868746K-&gt;2864292K(3061272K), 0.0063360 secs] [Times: user=0.05 sys=0.00, real=0.01 secs]
64901.476: [GC 64901.476: [ParNew: 5563K-&gt;575K(5568K), 0.0072510 secs] 2869283K-&gt;2864837K(3061272K), 0.0073320 secs] [Times: user=0.05 sys=0.01, real=0.01 secs]
64901.500: [GC 64901.500: [ParNew: 5517K-&gt;573K(5568K), 0.0120390 secs] 2869780K-&gt;2865267K(3061272K), 0.0121150 secs] [Times: user=0.09 sys=0.00, real=0.01 secs]
64901.529: [GC 64901.529: [ParNew: 5507K-&gt;569K(5568K), 0.0086240 secs] 2870200K-&gt;2865742K(3061272K), 0.0087180 secs] [Times: user=0.05 sys=0.00, real=0.01 secs]
64901.554: [GC 64901.555: [ParNew: 5516K-&gt;575K(5568K), 0.0107130 secs] 2870689K-&gt;2866291K(3061272K), 0.0107820 secs] [Times: user=0.06 sys=0.00, real=0.01 secs]
64901.578: [CMS-concurrent-preclean: 0.070/0.133 secs] [Times: user=0.48 sys=0.01, real=0.14 secs]
64901.578: [CMS-concurrent-abortable-preclean-start]
64901.584: [GC 64901.584: [ParNew: 5504K-&gt;571K(5568K), 0.0087270 secs] 2871220K-&gt;2866830K(3061272K), 0.0088220 secs] [Times: user=0.05 sys=0.00, real=0.01 secs]
64901.609: [GC 64901.609: [ParNew: 5512K-&gt;569K(5568K), 0.0063370 secs] 2871771K-&gt;2867322K(3061272K), 0.0064230 secs] [Times: user=0.06 sys=0.00, real=0.01 secs]
64901.615: [CMS-concurrent-abortable-preclean: 0.007/0.037 secs] [Times: user=0.13 sys=0.00, real=0.03 secs]
64901.616: [GC[YG occupancy: 645 K (5568 K)]64901.616: [Rescan (parallel) , 0.0020210 secs]64901.618: [weak refs processing, 0.0027950 secs] [1 CMS-remark: 2866753K(3055704K)] 2867399K(3061272K), 0.0049380 secs] [Times: user=0.00 sys=0.01, real=0.01 secs]
64901.621: [CMS-concurrent-sweep-start]
            </pre><p> The first line indicates that the CMS concurrent mark (finding garbage) has taken 2.4
        seconds. But this is a _concurrent_ 2.4 seconds, Java has not been paused at any point in
        time. </p><p> There are a few more minor GCs, then there is a pause at the 2nd last line:
        </p><pre class="programlisting">
64901.616: [GC[YG occupancy: 645 K (5568 K)]64901.616: [Rescan (parallel) , 0.0020210 secs]64901.618: [weak refs processing, 0.0027950 secs] [1 CMS-remark: 2866753K(3055704K)] 2867399K(3061272K), 0.0049380 secs] [Times: user=0.00 sys=0.01, real=0.01 secs]
            </pre><p>
      </p><p> The pause here is 0.0049380 seconds (aka 4.9 milliseconds) to 'remark' the heap. </p><p> At this point the sweep starts, and you can watch the heap size go down:</p><pre class="programlisting">
64901.637: [GC 64901.637: [ParNew: 5501K-&gt;569K(5568K), 0.0097350 secs] 2871958K-&gt;2867441K(3061272K), 0.0098370 secs] [Times: user=0.05 sys=0.00, real=0.01 secs]
...  lines removed ...
64904.936: [GC 64904.936: [ParNew: 5532K-&gt;568K(5568K), 0.0070720 secs] 1365024K-&gt;1360689K(3061272K), 0.0071930 secs] [Times: user=0.05 sys=0.00, real=0.01 secs]
64904.953: [CMS-concurrent-sweep: 2.030/3.332 secs] [Times: user=9.57 sys=0.26, real=3.33 secs]
            </pre><p>At this point, the CMS sweep took 3.332 seconds, and heap went from about ~ 2.8 GB to
        1.3 GB (approximate). </p><p> The key points here is to keep all these pauses low. CMS pauses are always low, but if
        your ParNew starts growing, you can see minor GC pauses approach 100ms, exceed 100ms and hit
        as high at 400ms. </p><p> This can be due to the size of the ParNew, which should be relatively small. If your
        ParNew is very large after running HBase for a while, in one example a ParNew was about
        150MB, then you might have to constrain the size of ParNew (The larger it is, the longer the
        collections take but if its too small, objects are promoted to old gen too quickly). In the
        below we constrain new gen size to 64m. </p><p> Add the below line in <code class="filename">hbase-env.sh</code>:
        </p><pre class="programlisting">
export SERVER_GC_OPTS="$SERVER_GC_OPTS -XX:NewSize=64m -XX:MaxNewSize=64m"
            </pre><p>
      </p><p> Similarly, to enable GC logging for client processes, uncomment one of the below lines
        in <code class="filename">hbase-env.sh</code>:</p><pre class="programlisting">
# This enables basic gc logging to the .out file.
# export CLIENT_GC_OPTS="-verbose:gc -XX:+PrintGCDetails -XX:+PrintGCDateStamps"

# This enables basic gc logging to its own file.
# export CLIENT_GC_OPTS="-verbose:gc -XX:+PrintGCDetails -XX:+PrintGCDateStamps -Xloggc:&lt;FILE-PATH&gt;"

# This enables basic GC logging to its own file with automatic log rolling. Only applies to jdk 1.6.0_34+ and 1.7.0_2+.
# export CLIENT_GC_OPTS="-verbose:gc -XX:+PrintGCDetails -XX:+PrintGCDateStamps -Xloggc:&lt;FILE-PATH&gt; -XX:+UseGCLogFileRotation -XX:NumberOfGCLogFiles=1 -XX:GCLogFileSize=512M"

# If &lt;FILE-PATH&gt; is not replaced, the log file(.gc) would be generated in the HBASE_LOG_DIR .
            </pre><p> For more information on GC pauses, see the <a class="link" href="http://www.cloudera.com/blog/2011/02/avoiding-full-gcs-in-hbase-with-memstore-local-allocation-buffers-part-1/" target="_top">3
          part blog post</a> by Todd Lipcon and <a class="xref" href="#">???</a> above. </p></div></div><div class="section" title="1.3.&nbsp;Resources"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="trouble.resources"></a>1.3.&nbsp;Resources</h2></div></div></div><div class="section" title="1.3.1.&nbsp;search-hadoop.com"><div class="titlepage"><div><div><h3 class="title"><a name="trouble.resources.searchhadoop"></a>1.3.1.&nbsp;search-hadoop.com</h3></div></div></div><p>
        <a class="link" href="http://search-hadoop.com" target="_top">search-hadoop.com</a> indexes all the mailing
        lists and is great for historical searches. Search here first when you have an issue as its
        more than likely someone has already had your problem. </p></div><div class="section" title="1.3.2.&nbsp;Mailing Lists"><div class="titlepage"><div><div><h3 class="title"><a name="trouble.resources.lists"></a>1.3.2.&nbsp;Mailing Lists</h3></div></div></div><p>Ask a question on the <a class="link" href="http://hbase.apache.org/mail-lists.html" target="_top">Apache
          HBase mailing lists</a>. The 'dev' mailing list is aimed at the community of developers
        actually building Apache HBase and for features currently under development, and 'user' is
        generally used for questions on released versions of Apache HBase. Before going to the
        mailing list, make sure your question has not already been answered by searching the mailing
        list archives first. Use <a class="xref" href="#trouble.resources.searchhadoop" title="1.3.1.&nbsp;search-hadoop.com">Section&nbsp;1.3.1, &#8220;search-hadoop.com&#8221;</a>. Take some time
        crafting your question. See <a class="link" href="http://www.mikeash.com/getting_answers.html" target="_top">Getting Answers</a> for ideas on crafting good questions. A quality question that
        includes all context and exhibits evidence the author has tried to find answers in the
        manual and out on lists is more likely to get a prompt response. </p></div><div class="section" title="1.3.3.&nbsp;IRC"><div class="titlepage"><div><div><h3 class="title"><a name="trouble.resources.irc"></a>1.3.3.&nbsp;IRC</h3></div></div></div><p>#hbase on irc.freenode.net</p></div><div class="section" title="1.3.4.&nbsp;JIRA"><div class="titlepage"><div><div><h3 class="title"><a name="trouble.resources.jira"></a>1.3.4.&nbsp;JIRA</h3></div></div></div><p>
        <a class="link" href="https://issues.apache.org/jira/browse/HBASE" target="_top">JIRA</a> is also really
        helpful when looking for Hadoop/HBase-specific issues. </p></div></div><div class="section" title="1.4.&nbsp;Tools"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="trouble.tools"></a>1.4.&nbsp;Tools</h2></div></div></div><div class="section" title="1.4.1.&nbsp;Builtin Tools"><div class="titlepage"><div><div><h3 class="title"><a name="trouble.tools.builtin"></a>1.4.1.&nbsp;Builtin Tools</h3></div></div></div><div class="section" title="1.4.1.1.&nbsp;Master Web Interface"><div class="titlepage"><div><div><h4 class="title"><a name="trouble.tools.builtin.webmaster"></a>1.4.1.1.&nbsp;Master Web Interface</h4></div></div></div><p>The Master starts a web-interface on port 16010 by default. (Up to and including 0.98
          this was port 60010) </p><p>The Master web UI lists created tables and their definition (e.g., ColumnFamilies,
          blocksize, etc.). Additionally, the available RegionServers in the cluster are listed
          along with selected high-level metrics (requests, number of regions, usedHeap, maxHeap).
          The Master web UI allows navigation to each RegionServer's web UI. </p></div><div class="section" title="1.4.1.2.&nbsp;RegionServer Web Interface"><div class="titlepage"><div><div><h4 class="title"><a name="trouble.tools.builtin.webregion"></a>1.4.1.2.&nbsp;RegionServer Web Interface</h4></div></div></div><p>RegionServers starts a web-interface on port 16030 by default. (Up to an including
          0.98 this was port 60030) </p><p>The RegionServer web UI lists online regions and their start/end keys, as well as
          point-in-time RegionServer metrics (requests, regions, storeFileIndexSize,
          compactionQueueSize, etc.). </p><p>See <a class="xref" href="#">???</a> for more information in metric definitions. </p></div><div class="section" title="1.4.1.3.&nbsp;zkcli"><div class="titlepage"><div><div><h4 class="title"><a name="trouble.tools.builtin.zkcli"></a>1.4.1.3.&nbsp;zkcli</h4></div></div></div><p><code class="code">zkcli</code> is a very useful tool for investigating ZooKeeper-related issues.
          To invoke:
          </p><pre class="programlisting">
./hbase zkcli -server host:port &lt;cmd&gt; &lt;args&gt;
</pre><p>
          The commands (and arguments) are:</p><pre class="programlisting">
	connect host:port
	get path [watch]
	ls path [watch]
	set path data [version]
	delquota [-n|-b] path
	quit
	printwatches on|off
	create [-s] [-e] path data acl
	stat path [watch]
	close
	ls2 path [watch]
	history
	listquota path
	setAcl path acl
	getAcl path
	sync path
	redo cmdno
	addauth scheme auth
	delete path [version]
	setquota -n|-b val path
</pre></div></div><div class="section" title="1.4.2.&nbsp;External Tools"><div class="titlepage"><div><div><h3 class="title"><a name="trouble.tools.external"></a>1.4.2.&nbsp;External Tools</h3></div></div></div><div class="section" title="1.4.2.1.&nbsp;tail"><div class="titlepage"><div><div><h4 class="title"><a name="trouble.tools.tail"></a>1.4.2.1.&nbsp;tail</h4></div></div></div><p>
          <code class="code">tail</code> is the command line tool that lets you look at the end of a file. Add
          the &#8220;-f&#8221; option and it will refresh when new data is available. It&#8217;s useful when you are
          wondering what&#8217;s happening, for example, when a cluster is taking a long time to shutdown
          or startup as you can just fire a new terminal and tail the master log (and maybe a few
          RegionServers). </p></div><div class="section" title="1.4.2.2.&nbsp;top"><div class="titlepage"><div><div><h4 class="title"><a name="trouble.tools.top"></a>1.4.2.2.&nbsp;top</h4></div></div></div><p>
          <code class="code">top</code> is probably one of the most important tool when first trying to see
          what&#8217;s running on a machine and how the resources are consumed. Here&#8217;s an example from
          production system:</p><pre class="programlisting">
top - 14:46:59 up 39 days, 11:55,  1 user,  load average: 3.75, 3.57, 3.84
Tasks: 309 total,   1 running, 308 sleeping,   0 stopped,   0 zombie
Cpu(s):  4.5%us,  1.6%sy,  0.0%ni, 91.7%id,  1.4%wa,  0.1%hi,  0.6%si,  0.0%st
Mem:  24414432k total, 24296956k used,   117476k free,     7196k buffers
Swap: 16008732k total,	14348k used, 15994384k free, 11106908k cached

  PID USER  	PR  NI  VIRT  RES  SHR S %CPU %MEM	TIME+  COMMAND
15558 hadoop	18  -2 3292m 2.4g 3556 S   79 10.4   6523:52 java
13268 hadoop	18  -2 8967m 8.2g 4104 S   21 35.1   5170:30 java
 8895 hadoop	18  -2 1581m 497m 3420 S   11  2.1   4002:32 java
&#8230;
        </pre><p> Here we can see that the system load average during the last five minutes is 3.75,
          which very roughly means that on average 3.75 threads were waiting for CPU time during
          these 5 minutes. In general, the &#8220;perfect&#8221; utilization equals to the number of cores,
          under that number the machine is under utilized and over that the machine is over
          utilized. This is an important concept, see this article to understand it more: <a class="link" href="http://www.linuxjournal.com/article/9001" target="_top">http://www.linuxjournal.com/article/9001</a>. </p><p> Apart from load, we can see that the system is using almost all its available RAM but
          most of it is used for the OS cache (which is good). The swap only has a few KBs in it and
          this is wanted, high numbers would indicate swapping activity which is the nemesis of
          performance of Java systems. Another way to detect swapping is when the load average goes
          through the roof (although this could also be caused by things like a dying disk, among
          others). </p><p> The list of processes isn&#8217;t super useful by default, all we know is that 3 java
          processes are using about 111% of the CPUs. To know which is which, simply type &#8220;c&#8221; and
          each line will be expanded. Typing &#8220;1&#8221; will give you the detail of how each CPU is used
          instead of the average for all of them like shown here. </p></div><div class="section" title="1.4.2.3.&nbsp;jps"><div class="titlepage"><div><div><h4 class="title"><a name="trouble.tools.jps"></a>1.4.2.3.&nbsp;jps</h4></div></div></div><p>
          <code class="code">jps</code> is shipped with every JDK and gives the java process ids for the current
          user (if root, then it gives the ids for all users). Example:</p><pre class="programlisting">
hadoop@sv4borg12:~$ jps
1322 TaskTracker
17789 HRegionServer
27862 Child
1158 DataNode
25115 HQuorumPeer
2950 Jps
19750 ThriftServer
18776 jmx
        </pre><p>In order, we see a: </p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem"><p>Hadoop TaskTracker, manages the local Childs</p></li><li class="listitem"><p>HBase RegionServer, serves regions</p></li><li class="listitem"><p>Child, its MapReduce task, cannot tell which type exactly</p></li><li class="listitem"><p>Hadoop TaskTracker, manages the local Childs</p></li><li class="listitem"><p>Hadoop DataNode, serves blocks</p></li><li class="listitem"><p>HQuorumPeer, a ZooKeeper ensemble member</p></li><li class="listitem"><p>Jps, well&#8230; it&#8217;s the current process</p></li><li class="listitem"><p>ThriftServer, it&#8217;s a special one will be running only if thrift was started</p></li><li class="listitem"><p>jmx, this is a local process that&#8217;s part of our monitoring platform ( poorly named
              maybe). You probably don&#8217;t have that.</p></li></ul></div><p> You can then do stuff like checking out the full command line that started the
          process:</p><pre class="programlisting">
hadoop@sv4borg12:~$ ps aux | grep HRegionServer
hadoop   17789  155 35.2 9067824 8604364 ?     S&lt;l  Mar04 9855:48 /usr/java/jdk1.6.0_14/bin/java -Xmx8000m -XX:+DoEscapeAnalysis -XX:+AggressiveOpts -XX:+UseConcMarkSweepGC -XX:NewSize=64m -XX:MaxNewSize=64m -XX:CMSInitiatingOccupancyFraction=88 -verbose:gc -XX:+PrintGCDetails -XX:+PrintGCTimeStamps -Xloggc:/export1/hadoop/logs/gc-hbase.log -Dcom.sun.management.jmxremote.port=10102 -Dcom.sun.management.jmxremote.authenticate=true -Dcom.sun.management.jmxremote.ssl=false -Dcom.sun.management.jmxremote.password.file=/home/hadoop/hbase/conf/jmxremote.password -Dcom.sun.management.jmxremote -Dhbase.log.dir=/export1/hadoop/logs -Dhbase.log.file=hbase-hadoop-regionserver-sv4borg12.log -Dhbase.home.dir=/home/hadoop/hbase -Dhbase.id.str=hadoop -Dhbase.root.logger=INFO,DRFA -Djava.library.path=/home/hadoop/hbase/lib/native/Linux-amd64-64 -classpath /home/hadoop/hbase/bin/../conf:[many jars]:/home/hadoop/hadoop/conf org.apache.hadoop.hbase.regionserver.HRegionServer start
        </pre></div><div class="section" title="1.4.2.4.&nbsp;jstack"><div class="titlepage"><div><div><h4 class="title"><a name="trouble.tools.jstack"></a>1.4.2.4.&nbsp;jstack</h4></div></div></div><p>
          <code class="code">jstack</code> is one of the most important tools when trying to figure out what a
          java process is doing apart from looking at the logs. It has to be used in conjunction
          with jps in order to give it a process id. It shows a list of threads, each one has a
          name, and they appear in the order that they were created (so the top ones are the most
          recent threads). Here&#8217;s a few example: </p><p> The main thread of a RegionServer that&#8217;s waiting for something to do from the
          master:</p><pre class="programlisting">
"regionserver60020" prio=10 tid=0x0000000040ab4000 nid=0x45cf waiting on condition [0x00007f16b6a96000..0x00007f16b6a96a70]
java.lang.Thread.State: TIMED_WAITING (parking)
    at sun.misc.Unsafe.park(Native Method)
    - parking to wait for  &lt;0x00007f16cd5c2f30&gt; (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
    at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:198)
    at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1963)
    at java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:395)
    at org.apache.hadoop.hbase.regionserver.HRegionServer.run(HRegionServer.java:647)
    at java.lang.Thread.run(Thread.java:619)

    The MemStore flusher thread that is currently flushing to a file:
"regionserver60020.cacheFlusher" daemon prio=10 tid=0x0000000040f4e000 nid=0x45eb in Object.wait() [0x00007f16b5b86000..0x00007f16b5b87af0]
java.lang.Thread.State: WAITING (on object monitor)
    at java.lang.Object.wait(Native Method)
    at java.lang.Object.wait(Object.java:485)
    at org.apache.hadoop.ipc.Client.call(Client.java:803)
    - locked &lt;0x00007f16cb14b3a8&gt; (a org.apache.hadoop.ipc.Client$Call)
    at org.apache.hadoop.ipc.RPC$Invoker.invoke(RPC.java:221)
    at $Proxy1.complete(Unknown Source)
    at sun.reflect.GeneratedMethodAccessor38.invoke(Unknown Source)
    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
    at java.lang.reflect.Method.invoke(Method.java:597)
    at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:82)
    at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:59)
    at $Proxy1.complete(Unknown Source)
    at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.closeInternal(DFSClient.java:3390)
    - locked &lt;0x00007f16cb14b470&gt; (a org.apache.hadoop.hdfs.DFSClient$DFSOutputStream)
    at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.close(DFSClient.java:3304)
    at org.apache.hadoop.fs.FSDataOutputStream$PositionCache.close(FSDataOutputStream.java:61)
    at org.apache.hadoop.fs.FSDataOutputStream.close(FSDataOutputStream.java:86)
    at org.apache.hadoop.hbase.io.hfile.HFile$Writer.close(HFile.java:650)
    at org.apache.hadoop.hbase.regionserver.StoreFile$Writer.close(StoreFile.java:853)
    at org.apache.hadoop.hbase.regionserver.Store.internalFlushCache(Store.java:467)
    - locked &lt;0x00007f16d00e6f08&gt; (a java.lang.Object)
    at org.apache.hadoop.hbase.regionserver.Store.flushCache(Store.java:427)
    at org.apache.hadoop.hbase.regionserver.Store.access$100(Store.java:80)
    at org.apache.hadoop.hbase.regionserver.Store$StoreFlusherImpl.flushCache(Store.java:1359)
    at org.apache.hadoop.hbase.regionserver.HRegion.internalFlushcache(HRegion.java:907)
    at org.apache.hadoop.hbase.regionserver.HRegion.internalFlushcache(HRegion.java:834)
    at org.apache.hadoop.hbase.regionserver.HRegion.flushcache(HRegion.java:786)
    at org.apache.hadoop.hbase.regionserver.MemStoreFlusher.flushRegion(MemStoreFlusher.java:250)
    at org.apache.hadoop.hbase.regionserver.MemStoreFlusher.flushRegion(MemStoreFlusher.java:224)
    at org.apache.hadoop.hbase.regionserver.MemStoreFlusher.run(MemStoreFlusher.java:146)
        </pre><p> A handler thread that&#8217;s waiting for stuff to do (like put, delete, scan, etc):</p><pre class="programlisting">
"IPC Server handler 16 on 60020" daemon prio=10 tid=0x00007f16b011d800 nid=0x4a5e waiting on condition [0x00007f16afefd000..0x00007f16afefd9f0]
   java.lang.Thread.State: WAITING (parking)
        	at sun.misc.Unsafe.park(Native Method)
        	- parking to wait for  &lt;0x00007f16cd3f8dd8&gt; (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
        	at java.util.concurrent.locks.LockSupport.park(LockSupport.java:158)
        	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:1925)
        	at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:358)
        	at org.apache.hadoop.hbase.ipc.HBaseServer$Handler.run(HBaseServer.java:1013)
        </pre><p> And one that&#8217;s busy doing an increment of a counter (it&#8217;s in the phase where it&#8217;s
          trying to create a scanner in order to read the last value):</p><pre class="programlisting">
"IPC Server handler 66 on 60020" daemon prio=10 tid=0x00007f16b006e800 nid=0x4a90 runnable [0x00007f16acb77000..0x00007f16acb77cf0]
   java.lang.Thread.State: RUNNABLE
        	at org.apache.hadoop.hbase.regionserver.KeyValueHeap.&lt;init&gt;(KeyValueHeap.java:56)
        	at org.apache.hadoop.hbase.regionserver.StoreScanner.&lt;init&gt;(StoreScanner.java:79)
        	at org.apache.hadoop.hbase.regionserver.Store.getScanner(Store.java:1202)
        	at org.apache.hadoop.hbase.regionserver.HRegion$RegionScanner.&lt;init&gt;(HRegion.java:2209)
        	at org.apache.hadoop.hbase.regionserver.HRegion.instantiateInternalScanner(HRegion.java:1063)
        	at org.apache.hadoop.hbase.regionserver.HRegion.getScanner(HRegion.java:1055)
        	at org.apache.hadoop.hbase.regionserver.HRegion.getScanner(HRegion.java:1039)
        	at org.apache.hadoop.hbase.regionserver.HRegion.getLastIncrement(HRegion.java:2875)
        	at org.apache.hadoop.hbase.regionserver.HRegion.incrementColumnValue(HRegion.java:2978)
        	at org.apache.hadoop.hbase.regionserver.HRegionServer.incrementColumnValue(HRegionServer.java:2433)
        	at sun.reflect.GeneratedMethodAccessor20.invoke(Unknown Source)
        	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
        	at java.lang.reflect.Method.invoke(Method.java:597)
        	at org.apache.hadoop.hbase.ipc.HBaseRPC$Server.call(HBaseRPC.java:560)
        	at org.apache.hadoop.hbase.ipc.HBaseServer$Handler.run(HBaseServer.java:1027)
        </pre><p> A thread that receives data from HDFS:</p><pre class="programlisting">
"IPC Client (47) connection to sv4borg9/10.4.24.40:9000 from hadoop" daemon prio=10 tid=0x00007f16a02d0000 nid=0x4fa3 runnable [0x00007f16b517d000..0x00007f16b517dbf0]
   java.lang.Thread.State: RUNNABLE
        	at sun.nio.ch.EPollArrayWrapper.epollWait(Native Method)
        	at sun.nio.ch.EPollArrayWrapper.poll(EPollArrayWrapper.java:215)
        	at sun.nio.ch.EPollSelectorImpl.doSelect(EPollSelectorImpl.java:65)
        	at sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:69)
        	- locked &lt;0x00007f17d5b68c00&gt; (a sun.nio.ch.Util$1)
        	- locked &lt;0x00007f17d5b68be8&gt; (a java.util.Collections$UnmodifiableSet)
        	- locked &lt;0x00007f1877959b50&gt; (a sun.nio.ch.EPollSelectorImpl)
        	at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:80)
        	at org.apache.hadoop.net.SocketIOWithTimeout$SelectorPool.select(SocketIOWithTimeout.java:332)
        	at org.apache.hadoop.net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:157)
        	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:155)
        	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:128)
        	at java.io.FilterInputStream.read(FilterInputStream.java:116)
        	at org.apache.hadoop.ipc.Client$Connection$PingInputStream.read(Client.java:304)
        	at java.io.BufferedInputStream.fill(BufferedInputStream.java:218)
        	at java.io.BufferedInputStream.read(BufferedInputStream.java:237)
        	- locked &lt;0x00007f1808539178&gt; (a java.io.BufferedInputStream)
        	at java.io.DataInputStream.readInt(DataInputStream.java:370)
        	at org.apache.hadoop.ipc.Client$Connection.receiveResponse(Client.java:569)
        	at org.apache.hadoop.ipc.Client$Connection.run(Client.java:477)
          </pre><p> And here is a master trying to recover a lease after a RegionServer died:</p><pre class="programlisting">
"LeaseChecker" daemon prio=10 tid=0x00000000407ef800 nid=0x76cd waiting on condition [0x00007f6d0eae2000..0x00007f6d0eae2a70]
--
   java.lang.Thread.State: WAITING (on object monitor)
        	at java.lang.Object.wait(Native Method)
        	at java.lang.Object.wait(Object.java:485)
        	at org.apache.hadoop.ipc.Client.call(Client.java:726)
        	- locked &lt;0x00007f6d1cd28f80&gt; (a org.apache.hadoop.ipc.Client$Call)
        	at org.apache.hadoop.ipc.RPC$Invoker.invoke(RPC.java:220)
        	at $Proxy1.recoverBlock(Unknown Source)
        	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.processDatanodeError(DFSClient.java:2636)
        	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.&lt;init&gt;(DFSClient.java:2832)
        	at org.apache.hadoop.hdfs.DFSClient.append(DFSClient.java:529)
        	at org.apache.hadoop.hdfs.DistributedFileSystem.append(DistributedFileSystem.java:186)
        	at org.apache.hadoop.fs.FileSystem.append(FileSystem.java:530)
        	at org.apache.hadoop.hbase.util.FSUtils.recoverFileLease(FSUtils.java:619)
        	at org.apache.hadoop.hbase.regionserver.wal.HLog.splitLog(HLog.java:1322)
        	at org.apache.hadoop.hbase.regionserver.wal.HLog.splitLog(HLog.java:1210)
        	at org.apache.hadoop.hbase.master.HMaster.splitLogAfterStartup(HMaster.java:648)
        	at org.apache.hadoop.hbase.master.HMaster.joinCluster(HMaster.java:572)
        	at org.apache.hadoop.hbase.master.HMaster.run(HMaster.java:503)
          </pre></div><div class="section" title="1.4.2.5.&nbsp;OpenTSDB"><div class="titlepage"><div><div><h4 class="title"><a name="trouble.tools.opentsdb"></a>1.4.2.5.&nbsp;OpenTSDB</h4></div></div></div><p>
          <a class="link" href="http://opentsdb.net" target="_top">OpenTSDB</a> is an excellent alternative to Ganglia
          as it uses Apache HBase to store all the time series and doesn&#8217;t have to downsample.
          Monitoring your own HBase cluster that hosts OpenTSDB is a good exercise. </p><p> Here&#8217;s an example of a cluster that&#8217;s suffering from hundreds of compactions launched
          almost all around the same time, which severely affects the IO performance: (TODO: insert
          graph plotting compactionQueueSize) </p><p> It&#8217;s a good practice to build dashboards with all the important graphs per machine
          and per cluster so that debugging issues can be done with a single quick look. For
          example, at StumbleUpon there&#8217;s one dashboard per cluster with the most important metrics
          from both the OS and Apache HBase. You can then go down at the machine level and get even
          more detailed metrics. </p></div><div class="section" title="1.4.2.6.&nbsp;clusterssh+top"><div class="titlepage"><div><div><h4 class="title"><a name="trouble.tools.clustersshtop"></a>1.4.2.6.&nbsp;clusterssh+top</h4></div></div></div><p> clusterssh+top, it&#8217;s like a poor man&#8217;s monitoring system and it can be quite useful
          when you have only a few machines as it&#8217;s very easy to setup. Starting clusterssh will
          give you one terminal per machine and another terminal in which whatever you type will be
          retyped in every window. This means that you can type &#8220;top&#8221; once and it will start it for
          all of your machines at the same time giving you full view of the current state of your
          cluster. You can also tail all the logs at the same time, edit files, etc. </p></div></div></div><div class="section" title="1.5.&nbsp;Client"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="trouble.client"></a>1.5.&nbsp;Client</h2></div></div></div><p>For more information on the HBase client, see <a class="xref" href="#">???</a>. </p><div class="section" title="1.5.1.&nbsp;ScannerTimeoutException or UnknownScannerException"><div class="titlepage"><div><div><h3 class="title"><a name="trouble.client.scantimeout"></a>1.5.1.&nbsp;ScannerTimeoutException or UnknownScannerException</h3></div></div></div><p>This is thrown if the time between RPC calls from the client to RegionServer exceeds the
        scan timeout. For example, if <code class="code">Scan.setCaching</code> is set to 500, then there will be
        an RPC call to fetch the next batch of rows every 500 <code class="code">.next()</code> calls on the
        ResultScanner because data is being transferred in blocks of 500 rows to the client.
        Reducing the setCaching value may be an option, but setting this value too low makes for
        inefficient processing on numbers of rows. </p><p>See <a class="xref" href="#">???</a>. </p></div><div class="section" title="1.5.2.&nbsp;Performance Differences in Thrift and Java APIs"><div class="titlepage"><div><div><h3 class="title"><a name="d0e396"></a>1.5.2.&nbsp;Performance Differences in Thrift and Java APIs</h3></div></div></div><p>Poor performance, or even <code class="code">ScannerTimeoutExceptions</code>, can occur if
          <code class="code">Scan.setCaching</code> is too high, as discussed in <a class="xref" href="#trouble.client.scantimeout" title="1.5.1.&nbsp;ScannerTimeoutException or UnknownScannerException">Section&nbsp;1.5.1, &#8220;ScannerTimeoutException or UnknownScannerException&#8221;</a>. If the Thrift client uses the wrong caching
        settings for a given workload, performance can suffer compared to the Java API. To set
        caching for a given scan in the Thrift client, use the <code class="code">scannerGetList(scannerId,
          numRows)</code> method, where <code class="code">numRows</code> is an integer representing the number
        of rows to cache. In one case, it was found that reducing the cache for Thrift scans from
        1000 to 100 increased performance to near parity with the Java API given the same
        queries.</p><p>See also Jesse Andersen's <a class="link" href="http://blog.cloudera.com/blog/2014/04/how-to-use-the-hbase-thrift-interface-part-3-using-scans/" target="_top">blog post</a> 
        about using Scans with Thrift.</p></div><div class="section" title="1.5.3.&nbsp;LeaseException when calling Scanner.next"><div class="titlepage"><div><div><h3 class="title"><a name="trouble.client.lease.exception"></a>1.5.3.&nbsp;<code class="classname">LeaseException</code> when calling
        <code class="classname">Scanner.next</code></h3></div></div></div><p> In some situations clients that fetch data from a RegionServer get a LeaseException
        instead of the usual <a class="xref" href="#trouble.client.scantimeout" title="1.5.1.&nbsp;ScannerTimeoutException or UnknownScannerException">Section&nbsp;1.5.1, &#8220;ScannerTimeoutException or UnknownScannerException&#8221;</a>. Usually the source of the exception is
          <code class="classname">org.apache.hadoop.hbase.regionserver.Leases.removeLease(Leases.java:230)</code>
        (line number may vary). It tends to happen in the context of a slow/freezing
        RegionServer#next call. It can be prevented by having <code class="varname">hbase.rpc.timeout</code> &gt;
          <code class="varname">hbase.regionserver.lease.period</code>. Harsh J investigated the issue as part
        of the mailing list thread <a class="link" href="http://mail-archives.apache.org/mod_mbox/hbase-user/201209.mbox/%3CCAOcnVr3R-LqtKhFsk8Bhrm-YW2i9O6J6Fhjz2h7q6_sxvwd2yw%40mail.gmail.com%3E" target="_top">HBase,
          mail # user - Lease does not exist exceptions</a>
      </p></div><div class="section" title="1.5.4.&nbsp;Shell or client application throws lots of scary exceptions during normal operation"><div class="titlepage"><div><div><h3 class="title"><a name="trouble.client.scarylogs"></a>1.5.4.&nbsp;Shell or client application throws lots of scary exceptions during normal
        operation</h3></div></div></div><p>Since 0.20.0 the default log level for <code class="code">org.apache.hadoop.hbase.*</code>is DEBUG. </p><p> On your clients, edit <code class="filename">$HBASE_HOME/conf/log4j.properties</code> and change
        this: <code class="code">log4j.logger.org.apache.hadoop.hbase=DEBUG</code> to this:
          <code class="code">log4j.logger.org.apache.hadoop.hbase=INFO</code>, or even
          <code class="code">log4j.logger.org.apache.hadoop.hbase=WARN</code>. </p></div><div class="section" title="1.5.5.&nbsp;Long Client Pauses With Compression"><div class="titlepage"><div><div><h3 class="title"><a name="trouble.client.longpauseswithcompression"></a>1.5.5.&nbsp;Long Client Pauses With Compression</h3></div></div></div><p>This is a fairly frequent question on the Apache HBase dist-list. The scenario is that a
        client is typically inserting a lot of data into a relatively un-optimized HBase cluster.
        Compression can exacerbate the pauses, although it is not the source of the problem.</p><p>See <a class="xref" href="#">???</a> on the pattern for pre-creating regions and confirm that
        the table isn't starting with a single region.</p><p>See <a class="xref" href="#">???</a> for cluster configuration, particularly
          <code class="code">hbase.hstore.blockingStoreFiles</code>,
          <code class="code">hbase.hregion.memstore.block.multiplier</code>, <code class="code">MAX_FILESIZE</code> (region
        size), and <code class="code">MEMSTORE_FLUSHSIZE.</code>
      </p><p>A slightly longer explanation of why pauses can happen is as follows: Puts are sometimes
        blocked on the MemStores which are blocked by the flusher thread which is blocked because
        there are too many files to compact because the compactor is given too many small files to
        compact and has to compact the same data repeatedly. This situation can occur even with
        minor compactions. Compounding this situation, Apache HBase doesn't compress data in memory.
        Thus, the 64MB that lives in the MemStore could become a 6MB file after compression - which
        results in a smaller StoreFile. The upside is that more data is packed into the same region,
        but performance is achieved by being able to write larger files - which is why HBase waits
        until the flushize before writing a new StoreFile. And smaller StoreFiles become targets for
        compaction. Without compression the files are much bigger and don't need as much compaction,
        however this is at the expense of I/O. </p><p> For additional information, see this thread on <a class="link" href="http://search-hadoop.com/m/WUnLM6ojHm1/Long+client+pauses+with+compression&amp;subj=Long+client+pauses+with+compression" target="_top">Long
          client pauses with compression</a>. </p></div><div class="section" title="1.5.6.&nbsp;Secure Client Connect ([Caused by GSSException: No valid credentials provided...])"><div class="titlepage"><div><div><h3 class="title"><a name="trouble.client.security.rpc.krb"></a>1.5.6.&nbsp;Secure Client Connect ([Caused by GSSException: No valid credentials provided...])</h3></div></div></div><p>You may encounter the following error:</p><pre class="screen">Secure Client Connect ([Caused by GSSException: No valid credentials provided
        (Mechanism level: Request is a replay (34) V PROCESS_TGS)])</pre><p> This issue is caused by bugs in the MIT Kerberos replay_cache component, <a class="link" href="http://krbdev.mit.edu/rt/Ticket/Display.html?id=1201" target="_top">#1201</a> and <a class="link" href="http://krbdev.mit.edu/rt/Ticket/Display.html?id=5924" target="_top">#5924</a>. These bugs
        caused the old version of krb5-server to erroneously block subsequent requests sent from a
        Principal. This caused krb5-server to block the connections sent from one Client (one HTable
        instance with multi-threading connection instances for each regionserver); Messages, such as
          <code class="literal">Request is a replay (34)</code>, are logged in the client log You can ignore
        the messages, because HTable will retry 5 * 10 (50) times for each failed connection by
        default. HTable will throw IOException if any connection to the regionserver fails after the
        retries, so that the user client code for HTable instance can handle it further. </p><p> Alternatively, update krb5-server to a version which solves these issues, such as
        krb5-server-1.10.3. See JIRA <a class="link" href="https://issues.apache.org/jira/browse/HBASE-10379" target="_top">HBASE-10379</a> for more
        details. </p></div><div class="section" title="1.5.7.&nbsp;ZooKeeper Client Connection Errors"><div class="titlepage"><div><div><h3 class="title"><a name="trouble.client.zookeeper"></a>1.5.7.&nbsp;ZooKeeper Client Connection Errors</h3></div></div></div><p>Errors like this...</p><pre class="programlisting">
11/07/05 11:26:41 WARN zookeeper.ClientCnxn: Session 0x0 for server null,
 unexpected error, closing socket connection and attempting reconnect
 java.net.ConnectException: Connection refused: no further information
        at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
        at sun.nio.ch.SocketChannelImpl.finishConnect(Unknown Source)
        at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1078)
 11/07/05 11:26:43 INFO zookeeper.ClientCnxn: Opening socket connection to
 server localhost/127.0.0.1:2181
 11/07/05 11:26:44 WARN zookeeper.ClientCnxn: Session 0x0 for server null,
 unexpected error, closing socket connection and attempting reconnect
 java.net.ConnectException: Connection refused: no further information
        at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
        at sun.nio.ch.SocketChannelImpl.finishConnect(Unknown Source)
        at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1078)
 11/07/05 11:26:45 INFO zookeeper.ClientCnxn: Opening socket connection to
 server localhost/127.0.0.1:2181
</pre><p>... are either due to ZooKeeper being down, or unreachable due to network issues. </p><p>The utility <a class="xref" href="#trouble.tools.builtin.zkcli" title="1.4.1.3.&nbsp;zkcli">Section&nbsp;1.4.1.3, &#8220;zkcli&#8221;</a> may help investigate ZooKeeper issues. </p></div><div class="section" title="1.5.8.&nbsp;Client running out of memory though heap size seems to be stable (but the off-heap/direct heap keeps growing)"><div class="titlepage"><div><div><h3 class="title"><a name="trouble.client.oome.directmemory.leak"></a>1.5.8.&nbsp;Client running out of memory though heap size seems to be stable (but the
        off-heap/direct heap keeps growing)</h3></div></div></div><p> You are likely running into the issue that is described and worked through in the mail
        thread <a class="link" href="http://search-hadoop.com/m/ubhrX8KvcH/Suspected+memory+leak&amp;subj=Re+Suspected+memory+leak" target="_top">HBase,
          mail # user - Suspected memory leak</a> and continued over in <a class="link" href="http://search-hadoop.com/m/p2Agc1Zy7Va/MaxDirectMemorySize+Was%253A+Suspected+memory+leak&amp;subj=Re+FeedbackRe+Suspected+memory+leak" target="_top">HBase,
          mail # dev - FeedbackRe: Suspected memory leak</a>. A workaround is passing your
        client-side JVM a reasonable value for <code class="code">-XX:MaxDirectMemorySize</code>. By default, the
          <code class="varname">MaxDirectMemorySize</code> is equal to your <code class="code">-Xmx</code> max heapsize
        setting (if <code class="code">-Xmx</code> is set). Try seting it to something smaller (for example, one
        user had success setting it to <code class="code">1g</code> when they had a client-side heap of
          <code class="code">12g</code>). If you set it too small, it will bring on <code class="code">FullGCs</code> so keep
        it a bit hefty. You want to make this setting client-side only especially if you are running
        the new experiemental server-side off-heap cache since this feature depends on being able to
        use big direct buffers (You may have to keep separate client-side and server-side config
        dirs). </p></div><div class="section" title="1.5.9.&nbsp;Client Slowdown When Calling Admin Methods (flush, compact, etc.)"><div class="titlepage"><div><div><h3 class="title"><a name="trouble.client.slowdown.admin"></a>1.5.9.&nbsp;Client Slowdown When Calling Admin Methods (flush, compact, etc.)</h3></div></div></div><p> This is a client issue fixed by <a class="link" href="https://issues.apache.org/jira/browse/HBASE-5073" target="_top">HBASE-5073</a> in 0.90.6.
        There was a ZooKeeper leak in the client and the client was getting pummeled by ZooKeeper
        events with each additional invocation of the admin API. </p></div><div class="section" title="1.5.10.&nbsp;Secure Client Cannot Connect ([Caused by GSSException: No valid credentials provided (Mechanism level: Failed to find any Kerberos tgt)])"><div class="titlepage"><div><div><h3 class="title"><a name="trouble.client.security.rpc"></a>1.5.10.&nbsp;Secure Client Cannot Connect ([Caused by GSSException: No valid credentials provided
        (Mechanism level: Failed to find any Kerberos tgt)])</h3></div></div></div><p> There can be several causes that produce this symptom. </p><p> First, check that you have a valid Kerberos ticket. One is required in order to set up
        communication with a secure Apache HBase cluster. Examine the ticket currently in the
        credential cache, if any, by running the klist command line utility. If no ticket is listed,
        you must obtain a ticket by running the kinit command with either a keytab specified, or by
        interactively entering a password for the desired principal. </p><p> Then, consult the <a class="link" href="http://docs.oracle.com/javase/1.5.0/docs/guide/security/jgss/tutorials/Troubleshooting.html" target="_top">Java
          Security Guide troubleshooting section</a>. The most common problem addressed there is
        resolved by setting javax.security.auth.useSubjectCredsOnly system property value to false. </p><p> Because of a change in the format in which MIT Kerberos writes its credentials cache,
        there is a bug in the Oracle JDK 6 Update 26 and earlier that causes Java to be unable to
        read the Kerberos credentials cache created by versions of MIT Kerberos 1.8.1 or higher. If
        you have this problematic combination of components in your environment, to work around this
        problem, first log in with kinit and then immediately refresh the credential cache with
        kinit -R. The refresh will rewrite the credential cache without the problematic formatting. </p><p> Finally, depending on your Kerberos configuration, you may need to install the <a class="link" href="http://docs.oracle.com/javase/1.4.2/docs/guide/security/jce/JCERefGuide.html" target="_top">Java
          Cryptography Extension</a>, or JCE. Insure the JCE jars are on the classpath on both
        server and client systems. </p><p> You may also need to download the <a class="link" href="http://www.oracle.com/technetwork/java/javase/downloads/jce-6-download-429243.html" target="_top">unlimited
          strength JCE policy files</a>. Uncompress and extract the downloaded file, and install
        the policy jars into &lt;java-home&gt;/lib/security. </p></div></div><div class="section" title="1.6.&nbsp;MapReduce"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="trouble.mapreduce"></a>1.6.&nbsp;MapReduce</h2></div></div></div><div class="section" title="1.6.1.&nbsp;You Think You're On The Cluster, But You're Actually Local"><div class="titlepage"><div><div><h3 class="title"><a name="trouble.mapreduce.local"></a>1.6.1.&nbsp;You Think You're On The Cluster, But You're Actually Local</h3></div></div></div><p>This following stacktrace happened using <code class="code">ImportTsv</code>, but things like this
        can happen on any job with a mis-configuration.</p><pre class="programlisting">
    WARN mapred.LocalJobRunner: job_local_0001
java.lang.IllegalArgumentException: Can't read partitions file
       at org.apache.hadoop.hbase.mapreduce.hadoopbackport.TotalOrderPartitioner.setConf(TotalOrderPartitioner.java:111)
       at org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:62)
       at org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:117)
       at org.apache.hadoop.mapred.MapTask$NewOutputCollector.&lt;init&gt;(MapTask.java:560)
       at org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:639)
       at org.apache.hadoop.mapred.MapTask.run(MapTask.java:323)
       at org.apache.hadoop.mapred.LocalJobRunner$Job.run(LocalJobRunner.java:210)
Caused by: java.io.FileNotFoundException: File _partition.lst does not exist.
       at org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:383)
       at org.apache.hadoop.fs.FilterFileSystem.getFileStatus(FilterFileSystem.java:251)
       at org.apache.hadoop.fs.FileSystem.getLength(FileSystem.java:776)
       at org.apache.hadoop.io.SequenceFile$Reader.&lt;init&gt;(SequenceFile.java:1424)
       at org.apache.hadoop.io.SequenceFile$Reader.&lt;init&gt;(SequenceFile.java:1419)
       at org.apache.hadoop.hbase.mapreduce.hadoopbackport.TotalOrderPartitioner.readPartitions(TotalOrderPartitioner.java:296)
</pre><p>.. see the critical portion of the stack? It's...</p><pre class="programlisting">
at org.apache.hadoop.mapred.LocalJobRunner$Job.run(LocalJobRunner.java:210)
</pre><p>LocalJobRunner means the job is running locally, not on the cluster. </p><p>To solve this problem, you should run your MR job with your
          <code class="code">HADOOP_CLASSPATH</code> set to include the HBase dependencies. The "hbase classpath"
        utility can be used to do this easily. For example (substitute VERSION with your HBase
        version):</p><pre class="programlisting">
          HADOOP_CLASSPATH=`hbase classpath` hadoop jar $HBASE_HOME/hbase-server-VERSION.jar rowcounter usertable
      </pre><p>See <a class="link" href="http://hbase.apache.org/apidocs/org/apache/hadoop/hbase/mapreduce/package-summary.html#classpath" target="_top">
          http://hbase.apache.org/apidocs/org/apache/hadoop/hbase/mapreduce/package-summary.html#classpath</a>
        for more information on HBase MapReduce jobs and classpaths. </p></div><div class="section" title="1.6.2.&nbsp;Launching a job, you get java.lang.IllegalAccessError: com/google/protobuf/HBaseZeroCopyByteString or class com.google.protobuf.ZeroCopyLiteralByteString cannot access its superclass com.google.protobuf.LiteralByteString"><div class="titlepage"><div><div><h3 class="title"><a name="trouble.hbasezerocopybytestring"></a>1.6.2.&nbsp;Launching a job, you get java.lang.IllegalAccessError: com/google/protobuf/HBaseZeroCopyByteString or class com.google.protobuf.ZeroCopyLiteralByteString cannot access its superclass com.google.protobuf.LiteralByteString</h3></div></div></div><p>See <a class="link" href="https://issues.apache.org/jira/browse/HBASE-10304" target="_top">HBASE-10304 Running an hbase job jar: IllegalAccessError: class com.google.protobuf.ZeroCopyLiteralByteString cannot access its superclass com.google.protobuf.LiteralByteString</a> and <a class="link" href="https://issues.apache.org/jira/browse/HBASE-11118" target="_top">HBASE-11118 non environment variable solution for "IllegalAccessError: class com.google.protobuf.ZeroCopyLiteralByteString cannot access its superclass com.google.protobuf.LiteralByteString"</a>.  The issue can also show up
          when trying to run spark jobs.  See <a class="link" href="https://issues.apache.org/jira/browse/HBASE-10877" target="_top">HBASE-10877 HBase non-retriable exception list should be expanded</a>.
      </p></div></div><div class="section" title="1.7.&nbsp;NameNode"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="trouble.namenode"></a>1.7.&nbsp;NameNode</h2></div></div></div><p>For more information on the NameNode, see <a class="xref" href="#">???</a>. </p><div class="section" title="1.7.1.&nbsp;HDFS Utilization of Tables and Regions"><div class="titlepage"><div><div><h3 class="title"><a name="trouble.namenode.disk"></a>1.7.1.&nbsp;HDFS Utilization of Tables and Regions</h3></div></div></div><p>To determine how much space HBase is using on HDFS use the <code class="code">hadoop</code> shell
        commands from the NameNode. For example... </p><pre class="programlisting">hadoop fs -dus /hbase/</pre><p> ...returns the summarized disk
        utilization for all HBase objects. </p><pre class="programlisting">hadoop fs -dus /hbase/myTable</pre><p> ...returns the summarized
        disk utilization for the HBase table 'myTable'. </p><pre class="programlisting">hadoop fs -du /hbase/myTable</pre><p> ...returns a list of the
        regions under the HBase table 'myTable' and their disk utilization. </p><p>For more information on HDFS shell commands, see the <a class="link" href="http://hadoop.apache.org/common/docs/current/file_system_shell.html" target="_top">HDFS
          FileSystem Shell documentation</a>. </p></div><div class="section" title="1.7.2.&nbsp;Browsing HDFS for HBase Objects"><div class="titlepage"><div><div><h3 class="title"><a name="trouble.namenode.hbase.objects"></a>1.7.2.&nbsp;Browsing HDFS for HBase Objects</h3></div></div></div><p>Sometimes it will be necessary to explore the HBase objects that exist on HDFS. These
        objects could include the WALs (Write Ahead Logs), tables, regions, StoreFiles, etc. The
        easiest way to do this is with the NameNode web application that runs on port 50070. The
        NameNode web application will provide links to the all the DataNodes in the cluster so that
        they can be browsed seamlessly. </p><p>The HDFS directory structure of HBase tables in the cluster is...
        </p><pre class="programlisting">
<code class="filename">/hbase</code>
     <code class="filename">/&lt;Table&gt;</code>             (Tables in the cluster)
          <code class="filename">/&lt;Region&gt;</code>           (Regions for the table)
               <code class="filename">/&lt;ColumnFamily&gt;</code>      (ColumnFamilies for the Region for the table)
                    <code class="filename">/&lt;StoreFile&gt;</code>        (StoreFiles for the ColumnFamily for the Regions for the table)
            </pre><p>
      </p><p>The HDFS directory structure of HBase WAL is..
        </p><pre class="programlisting">
<code class="filename">/hbase</code>
     <code class="filename">/.logs</code>
          <code class="filename">/&lt;RegionServer&gt;</code>    (RegionServers)
               <code class="filename">/&lt;HLog&gt;</code>           (WAL HLog files for the RegionServer)
            </pre><p>
      </p><p>See the <a class="link" href="http://hadoop.apache.org/common/docs/current/hdfs_user_guide.html" target="_top">HDFS User
          Guide</a> for other non-shell diagnostic utilities like <code class="code">fsck</code>. </p><div class="section" title="1.7.2.1.&nbsp;Zero size HLogs with data in them"><div class="titlepage"><div><div><h4 class="title"><a name="trouble.namenode.0size.hlogs"></a>1.7.2.1.&nbsp;Zero size HLogs with data in them</h4></div></div></div><p>Problem: when getting a listing of all the files in a region server's .logs directory,
          one file has a size of 0 but it contains data.</p><p>Answer: It's an HDFS quirk. A file that's currently being to will appear to have a
          size of 0 but once it's closed it will show its true size</p></div><div class="section" title="1.7.2.2.&nbsp;Use Cases"><div class="titlepage"><div><div><h4 class="title"><a name="trouble.namenode.uncompaction"></a>1.7.2.2.&nbsp;Use Cases</h4></div></div></div><p>Two common use-cases for querying HDFS for HBase objects is research the degree of
          uncompaction of a table. If there are a large number of StoreFiles for each ColumnFamily
          it could indicate the need for a major compaction. Additionally, after a major compaction
          if the resulting StoreFile is "small" it could indicate the need for a reduction of
          ColumnFamilies for the table. </p></div></div></div><div class="section" title="1.8.&nbsp;Network"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="trouble.network"></a>1.8.&nbsp;Network</h2></div></div></div><div class="section" title="1.8.1.&nbsp;Network Spikes"><div class="titlepage"><div><div><h3 class="title"><a name="trouble.network.spikes"></a>1.8.1.&nbsp;Network Spikes</h3></div></div></div><p>If you are seeing periodic network spikes you might want to check the
          <code class="code">compactionQueues</code> to see if major compactions are happening. </p><p>See <a class="xref" href="#">???</a> for more information on managing compactions. </p></div><div class="section" title="1.8.2.&nbsp;Loopback IP"><div class="titlepage"><div><div><h3 class="title"><a name="trouble.network.loopback"></a>1.8.2.&nbsp;Loopback IP</h3></div></div></div><p>HBase expects the loopback IP Address to be 127.0.0.1. See the Getting Started section
        on <a class="xref" href="#">???</a>. </p></div><div class="section" title="1.8.3.&nbsp;Network Interfaces"><div class="titlepage"><div><div><h3 class="title"><a name="trouble.network.ints"></a>1.8.3.&nbsp;Network Interfaces</h3></div></div></div><p>Are all the network interfaces functioning correctly? Are you sure? See the
        Troubleshooting Case Study in <a class="xref" href="#trouble.casestudy" title="1.17.&nbsp;Case Studies">Section&nbsp;1.17, &#8220;Case Studies&#8221;</a>. </p></div></div><div class="section" title="1.9.&nbsp;RegionServer"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="trouble.rs"></a>1.9.&nbsp;RegionServer</h2></div></div></div><p>For more information on the RegionServers, see <a class="xref" href="#">???</a>. </p><div class="section" title="1.9.1.&nbsp;Startup Errors"><div class="titlepage"><div><div><h3 class="title"><a name="trouble.rs.startup"></a>1.9.1.&nbsp;Startup Errors</h3></div></div></div><div class="section" title="1.9.1.1.&nbsp;Master Starts, But RegionServers Do Not"><div class="titlepage"><div><div><h4 class="title"><a name="trouble.rs.startup.master-no-region"></a>1.9.1.1.&nbsp;Master Starts, But RegionServers Do Not</h4></div></div></div><p>The Master believes the RegionServers have the IP of 127.0.0.1 - which is localhost
          and resolves to the master's own localhost. </p><p>The RegionServers are erroneously informing the Master that their IP addresses are
          127.0.0.1. </p><p>Modify <code class="filename">/etc/hosts</code> on the region servers, from...</p><pre class="programlisting">
# Do not remove the following line, or various programs
# that require network functionality will fail.
127.0.0.1               fully.qualified.regionservername regionservername  localhost.localdomain localhost
::1             localhost6.localdomain6 localhost6
            </pre><p>... to (removing the master node's name from localhost)...</p><pre class="programlisting">
# Do not remove the following line, or various programs
# that require network functionality will fail.
127.0.0.1               localhost.localdomain localhost
::1             localhost6.localdomain6 localhost6
            </pre></div><div class="section" title="1.9.1.2.&nbsp;Compression Link Errors"><div class="titlepage"><div><div><h4 class="title"><a name="trouble.rs.startup.compression"></a>1.9.1.2.&nbsp;Compression Link Errors</h4></div></div></div><p> Since compression algorithms such as LZO need to be installed and configured on each
          cluster this is a frequent source of startup error. If you see messages like
          this...</p><pre class="programlisting">
11/02/20 01:32:15 ERROR lzo.GPLNativeCodeLoader: Could not load native gpl library
java.lang.UnsatisfiedLinkError: no gplcompression in java.library.path
        at java.lang.ClassLoader.loadLibrary(ClassLoader.java:1734)
        at java.lang.Runtime.loadLibrary0(Runtime.java:823)
        at java.lang.System.loadLibrary(System.java:1028)
            </pre><p>.. then there is a path issue with the compression libraries. See the Configuration
          section on <a class="link" href="#">LZO compression configuration</a>. </p></div></div><div class="section" title="1.9.2.&nbsp;Runtime Errors"><div class="titlepage"><div><div><h3 class="title"><a name="trouble.rs.runtime"></a>1.9.2.&nbsp;Runtime Errors</h3></div></div></div><div class="section" title="1.9.2.1.&nbsp;RegionServer Hanging"><div class="titlepage"><div><div><h4 class="title"><a name="trouble.rs.runtime.hang"></a>1.9.2.1.&nbsp;RegionServer Hanging</h4></div></div></div><p> Are you running an old JVM (&lt; 1.6.0_u21?)? When you look at a thread dump, does it
          look like threads are BLOCKED but no one holds the lock all are blocked on? See <a class="link" href="https://issues.apache.org/jira/browse/HBASE-3622" target="_top">HBASE 3622 Deadlock in
            HBaseServer (JVM bug?)</a>. Adding <code class="code">-XX:+UseMembar</code> to the HBase
            <code class="varname">HBASE_OPTS</code> in <code class="filename">conf/hbase-env.sh</code> may fix it.
        </p></div><div class="section" title="1.9.2.2.&nbsp;java.io.IOException...(Too many open files)"><div class="titlepage"><div><div><h4 class="title"><a name="trouble.rs.runtime.filehandles"></a>1.9.2.2.&nbsp;java.io.IOException...(Too many open files)</h4></div></div></div><p> If you see log messages like this...</p><pre class="programlisting">
2010-09-13 01:24:17,336 WARN org.apache.hadoop.hdfs.server.datanode.DataNode:
Disk-related IOException in BlockReceiver constructor. Cause is java.io.IOException: Too many open files
        at java.io.UnixFileSystem.createFileExclusively(Native Method)
        at java.io.File.createNewFile(File.java:883)
</pre><p>... see the Getting Started section on <a class="link" href="#">ulimit and nproc configuration</a>. </p></div><div class="section" title="1.9.2.3.&nbsp;xceiverCount 258 exceeds the limit of concurrent xcievers 256"><div class="titlepage"><div><div><h4 class="title"><a name="trouble.rs.runtime.xceivers"></a>1.9.2.3.&nbsp;xceiverCount 258 exceeds the limit of concurrent xcievers 256</h4></div></div></div><p> This typically shows up in the DataNode logs. </p><p> See the Getting Started section on <a class="link" href="#">xceivers configuration</a>. </p></div><div class="section" title="1.9.2.4.&nbsp;System instability, and the presence of &#34;java.lang.OutOfMemoryError: unable to create new native thread in exceptions&#34; HDFS DataNode logs or that of any system daemon"><div class="titlepage"><div><div><h4 class="title"><a name="trouble.rs.runtime.oom-nt"></a>1.9.2.4.&nbsp;System instability, and the presence of "java.lang.OutOfMemoryError: unable to create
          new native thread in exceptions" HDFS DataNode logs or that of any system daemon</h4></div></div></div><p> See the Getting Started section on <a class="link" href="#">ulimit and nproc configuration</a>. The default on recent Linux
          distributions is 1024 - which is far too low for HBase. </p></div><div class="section" title="1.9.2.5.&nbsp;DFS instability and/or RegionServer lease timeouts"><div class="titlepage"><div><div><h4 class="title"><a name="trouble.rs.runtime.gc"></a>1.9.2.5.&nbsp;DFS instability and/or RegionServer lease timeouts</h4></div></div></div><p> If you see warning messages like this...</p><pre class="programlisting">
2009-02-24 10:01:33,516 WARN org.apache.hadoop.hbase.util.Sleeper: We slept xxx ms, ten times longer than scheduled: 10000
2009-02-24 10:01:33,516 WARN org.apache.hadoop.hbase.util.Sleeper: We slept xxx ms, ten times longer than scheduled: 15000
2009-02-24 10:01:36,472 WARN org.apache.hadoop.hbase.regionserver.HRegionServer: unable to report to master for xxx milliseconds - retrying
           </pre><p>... or see full GC compactions then you may be experiencing full GC's. </p></div><div class="section" title="1.9.2.6.&nbsp;&#34;No live nodes contain current block&#34; and/or YouAreDeadException"><div class="titlepage"><div><div><h4 class="title"><a name="trouble.rs.runtime.nolivenodes"></a>1.9.2.6.&nbsp;"No live nodes contain current block" and/or YouAreDeadException</h4></div></div></div><p> These errors can happen either when running out of OS file handles or in periods of
          severe network problems where the nodes are unreachable. </p><p> See the Getting Started section on <a class="link" href="#">ulimit and nproc configuration</a> and check your network. </p></div><div class="section" title="1.9.2.7.&nbsp;ZooKeeper SessionExpired events"><div class="titlepage"><div><div><h4 class="title"><a name="trouble.rs.runtime.zkexpired"></a>1.9.2.7.&nbsp;ZooKeeper SessionExpired events</h4></div></div></div><p>Master or RegionServers shutting down with messages like those in the logs: </p><pre class="programlisting">
WARN org.apache.zookeeper.ClientCnxn: Exception
closing session 0x278bd16a96000f to sun.nio.ch.SelectionKeyImpl@355811ec
java.io.IOException: TIMED OUT
       at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:906)
WARN org.apache.hadoop.hbase.util.Sleeper: We slept 79410ms, ten times longer than scheduled: 5000
INFO org.apache.zookeeper.ClientCnxn: Attempting connection to server hostname/IP:PORT
INFO org.apache.zookeeper.ClientCnxn: Priming connection to java.nio.channels.SocketChannel[connected local=/IP:PORT remote=hostname/IP:PORT]
INFO org.apache.zookeeper.ClientCnxn: Server connection successful
WARN org.apache.zookeeper.ClientCnxn: Exception closing session 0x278bd16a96000d to sun.nio.ch.SelectionKeyImpl@3544d65e
java.io.IOException: Session Expired
       at org.apache.zookeeper.ClientCnxn$SendThread.readConnectResult(ClientCnxn.java:589)
       at org.apache.zookeeper.ClientCnxn$SendThread.doIO(ClientCnxn.java:709)
       at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:945)
ERROR org.apache.hadoop.hbase.regionserver.HRegionServer: ZooKeeper session expired
           </pre><p> The JVM is doing a long running garbage collecting which is pausing every threads
          (aka "stop the world"). Since the RegionServer's local ZooKeeper client cannot send
          heartbeats, the session times out. By design, we shut down any node that isn't able to
          contact the ZooKeeper ensemble after getting a timeout so that it stops serving data that
          may already be assigned elsewhere. </p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem"><p>Make sure you give plenty of RAM (in <code class="filename">hbase-env.sh</code>), the
              default of 1GB won't be able to sustain long running imports.</p></li><li class="listitem"><p>Make sure you don't swap, the JVM never behaves well under swapping.</p></li><li class="listitem"><p>Make sure you are not CPU starving the RegionServer thread. For example, if you
              are running a MapReduce job using 6 CPU-intensive tasks on a machine with 4 cores, you
              are probably starving the RegionServer enough to create longer garbage collection
              pauses.</p></li><li class="listitem"><p>Increase the ZooKeeper session timeout</p></li></ul></div><p>If you wish to increase the session timeout, add the following to your
            <code class="filename">hbase-site.xml</code> to increase the timeout from the default of 60
          seconds to 120 seconds. </p><pre class="programlisting">
&lt;property&gt;
    &lt;name&gt;zookeeper.session.timeout&lt;/name&gt;
    &lt;value&gt;1200000&lt;/value&gt;
&lt;/property&gt;
&lt;property&gt;
    &lt;name&gt;hbase.zookeeper.property.tickTime&lt;/name&gt;
    &lt;value&gt;6000&lt;/value&gt;
&lt;/property&gt;
            </pre><p>
           Be aware that setting a higher timeout means that the regions served by a failed RegionServer will take at least
           that amount of time to be transfered to another RegionServer. For a production system serving live requests, we would instead
           recommend setting it lower than 1 minute and over-provision your cluster in order the lower the memory load on each machines (hence having
           less garbage to collect per machine).
           </p><p>
           If this is happening during an upload which only happens once (like initially loading all your data into HBase), consider bulk loading.
           </p><p>See <a class="xref" href="#trouble.zookeeper.general" title="1.11.2.&nbsp;ZooKeeper, The Cluster Canary">Section&nbsp;1.11.2, &#8220;ZooKeeper, The Cluster Canary&#8221;</a> for other general information about ZooKeeper troubleshooting.
</p></div><div class="section" title="1.9.2.8.&nbsp;NotServingRegionException"><div class="titlepage"><div><div><h4 class="title"><a name="trouble.rs.runtime.notservingregion"></a>1.9.2.8.&nbsp;NotServingRegionException</h4></div></div></div><p>This exception is "normal" when found in the RegionServer logs at DEBUG level.  This exception is returned back to the client
           and then the client goes back to hbase:meta to find the new location of the moved region.</p><p>However, if the NotServingRegionException is logged ERROR, then the client ran out of retries and something probably wrong.</p></div><div class="section" title="1.9.2.9.&nbsp;Regions listed by domain name, then IP"><div class="titlepage"><div><div><h4 class="title"><a name="trouble.rs.runtime.double_listed_regions"></a>1.9.2.9.&nbsp;Regions listed by domain name, then IP</h4></div></div></div><p>
           Fix your DNS.  In versions of Apache HBase before 0.92.x, reverse DNS needs to give same answer
           as forward lookup. See <a class="link" href="https://issues.apache.org/jira/browse/HBASE-3431" target="_top">HBASE 3431
           RegionServer is not using the name given it by the master; double entry in master listing of servers</a> for gorey details.
          </p></div><div class="section" title="1.9.2.10.&nbsp;Logs flooded with '2011-01-10 12:40:48,407 INFO org.apache.hadoop.io.compress.CodecPool: Got brand-new compressor' messages"><div class="titlepage"><div><div><h4 class="title"><a name="brand.new.compressor"></a>1.9.2.10.&nbsp;Logs flooded with '2011-01-10 12:40:48,407 INFO org.apache.hadoop.io.compress.CodecPool: Got
            brand-new compressor' messages</h4></div></div></div><p>We are not using the native versions of compression
                    libraries.  See <a class="link" href="https://issues.apache.org/jira/browse/HBASE-1900" target="_top">HBASE-1900 Put back native support when hadoop 0.21 is released</a>.
                    Copy the native libs from hadoop under hbase lib dir or
                    symlink them into place and the message should go away.
                </p></div><div class="section" title="1.9.2.11.&nbsp;Server handler X on 60020 caught: java.nio.channels.ClosedChannelException"><div class="titlepage"><div><div><h4 class="title"><a name="trouble.rs.runtime.client_went_away"></a>1.9.2.11.&nbsp;Server handler X on 60020 caught: java.nio.channels.ClosedChannelException</h4></div></div></div><p>
           If you see this type of message it means that the region server was trying to read/send data from/to a client but
           it already went away. Typical causes for this are if the client was killed (you see a storm of messages like this when a MapReduce
           job is killed or fails) or if the client receives a SocketTimeoutException. It's harmless, but you should consider digging in
           a bit more if you aren't doing something to trigger them.
           </p></div></div><div class="section" title="1.9.3.&nbsp;Snapshot Errors Due to Reverse DNS"><div class="titlepage"><div><div><h3 class="title"><a name="d0e942"></a>1.9.3.&nbsp;Snapshot Errors Due to Reverse DNS</h3></div></div></div><p>Several operations within HBase, including snapshots, rely on properly configured
        reverse DNS. Some environments, such as Amazon EC2, have trouble with reverse DNS. If you
        see errors like the following on your RegionServers, check your reverse DNS configuration:</p><pre class="screen">
2013-05-01 00:04:56,356 DEBUG org.apache.hadoop.hbase.procedure.Subprocedure: Subprocedure 'backup1' 
coordinator notified of 'acquire', waiting on 'reached' or 'abort' from coordinator.        
      </pre><p>In general, the hostname reported by the RegionServer needs to be the same as the
        hostname the Master is trying to reach. You can see a hostname mismatch by looking for the
        following type of message in the RegionServer's logs at start-up.</p><pre class="screen">
2013-05-01 00:03:00,614 INFO org.apache.hadoop.hbase.regionserver.HRegionServer: Master passed us hostname 
to use. Was=myhost-1234, Now=ip-10-55-88-99.ec2.internal        
      </pre></div><div class="section" title="1.9.4.&nbsp;Shutdown Errors"><div class="titlepage"><div><div><h3 class="title"><a name="trouble.rs.shutdown"></a>1.9.4.&nbsp;Shutdown Errors</h3></div></div></div><p></p></div></div><div class="section" title="1.10.&nbsp;Master"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="trouble.master"></a>1.10.&nbsp;Master</h2></div></div></div><p>For more information on the Master, see <a class="xref" href="#">???</a>.
       </p><div class="section" title="1.10.1.&nbsp;Startup Errors"><div class="titlepage"><div><div><h3 class="title"><a name="trouble.master.startup"></a>1.10.1.&nbsp;Startup Errors</h3></div></div></div><div class="section" title="1.10.1.1.&nbsp;Master says that you need to run the hbase migrations script"><div class="titlepage"><div><div><h4 class="title"><a name="trouble.master.startup.migration"></a>1.10.1.1.&nbsp;Master says that you need to run the hbase migrations script</h4></div></div></div><p>Upon running that, the hbase migrations script says no files in root directory.</p><p>HBase expects the root directory to either not exist, or to have already been initialized by hbase running a previous time. If you create a new directory for HBase using Hadoop DFS, this error will occur.
             Make sure the HBase root directory does not currently exist or has been initialized by a previous run of HBase. Sure fire solution is to just use Hadoop dfs to delete the HBase root and let HBase create and initialize the directory itself.
             </p></div><div class="section" title="1.10.1.2.&nbsp;Packet len6080218 is out of range!"><div class="titlepage"><div><div><h4 class="title"><a name="trouble.master.startup.zk.buffer"></a>1.10.1.2.&nbsp;Packet len6080218 is out of range!</h4></div></div></div><p>If you have many regions on your cluster and you see an error
                  like that reported above in this sections title in your logs, see
                  <a class="link" href="https://issues.apache.org/jira/browse/HBASE-4246" target="_top">HBASE-4246 Cluster with too many regions cannot withstand some master failover scenarios</a>.</p></div></div><div class="section" title="1.10.2.&nbsp;Shutdown Errors"><div class="titlepage"><div><div><h3 class="title"><a name="trouble.master.shutdown"></a>1.10.2.&nbsp;Shutdown Errors</h3></div></div></div><p></p></div></div><div class="section" title="1.11.&nbsp;ZooKeeper"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="trouble.zookeeper"></a>1.11.&nbsp;ZooKeeper</h2></div></div></div><div class="section" title="1.11.1.&nbsp;Startup Errors"><div class="titlepage"><div><div><h3 class="title"><a name="trouble.zookeeper.startup"></a>1.11.1.&nbsp;Startup Errors</h3></div></div></div><div class="section" title="1.11.1.1.&nbsp;Could not find my address: xyz in list of ZooKeeper quorum servers"><div class="titlepage"><div><div><h4 class="title"><a name="trouble.zookeeper.startup.address"></a>1.11.1.1.&nbsp;Could not find my address: xyz in list of ZooKeeper quorum servers</h4></div></div></div><p>A ZooKeeper server wasn't able to start, throws that error. xyz is the name of your server.</p><p>This is a name lookup problem. HBase tries to start a ZooKeeper server on some machine but that machine isn't able to find itself in the <code class="varname">hbase.zookeeper.quorum</code> configuration.
             </p><p>Use the hostname presented in the error message instead of the value you used. If you have a DNS server, you can set <code class="varname">hbase.zookeeper.dns.interface</code> and <code class="varname">hbase.zookeeper.dns.nameserver</code> in <code class="filename">hbase-site.xml</code> to make sure it resolves to the correct FQDN.
             </p></div></div><div class="section" title="1.11.2.&nbsp;ZooKeeper, The Cluster Canary"><div class="titlepage"><div><div><h3 class="title"><a name="trouble.zookeeper.general"></a>1.11.2.&nbsp;ZooKeeper, The Cluster Canary</h3></div></div></div><p>ZooKeeper is the cluster's "canary in the mineshaft". It'll be the first to notice issues if any so making sure its happy is the short-cut to a humming cluster.
          </p><p>
          See the <a class="link" href="http://wiki.apache.org/hadoop/ZooKeeper/Troubleshooting" target="_top">ZooKeeper Operating Environment Troubleshooting</a> page. It has suggestions and tools for checking disk and networking performance; i.e. the operating environment your ZooKeeper and HBase are running in.
          </p><p>Additionally, the utility <a class="xref" href="#trouble.tools.builtin.zkcli" title="1.4.1.3.&nbsp;zkcli">Section&nbsp;1.4.1.3, &#8220;zkcli&#8221;</a> may help investigate ZooKeeper issues.
         </p></div></div><div class="section" title="1.12.&nbsp;Amazon EC2"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="trouble.ec2"></a>1.12.&nbsp;Amazon EC2</h2></div></div></div><div class="section" title="1.12.1.&nbsp;ZooKeeper does not seem to work on Amazon EC2"><div class="titlepage"><div><div><h3 class="title"><a name="trouble.ec2.zookeeper"></a>1.12.1.&nbsp;ZooKeeper does not seem to work on Amazon EC2</h3></div></div></div><p>HBase does not start when deployed as Amazon EC2 instances.  Exceptions like the below appear in the Master and/or RegionServer logs: </p><pre class="programlisting">
  2009-10-19 11:52:27,030 INFO org.apache.zookeeper.ClientCnxn: Attempting
  connection to server ec2-174-129-15-236.compute-1.amazonaws.com/10.244.9.171:2181
  2009-10-19 11:52:27,032 WARN org.apache.zookeeper.ClientCnxn: Exception
  closing session 0x0 to sun.nio.ch.SelectionKeyImpl@656dc861
  java.net.ConnectException: Connection refused
             </pre><p>
             Security group policy is blocking the ZooKeeper port on a public address.
             Use the internal EC2 host names when configuring the ZooKeeper quorum peer list.
             </p></div><div class="section" title="1.12.2.&nbsp;Instability on Amazon EC2"><div class="titlepage"><div><div><h3 class="title"><a name="trouble.ec2.instability"></a>1.12.2.&nbsp;Instability on Amazon EC2</h3></div></div></div><p>Questions on HBase and Amazon EC2 come up frequently on the HBase dist-list. Search for old threads using <a class="link" href="http://search-hadoop.com/" target="_top">Search Hadoop</a>
             </p></div><div class="section" title="1.12.3.&nbsp;Remote Java Connection into EC2 Cluster Not Working"><div class="titlepage"><div><div><h3 class="title"><a name="trouble.ec2.connection"></a>1.12.3.&nbsp;Remote Java Connection into EC2 Cluster Not Working</h3></div></div></div><p>
             See Andrew's answer here, up on the user list: <a class="link" href="http://search-hadoop.com/m/sPdqNFAwyg2" target="_top">Remote Java client connection into EC2 instance</a>.
             </p></div></div><div class="section" title="1.13.&nbsp;HBase and Hadoop version issues"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="trouble.versions"></a>1.13.&nbsp;HBase and Hadoop version issues</h2></div></div></div><div class="section" title="1.13.1.&nbsp;NoClassDefFoundError when trying to run 0.90.x on hadoop-0.20.205.x (or hadoop-1.0.x)"><div class="titlepage"><div><div><h3 class="title"><a name="trouble.versions.205"></a>1.13.1.&nbsp;<code class="code">NoClassDefFoundError</code> when trying to run 0.90.x on hadoop-0.20.205.x (or hadoop-1.0.x)</h3></div></div></div><p>Apache HBase 0.90.x does not ship with hadoop-0.20.205.x, etc.  To make it run, you need to replace the hadoop
             jars that Apache HBase shipped with in its <code class="filename">lib</code> directory with those of the Hadoop you want to
             run HBase on.  If even after replacing Hadoop jars you get the below exception:</p><pre class="programlisting">
sv4r6s38: Exception in thread "main" java.lang.NoClassDefFoundError: org/apache/commons/configuration/Configuration
sv4r6s38:       at org.apache.hadoop.metrics2.lib.DefaultMetricsSystem.&lt;init&gt;(DefaultMetricsSystem.java:37)
sv4r6s38:       at org.apache.hadoop.metrics2.lib.DefaultMetricsSystem.&lt;clinit&gt;(DefaultMetricsSystem.java:34)
sv4r6s38:       at org.apache.hadoop.security.UgiInstrumentation.create(UgiInstrumentation.java:51)
sv4r6s38:       at org.apache.hadoop.security.UserGroupInformation.initialize(UserGroupInformation.java:209)
sv4r6s38:       at org.apache.hadoop.security.UserGroupInformation.ensureInitialized(UserGroupInformation.java:177)
sv4r6s38:       at org.apache.hadoop.security.UserGroupInformation.isSecurityEnabled(UserGroupInformation.java:229)
sv4r6s38:       at org.apache.hadoop.security.KerberosName.&lt;clinit&gt;(KerberosName.java:83)
sv4r6s38:       at org.apache.hadoop.security.UserGroupInformation.initialize(UserGroupInformation.java:202)
sv4r6s38:       at org.apache.hadoop.security.UserGroupInformation.ensureInitialized(UserGroupInformation.java:177)
</pre><p>you need to copy under <code class="filename">hbase/lib</code>, the
          <code class="filename">commons-configuration-X.jar</code> you find in your Hadoop's
          <code class="filename">lib</code> directory. That should fix the above complaint. </p></div><div class="section" title="1.13.2.&nbsp;...cannot communicate with client version..."><div class="titlepage"><div><div><h3 class="title"><a name="trouble.wrong.version"></a>1.13.2.&nbsp;...cannot communicate with client version...</h3></div></div></div><p>If you see something like the following in your logs <code class="computeroutput">... 2012-09-24
          10:20:52,168 FATAL org.apache.hadoop.hbase.master.HMaster: Unhandled exception. Starting
          shutdown. org.apache.hadoop.ipc.RemoteException: Server IPC version 7 cannot communicate
          with client version 4 ...</code> ...are you trying to talk to an Hadoop 2.0.x
        from an HBase that has an Hadoop 1.0.x client? Use the HBase built against Hadoop 2.0 or
        rebuild your HBase passing the <span class="command"><strong>-Dhadoop.profile=2.0</strong></span> attribute to Maven
        (See <a class="xref" href="#">???</a> for more). </p></div></div><div class="section" title="1.14.&nbsp;IPC Configuration Conflicts with Hadoop"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="d0e1094"></a>1.14.&nbsp;IPC Configuration Conflicts with Hadoop</h2></div></div></div><p>If the Hadoop configuration is loaded after the HBase configuration, and you have
      configured custom IPC settings in both HBase and Hadoop, the Hadoop values may overwrite the
      HBase values. There is normally no need to change these settings for HBase, so this problem is
      an edge case. However, <a class="link" href="https://issues.apache.org/jira/browse/HBASE-11492" target="_top">HBASE-11492</a> renames
      these settings for HBase to remove the chance of a conflict. Each of the setting names have
      been prefixed with <code class="literal">hbase.</code>, as shown in the following table. No action is
      required related to these changes unless you are already experiencing a conflict.</p><p>These changes were backported to HBase 0.98.x and apply to all newer versions.</p><div class="informaltable"><table border="1"><colgroup><col><col></colgroup><thead><tr><th>Pre-0.98.x</th><th>0.98-x And Newer</th></tr></thead><tbody><tr><td><p><code class="code">ipc.server.listen.queue.size</code></p></td><td><p><code class="code">hbase.ipc.server.listen.queue.size</code></p></td></tr><tr><td><p><code class="code">ipc.server.max.callqueue.size</code></p></td><td><p><code class="code">hbase.ipc.server.max.callqueue.size</code></p></td></tr><tr><td><p><code class="code">ipc.server.callqueue.handler.factor</code></p></td><td><p><code class="code">hbase.ipc.server.callqueue.handler.factor</code></p></td></tr><tr><td><p><code class="code">ipc.server.callqueue.read.share</code></p></td><td><p><code class="code">hbase.ipc.server.callqueue.read.share</code></p></td></tr><tr><td><p><code class="code">ipc.server.callqueue.type</code></p></td><td><p><code class="code">hbase.ipc.server.callqueue.type</code></p></td></tr><tr><td><p><code class="code">ipc.server.queue.max.call.delay</code></p></td><td><p><code class="code">hbase.ipc.server.queue.max.call.delay</code></p></td></tr><tr><td><p><code class="code">ipc.server.max.callqueue.length</code></p></td><td><p><code class="code">hbase.ipc.server.max.callqueue.length</code></p></td></tr><tr><td><p><code class="code">ipc.server.read.threadpool.size</code></p></td><td><p><code class="code">hbase.ipc.server.read.threadpool.size</code></p></td></tr><tr><td><p><code class="code">ipc.server.tcpkeepalive</code></p></td><td><p><code class="code">hbase.ipc.server.tcpkeepalive</code></p></td></tr><tr><td><p><code class="code">ipc.server.tcpnodelay</code></p></td><td><p><code class="code">hbase.ipc.server.tcpnodelay</code></p></td></tr><tr><td><p><code class="code">ipc.client.call.purge.timeout</code></p></td><td><p><code class="code">hbase.ipc.client.call.purge.timeout</code></p></td></tr><tr><td><p><code class="code">ipc.client.connection.maxidletime</code></p></td><td><p><code class="code">hbase.ipc.client.connection.maxidletime</code></p></td></tr><tr><td><p><code class="code">ipc.client.idlethreshold</code></p></td><td><p><code class="code">hbase.ipc.client.idlethreshold</code></p></td></tr><tr><td><p><code class="code">ipc.client.kill.max</code></p></td><td><p><code class="code">hbase.ipc.client.kill.max</code></p></td></tr><tr><td><p><code class="code">ipc.server.scan.vtime.weight </code></p></td><td><p><code class="code">hbase.ipc.server.scan.vtime.weight </code></p></td></tr></tbody></table></div></div><div class="section" title="1.15.&nbsp;HBase and HDFS"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="d0e1251"></a>1.15.&nbsp;HBase and HDFS</h2></div></div></div><p>General configuration guidance for Apache HDFS is out of the scope of this guide. Refer to
      the documentation available at <a class="link" href="http://hadoop.apache.org/" target="_top">http://hadoop.apache.org/</a> for extensive
      information about configuring HDFS. This section deals with HDFS in terms of HBase. </p><p>In most cases, HBase stores its data in Apache HDFS. This includes the HFiles containing
      the data, as well as the write-ahead logs (WALs) which store data before it is written to the
      HFiles and protect against RegionServer crashes. HDFS provides reliability and protection to
      data in HBase because it is distributed. To operate with the most efficiency, HBase needs data
    to be available locally. Therefore, it is a good practice to run an HDFS datanode on each
    RegionServer.</p><div class="variablelist" title="Important Information and Guidelines for HBase and HDFS"><p class="title"><b>Important Information and Guidelines for HBase and HDFS</b></p><dl><dt><span class="term">HBase is a client of HDFS.</span></dt><dd><p>HBase is an HDFS client, using the HDFS <code class="code">DFSClient</code> class, and references
            to this class appear in HBase logs with other HDFS client log messages.</p></dd><dt><span class="term">Configuration is necessary in multiple places.</span></dt><dd><p>Some HDFS configurations relating to HBase need to be done at the HDFS (server) side.
            Others must be done within HBase (at the client side). Other settings need
            to be set at both the server and client side.
          </p></dd><dt><span class="term">Write errors which affect HBase may be logged in the HDFS logs rather than HBase logs.</span></dt><dd><p>When writing, HDFS pipelines communications from one datanode to another. HBase
            communicates to both the HDFS namenode and datanode, using the HDFS client classes.
            Communication problems between datanodes are logged in the HDFS logs, not the HBase
            logs.</p><p>HDFS writes are always local when possible. HBase RegionServers should not
            experience many write errors, because they write the local datanode. If the datanode
            cannot replicate the blocks, the errors are logged in HDFS, not in the HBase
            RegionServer logs.</p></dd><dt><span class="term">HBase communicates with HDFS using two different ports.</span></dt><dd><p>HBase communicates with datanodes using the <code class="code">ipc.Client</code> interface and
            the <code class="code">DataNode</code> class. References to these will appear in HBase logs. Each of
            these communication channels use a different port (50010 and 50020 by default). The
            ports are configured in the HDFS configuration, via the
              <code class="code">dfs.datanode.address</code> and <code class="code">dfs.datanode.ipc.address</code>
            parameters.</p></dd><dt><span class="term">Errors may be logged in HBase, HDFS, or both.</span></dt><dd><p>When troubleshooting HDFS issues in HBase, check logs in both places for errors.</p></dd><dt><span class="term">HDFS takes a while to mark a node as dead. You can configure HDFS to avoid using stale
          datanodes.</span></dt><dd><p>By default, HDFS does not mark a node as dead until it is unreachable for 630
            seconds. In Hadoop 1.1 and Hadoop 2.x, this can be alleviated by enabling checks for
            stale datanodes, though this check is disabled by default. You can enable the check for
            reads and writes separately, via <code class="code">dfs.namenode.avoid.read.stale.datanode</code> and
              <code class="code">dfs.namenode.avoid.write.stale.datanode settings</code>. A stale datanode is one
            that has not been reachable for <code class="code">dfs.namenode.stale.datanode.interval</code>
            (default is 30 seconds). Stale datanodes are avoided, and marked as the last possible
            target for a read or write operation. For configuration details, see the HDFS
            documentation.</p></dd><dt><span class="term">Settings for HDFS retries and timeouts are important to HBase.</span></dt><dd><p>You can configure settings for various retries and timeouts. Always refer to the
            HDFS documentation for current recommendations and defaults. Some of the settings
            important to HBase are listed here. Defaults are current as of Hadoop 2.3. Check the
            Hadoop documentation for the most current values and recommendations.</p><div class="variablelist" title="Retries"><p class="title"><b>Retries</b></p><dl><dt><span class="term"><code class="code">ipc.client.connect.max.retries</code> (default: 10)</span></dt><dd><p>The number of times a client will attempt to establish a connection with the
                  server. This value sometimes needs to be increased. You can specify different
                  setting for the maximum number of retries if a timeout occurs. For SASL
                  connections, the number of retries is hard-coded at 15 and cannot be
                  configured.</p></dd><dt><span class="term"><code class="code">ipc.client.connect.max.retries.on.timeouts</code> (default: 45)</span></dt><dd><p>The number of times a client will attempt to establish a connection
                with the server in the event of a timeout. If some retries are due to timeouts and
                some are due to other reasons, this counter is added to
                <code class="code">ipc.client.connect.max.retries</code>, so the maximum number of retries for
                all reasons could be the combined value.</p></dd><dt><span class="term"><code class="code">dfs.client.block.write.retries</code> (default: 3)</span></dt><dd><p>How many times the client attempts to write to the datanode. After the
              number of retries is reached, the client reconnects to the namenode to get a new
              location of a datanode. You can try increasing this value.</p></dd></dl></div><div class="variablelist" title="HDFS Heartbeats"><p class="title"><b>HDFS Heartbeats</b></p><p>HDFS heartbeats are entirely on the HDFS side, between the namenode and datanodes.</p><dl><dt><span class="term"><code class="code">dfs.heartbeat.interval</code> (default: 3)</span></dt><dd><p>The interval at which a node heartbeats.</p></dd><dt><span class="term"><code class="code">dfs.namenode.heartbeat.recheck-interval</code> (default: 300000)</span></dt><dd><p>The interval of time between heartbeat checks. The total time before a node is
                  marked as stale is determined by the following formula, which works out to 10
                  minutes and 30 seconds:</p><pre class="screen"> 2 * (dfs.namenode.heartbeat.recheck-interval) + 10 * 1000 * (dfs.heartbeat.interval)</pre></dd><dt><span class="term"><code class="code">dfs.namenode.stale.datanode.interval</code> (default: 3000)</span></dt><dd><p>How long (in milliseconds) a node can go without a heartbeat before it is
                  determined to be stale, if the other options to do with stale datanodes are
                  configured (off by default).</p></dd></dl></div></dd></dl></div><div class="variablelist" title="Connection Timeouts"><p class="title"><b>Connection Timeouts</b></p><p>Connection timeouts occur between the client (HBASE) and the HDFS datanode. They may
        occur when establishing a connection, attempting to read, or attempting to write. The two
        settings below are used in combination, and affect connections between the DFSClient and the
        datanode, the ipc.cClient and the datanode, and communication between two datanodes. </p><dl><dt><span class="term"><code class="code">dfs.client.socket-timeout</code> (default: 60000)</span></dt><dd><p>The amount of time before a client connection times out when establishing a
            connection or reading. The value is expressed in milliseconds, so the default is 60
            seconds.</p></dd><dt><span class="term"><code class="code">dfs.datanode.socket.write.timeout</code> (default: 480000)</span></dt><dd><p>The amount of time before a write operation times out. The default is 8
            minutes, expressed as milliseconds.</p></dd></dl></div><div class="variablelist" title="Typical Error Logs"><p class="title"><b>Typical Error Logs</b></p><p>The following types of errors are often seen in the logs.</p><dl><dt><span class="term"><code class="code">INFO HDFS.DFSClient: Failed to connect to /xxx50010, add to deadNodes and
            continue java.net.SocketTimeoutException: 60000 millis timeout while waiting for channel
            to be ready for connect. ch : java.nio.channels.SocketChannel[connection-pending
            remote=/region-server-1:50010]</code></span></dt><dd><p>All datanodes for a block are dead, and recovery is not possible. Here is the
            sequence of events that leads to this error:</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem"><p>The client attempts to connect to a dead datanode.</p></li><li class="listitem"><p>The connection fails, so the client moves down the list of datanodes and tries
                the next one. It also fails.</p></li><li class="listitem"><p>When the client exhausts its entire list, it sleeps for 3 seconds and requests a
              new list. It is very likely to receive the exact same list as before, in which case
              the error occurs again.</p></li></ul></div></dd><dt><span class="term"><code class="code">INFO org.apache.hadoop.HDFS.DFSClient: Exception in createBlockOutputStream
            java.net.SocketTimeoutException: 69000 millis timeout while waiting for channel to be
            ready for connect. ch : java.nio.channels.SocketChannel[connection-pending remote=/
            xxx:50010]</code></span></dt><dd><p>This type of error indicates a write issue. In this case, the master wants to split
            the log. It does not have a local datanode so it tries to connect to a remote datanode,
            but the datanode is dead.</p><p>In this situation, there will be three retries (by default). If all retries fail, a
            message like the following is logged:</p><pre class="screen">
WARN HDFS.DFSClient: DataStreamer Exception: java.io.IOException: Unable to create new block
          </pre><p>If the operation was an attempt to split the log, the following type of message may
            also appear:</p><pre class="screen">
FATAL wal.HLogSplitter: WriterThread-xxx Got while writing log entry to log            
          </pre></dd></dl></div></div><div class="section" title="1.16.&nbsp;Running unit or integration tests"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="trouble.tests"></a>1.16.&nbsp;Running unit or integration tests</h2></div></div></div><div class="section" title="1.16.1.&nbsp;Runtime exceptions from MiniDFSCluster when running tests"><div class="titlepage"><div><div><h3 class="title"><a name="trouble.HDFS-2556"></a>1.16.1.&nbsp;Runtime exceptions from MiniDFSCluster when running tests</h3></div></div></div><p>If you see something like the following</p><pre class="programlisting">...
java.lang.NullPointerException: null
at org.apache.hadoop.hdfs.MiniDFSCluster.startDataNodes
at org.apache.hadoop.hdfs.MiniDFSCluster.&lt;init&gt;
at org.apache.hadoop.hbase.MiniHBaseCluster.&lt;init&gt;
at org.apache.hadoop.hbase.HBaseTestingUtility.startMiniDFSCluster
at org.apache.hadoop.hbase.HBaseTestingUtility.startMiniCluster
...</pre><p> or</p><pre class="programlisting">...
java.io.IOException: Shutting down
at org.apache.hadoop.hbase.MiniHBaseCluster.init
at org.apache.hadoop.hbase.MiniHBaseCluster.&lt;init&gt;
at org.apache.hadoop.hbase.MiniHBaseCluster.&lt;init&gt;
at org.apache.hadoop.hbase.HBaseTestingUtility.startMiniHBaseCluster
at org.apache.hadoop.hbase.HBaseTestingUtility.startMiniCluster
...</pre><p>... then try issuing the command <span class="command"><strong>umask 022</strong></span> before launching tests.
        This is a workaround for <a class="link" href="https://issues.apache.org/jira/browse/HDFS-2556" target="_top">HDFS-2556</a>
      </p></div></div><div class="section" title="1.17.&nbsp;Case Studies"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="trouble.casestudy"></a>1.17.&nbsp;Case Studies</h2></div></div></div><p>For Performance and Troubleshooting Case Studies, see <a class="xref" href="#">???</a>. </p></div><div class="section" title="1.18.&nbsp;Cryptographic Features"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="trouble.crypto"></a>1.18.&nbsp;Cryptographic Features</h2></div></div></div><div class="section" title="1.18.1.&nbsp;sun.security.pkcs11.wrapper.PKCS11Exception: CKR_ARGUMENTS_BAD"><div class="titlepage"><div><div><h3 class="title"><a name="trouble.crypto.HBASE-10132"></a>1.18.1.&nbsp;sun.security.pkcs11.wrapper.PKCS11Exception: CKR_ARGUMENTS_BAD</h3></div></div></div><p>This problem manifests as exceptions ultimately caused by:</p><pre class="programlisting">
Caused by: sun.security.pkcs11.wrapper.PKCS11Exception: CKR_ARGUMENTS_BAD
	at sun.security.pkcs11.wrapper.PKCS11.C_DecryptUpdate(Native Method)
	at sun.security.pkcs11.P11Cipher.implDoFinal(P11Cipher.java:795)
</pre><p> This problem appears to affect some versions of OpenJDK 7 shipped by some Linux
        vendors. NSS is configured as the default provider. If the host has an x86_64 architecture,
        depending on if the vendor packages contain the defect, the NSS provider will not function
        correctly. </p><p> To work around this problem, find the JRE home directory and edit the file
          <code class="filename">lib/security/java.security</code>. Edit the file to comment out the line: </p><pre class="programlisting">
security.provider.1=sun.security.pkcs11.SunPKCS11 ${java.home}/lib/security/nss.cfg
</pre><p> Then renumber the remaining providers accordingly. </p></div></div><div class="section" title="1.19.&nbsp;Operating System Specific Issues"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="d0e1501"></a>1.19.&nbsp;Operating System Specific Issues</h2></div></div></div><div class="section" title="1.19.1.&nbsp;Page Allocation Failure"><div class="titlepage"><div><div><h3 class="title"><a name="d0e1504"></a>1.19.1.&nbsp;Page Allocation Failure</h3></div></div></div><div class="note" title="Note" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title">Note</h3><p>This issue is known to affect CentOS 6.2 and possibly CentOS 6.5. It may also affect
        some versions of Red Hat Enterprise Linux, according to <a class="link" href="https://bugzilla.redhat.com/show_bug.cgi?id=770545" target="_top">https://bugzilla.redhat.com/show_bug.cgi?id=770545</a>.</p></div><p>Some users have reported seeing the following error:</p><pre class="screen">kernel: java: page allocation failure. order:4, mode:0x20</pre><p>Raising the value of <code class="code">min_free_kbytes</code> was reported to fix this problem. This
      parameter is set to a percentage of the amount of RAM on your system, and is described in more
      detail at <a class="link" href="http://www.centos.org/docs/5/html/5.1/Deployment_Guide/s3-proc-sys-vm.html" target="_top">http://www.centos.org/docs/5/html/5.1/Deployment_Guide/s3-proc-sys-vm.html</a>. </p><p>To find the current value on your system, run the following command:</p><pre class="screen">[user@host]# <strong class="userinput"><code>cat /proc/sys/vm/min_free_kbytes</code></strong></pre><p>Next, raise the value. Try doubling, then quadrupling the value. Note that setting the
        value too low or too high could have detrimental effects on your system. Consult your
        operating system vendor for specific recommendations.</p><p>Use the following command to modify the value of <code class="code">min_free_kbytes</code>,
        substituting <em class="replaceable"><code>&lt;value&gt;</code></em> with your intended value:</p><pre class="screen">[user@host]# <strong class="userinput"><code>echo &lt;value&gt; &gt; /proc/sys/vm/min_free_kbytes</code></strong></pre></div></div><div class="section" title="1.20.&nbsp;JDK Issues"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="d0e1543"></a>1.20.&nbsp;JDK Issues</h2></div></div></div><div class="section" title="1.20.1.&nbsp;NoSuchMethodError: java.util.concurrent.ConcurrentHashMap.keySet"><div class="titlepage"><div><div><h3 class="title"><a name="d0e1546"></a>1.20.1.&nbsp;NoSuchMethodError: java.util.concurrent.ConcurrentHashMap.keySet</h3></div></div></div><p>
If you see this in your logs:
    </p><pre class="programlisting">Caused by: java.lang.NoSuchMethodError: java.util.concurrent.ConcurrentHashMap.keySet()Ljava/util/concurrent/ConcurrentHashMap$KeySetView;
  at org.apache.hadoop.hbase.master.ServerManager.findServerWithSameHostnamePortWithLock(ServerManager.java:393)
  at org.apache.hadoop.hbase.master.ServerManager.checkAndRecordNewServer(ServerManager.java:307)
  at org.apache.hadoop.hbase.master.ServerManager.regionServerStartup(ServerManager.java:244)
  at org.apache.hadoop.hbase.master.MasterRpcServices.regionServerStartup(MasterRpcServices.java:304)
  at org.apache.hadoop.hbase.protobuf.generated.RegionServerStatusProtos$RegionServerStatusService$2.callBlockingMethod(RegionServerStatusProtos.java:7910)
  at org.apache.hadoop.hbase.ipc.RpcServer.call(RpcServer.java:2020)
  ... 4 more</pre><p>
then check if you compiled with jdk8 and tried to run it on jdk7.  If so, this won't work.
Run on jdk8 or recompile with jdk7.  See <a class="link" href="https://issues.apache.org/jira/browse/HBASE-10607" target="_top">HBASE-10607 [JDK8] NoSuchMethodError involving ConcurrentHashMap.keySet if running on JRE 7</a>.
</p></div></div></div><div id="disqus_thread"></div><script type="text/javascript">
    /* * * DON'T EDIT BELOW THIS LINE * * */
    (function() {
        var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
        dsq.src = 'http://' + disqus_shortname + '.disqus.com/embed.js';
        (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
    })();
</script><noscript>Please enable JavaScript to view the <a href="http://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript><a href="http://disqus.com" class="dsq-brlink">comments powered by <span class="logo-disqus">Disqus</span></a></body></html>