<html><head>
      <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
   <title>Chapter&nbsp;1.&nbsp;Upgrading</title><link rel="stylesheet" type="text/css" href="css/freebsd_docbook.css"><meta name="generator" content="DocBook XSL-NS Stylesheets V1.76.1"></head><body bgcolor="white" text="black" link="#0000FF" vlink="#840084" alink="#0000FF"><script type="text/javascript">
    var disqus_shortname = 'hbase'; // required: replace example with your forum shortname
    var disqus_url = 'http://hbase.apache.org/book/upgrading.html';
    </script><div class="chapter" title="Chapter&nbsp;1.&nbsp;Upgrading"><div class="titlepage"><div><div><h2 class="title"><a name="upgrading"></a>Chapter&nbsp;1.&nbsp;Upgrading</h2></div></div></div><div class="toc"><p><b>Table of Contents</b></p><dl><dt><span class="section"><a href="#hbase.versioning">1.1. HBase version numbers</a></span></dt><dd><dl><dt><span class="section"><a href="#hbase.development.series">1.1.1. Odd/Even Versioning or "Development"" Series Releases</a></span></dt><dt><span class="section"><a href="#hbase.binary.compatibility">1.1.2. Binary Compatibility</a></span></dt><dt><span class="section"><a href="#hbase.rolling.upgrade">1.1.3. <em class="firstterm">Rolling Upgrades</em></a></span></dt></dl></dd><dt><span class="section"><a href="#upgrade1.0">1.2. Upgrading from 0.98.x to 1.0.x</a></span></dt><dd><dl><dt><span class="section"><a href="#upgrade1.0.changes">1.2.1. Changes of Note!</a></span></dt><dt><span class="section"><a href="#upgrade1.0.rolling.upgrade">1.2.2. Rolling upgrade from 0.98.x to HBase 1.0.0</a></span></dt><dt><span class="section"><a href="#upgrade1.0.from.0.94">1.2.3. Upgrading to 1.0 from 0.94</a></span></dt></dl></dd><dt><span class="section"><a href="#upgrade0.98">1.3. Upgrading from 0.96.x to 0.98.x</a></span></dt><dt><span class="section"><a href="#d2494e153">1.4. Upgrading from 0.94.x to 0.98.x</a></span></dt><dt><span class="section"><a href="#upgrade0.96">1.5. Upgrading from 0.94.x to 0.96.x</a></span></dt><dd><dl><dt><span class="section"><a href="#executing.the.0.96.upgrade">1.5.1. Executing the 0.96 Upgrade</a></span></dt></dl></dd><dt><span class="section"><a href="#upgrade0.94">1.6. Upgrading from 0.92.x to 0.94.x</a></span></dt><dt><span class="section"><a href="#upgrade0.92">1.7. Upgrading from 0.90.x to 0.92.x</a></span></dt><dd><dl><dt><span class="section"><a href="#d2494e365">1.7.1. You can&#8217;t go back! </a></span></dt><dt><span class="section"><a href="#d2494e379">1.7.2. MSLAB is ON by default </a></span></dt><dt><span class="section"><a href="#dls">1.7.3. Distributed Log Splitting is on by default </a></span></dt><dt><span class="section"><a href="#d2494e412">1.7.4. Memory accounting is different now </a></span></dt><dt><span class="section"><a href="#d2494e421">1.7.5. On the Hadoop version to use </a></span></dt><dt><span class="section"><a href="#d2494e433">1.7.6. HBase 0.92.0 ships with ZooKeeper 3.4.2 </a></span></dt><dt><span class="section"><a href="#d2494e438">1.7.7. Online alter is off by default </a></span></dt><dt><span class="section"><a href="#d2494e445">1.7.8. WebUI </a></span></dt><dt><span class="section"><a href="#d2494e450">1.7.9. Security tarball </a></span></dt><dt><span class="section"><a href="#d2494e455">1.7.10. Changes in HBase replication </a></span></dt><dt><span class="section"><a href="#d2494e460">1.7.11. RegionServer now aborts if OOME </a></span></dt><dt><span class="section"><a href="#d2494e465">1.7.12. HFile V2 and the &#8220;Bigger, Fewer&#8221; Tendency </a></span></dt></dl></dd><dt><span class="section"><a href="#upgrade0.90">1.8. Upgrading to HBase 0.90.x from 0.20.x or 0.89.x</a></span></dt></dl></div><p>You cannot skip major versions upgrading. If you are upgrading from version 0.90.x to
        0.94.x, you must first go from 0.90.x to 0.92.x and then go from 0.92.x to 0.94.x.</p><div class="note" title="Note" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title">Note</h3><p>It may be possible to skip across versions -- for example go from 0.92.2 straight to
            0.98.0 just following the 0.96.x upgrade instructions -- but we have not tried it so
            cannot say whether it works or not.</p></div><p> Review <a class="xref" href="#">???</a>, in particular the section on Hadoop version. </p><div class="section" title="1.1.&nbsp;HBase version numbers"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="hbase.versioning"></a>1.1.&nbsp;HBase version numbers</h2></div></div></div><p>HBase has not walked a straight line where version numbers are concerned. Since we
            came up out of hadoop itself, we originally tracked hadoop versioning. Later we left
            hadoop versioning behind because we were moving at a different rate to that of our
            parent. If you are into the arcane, checkout our old wiki page on <a class="link" href="http://wiki.apache.org/hadoop/Hbase/HBaseVersions" target="_top">HBase
                Versioning</a> which tries to connect the HBase version dots.</p><div class="section" title="1.1.1.&nbsp;Odd/Even Versioning or &#34;Development&#34;&#34; Series Releases"><div class="titlepage"><div><div><h3 class="title"><a name="hbase.development.series"></a>1.1.1.&nbsp;Odd/Even Versioning or "Development"" Series Releases</h3></div></div></div><p>Ahead of big releases, we have been putting up preview versions to start the
                feedback cycle turning-over earlier. These "Development" Series releases, always
                odd-numbered, come with no guarantees, not even regards being able to upgrade
                between two sequential releases (we reserve the right to break compatibility across
                "Development" Series releases). Needless to say, these releases are not for
                production deploys. They are a preview of what is coming in the hope that interested
                parties will take the release for a test drive and flag us early if we there are
                issues we've missed ahead of our rolling a production-worthy release. </p><p>Our first "Development" Series was the 0.89 set that came out ahead of HBase
                0.90.0. HBase 0.95 is another "Development" Series that portends HBase 0.96.0.
            </p></div><div class="section" title="1.1.2.&nbsp;Binary Compatibility"><div class="titlepage"><div><div><h3 class="title"><a name="hbase.binary.compatibility"></a>1.1.2.&nbsp;Binary Compatibility</h3></div></div></div><p>When we say two HBase versions are compatible, we mean that the versions are wire
                and binary compatible. Compatible HBase versions means that clients can talk to
                compatible but differently versioned servers. It means too that you can just swap
                out the jars of one version and replace them with the jars of another, compatible
                version and all will just work. Unless otherwise specified, HBase point versions are
                binary compatible. You can safely do rolling upgrades between binary compatible
                versions; i.e. across point versions: e.g. from 0.94.5 to 0.94.6. See <a class="link" href="http://search-hadoop.com/m/bOOvwHGW981/Does+compatibility+between+versions+also+mean+binary+compatibility%253F&amp;subj=Re+Does+compatibility+between+versions+also+mean+binary+compatibility+" target="_top">Does
                            compatibility between versions also mean binary compatibility?</a>
                        discussion on the hbaes dev mailing list. </p></div><div class="section" title="1.1.3.&nbsp;Rolling Upgrades"><div class="titlepage"><div><div><h3 class="title"><a name="hbase.rolling.upgrade"></a>1.1.3.&nbsp;<em class="firstterm">Rolling Upgrades</em></h3></div></div></div><p>A rolling upgrade is the process by which you update the servers
            in your cluster a server at a time. You can rolling upgrade across HBase versions
            if they are binary or wire compatible.
            See <span style="color: red">&lt;xlnk&gt;&lt;/xlnk&gt;</span> for more on what this means.
            Coarsely, a rolling upgrade is a graceful stop each server,
            update the software, and then restart.  You do this for each server in the cluster.
            Usually you upgrade the Master first and then the regionservers.
            See <span style="color: red">&lt;xlink&gt;&lt;/xlink&gt;</span> for tools that can help use the rolling upgrade process.
          </p><p>For example, in the below, hbase was symlinked to the actual hbase install.
            On upgrade, before running a rolling restart over the cluser, we changed the symlink
            to point at the new HBase software version and then ran 
            </p><pre class="programlisting">$ HADOOP_HOME=~/hadoop-2.6.0-CRC-SNAPSHOT ~/hbase/bin/rolling-restart.sh --config ~/conf_hbase</pre><p>
            The rolling-restart script will first gracefully stop and restart the master, and then
            each of the regionservers in turn. Because the symlink was changed, on restart the
            server will come up using the new hbase version.  Check logs for errors as the
            rolling upgrade proceeds.
          </p><div class="section" title="1.1.3.1.&nbsp;Rolling Upgrade between versions that are Binary/Wire compatibile"><div class="titlepage"><div><div><h4 class="title"><a name="hbase.rolling.restart"></a>1.1.3.1.&nbsp;Rolling Upgrade between versions that are Binary/Wire compatibile</h4></div></div></div><p>Unless otherwise specified, HBase point versions are binary compatible. You can do
              a <span style="color: red">&lt;xlink&gt;&lt;/xlink&gt;</span> between hbase point versions.
                For example, you can go to 0.94.6 from 0.94.5 by doing a rolling upgrade
                across the cluster replacing the 0.94.5 binary with a 0.94.6 binary.</p><p>In the minor version-particular sections below, we call out where the versions
                are wire/protocol compatible and in this case, it is also possible to do a
                <span style="color: red">&lt;xlink&gt;&lt;/xlink&gt;</span>. For example, in
            <span style="color: red">&lt;xlink&gt;&lt;/xlink&gt;</span>, we
              state that it is possible to do a rolling upgrade between hbase-0.98.x and hbase-1.0.0.</p></div></div></div><div class="section" title="1.2.&nbsp;Upgrading from 0.98.x to 1.0.x"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="upgrade1.0"></a>1.2.&nbsp;Upgrading from 0.98.x to 1.0.x</h2></div></div></div><p>In this section we first note the significant changes that come in with 1.0.0 HBase and then
          we go over the upgrade process.  Be sure to read the significant changes section with care
          so you avoid surprises.
        </p><div class="section" title="1.2.1.&nbsp;Changes of Note!"><div class="titlepage"><div><div><h3 class="title"><a name="upgrade1.0.changes"></a>1.2.1.&nbsp;Changes of Note!</h3></div></div></div><p>In here we list important changes that are in 1.0.0 since 0.98.x., changes you should
            be aware that will go into effect once you upgrade.</p><div class="section" title="1.2.1.1.&nbsp;ZooKeeper 3.4 is required in HBase 1.0.0"><div class="titlepage"><div><div><h4 class="title"><a name="zookeeper.3.4"></a>1.2.1.1.&nbsp;ZooKeeper 3.4 is required in HBase 1.0.0</h4></div></div></div><p>See <a class="xref" href="#">???</a>.</p></div><div class="section" title="1.2.1.2.&nbsp;HBase Default Ports Changed"><div class="titlepage"><div><div><h4 class="title"><a name="default.ports.changed"></a>1.2.1.2.&nbsp;HBase Default Ports Changed</h4></div></div></div><p>The ports used by HBase changed.  The used to be in the 600XX range.  In
                hbase-1.0.0 they have been moved up out of the ephemeral port range and are
                160XX instead (Master web UI was 60010 and is now 16030; the RegionServer
                web UI was 60030 and is now 16030, etc). If you want to keep the old port
                locations, copy the port setting configs from <code class="filename">hbase-default.xml</code>
                into <code class="filename">hbase-site.xml</code>, change them back to the old values
                from hbase-0.98.x era, and ensure you've distributed your configurations before
              you restart.</p></div><div class="section" title="1.2.1.3.&nbsp;hbase.bucketcache.percentage.in.combinedcache configuration has been REMOVED"><div class="titlepage"><div><div><h4 class="title"><a name="upgrade1.0.hbase.bucketcache.percentage.in.combinedcache"></a>1.2.1.3.&nbsp;hbase.bucketcache.percentage.in.combinedcache configuration has been REMOVED</h4></div></div></div><p>You may have made use of this configuration if you are using BucketCache.
                    If NOT using BucketCache, this change does not effect you.
                    Its removal means that your L1 LruBlockCache is now sized
                    using <code class="varname">hfile.block.cache.size</code> -- i.e. the way you
                    would size the onheap L1 LruBlockCache if you were NOT doing
                    BucketCache -- and the BucketCache size is not whatever the
                    setting for hbase.bucketcache.size is.  You may need to adjust
                    configs to get the LruBlockCache and BucketCache sizes set to
                    what they were in 0.98.x and previous.  If you did not set this
                    config., its default value was 0.9.  If you do nothing, your
                    BucketCache will increase in size by 10%.  Your L1 LruBlockCache will
                    become <code class="varname">hfile.block.cache.size</code> times your java
                    heap size (hfile.block.cache.size is a float between 0.0 and 1.0).
                    To read more, see
                    <a class="link" href="https://issues.apache.org/jira/browse/HBASE-11520" target="_top">HBASE-11520 Simplify offheap cache config by removing the confusing "hbase.bucketcache.percentage.in.combinedcache"</a>.
                </p></div><div class="section" title="1.2.1.4.&nbsp;If you have your own customer filters...."><div class="titlepage"><div><div><h4 class="title"><a name="hbase-12068"></a>1.2.1.4.&nbsp;If you have your own customer filters....</h4></div></div></div><p>See the release notes on the issue <a class="link" href="https://issues.apache.org/jira/browse/HBASE-12068" target="_top">HBASE-12068 [Branch-1] Avoid need to always do KeyValueUtil#ensureKeyValue for Filter transformCell</a>;
              be sure to follow the recommendations therein.
            </p></div></div><div class="section" title="1.2.2.&nbsp;Rolling upgrade from 0.98.x to HBase 1.0.0"><div class="titlepage"><div><div><h3 class="title"><a name="upgrade1.0.rolling.upgrade"></a>1.2.2.&nbsp;Rolling upgrade from 0.98.x to HBase 1.0.0</h3></div></div></div><div class="note" title="From 0.96.x to 1.0.0" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title">From 0.96.x to 1.0.0</h3><p>You cannot do a <span style="color: red">&lt;xlink&gt;&lt;/xlink&gt;</span> from 0.96.x to 1.0.0 without
              first doing a rolling upgrade to 0.98.x. See comment in
              <a class="link" href="https://issues.apache.org/jira/browse/HBASE-11164?focusedCommentId=14182330&amp;page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-14182330" target="_top">HBASE-11164 Document and test rolling updates from 0.98 -&gt; 1.0</a> for the why.
              Also because hbase-1.0.0 enables hfilev3 by default,
              <a class="link" href="https://issues.apache.org/jira/browse/HBASE-9801" target="_top">HBASE-9801 Change the default HFile version to V3</a>,
              and support for hfilev3 only arrives in 0.98, this is another reason you cannot rolling upgrade from hbase-0.96.x;
              if the rolling upgrade stalls, the 0.96.x servers cannot open files written by the servers running the newer hbase-1.0.0
              hfilev3 writing servers.
            </p></div><p>There are no known issues running a <span style="color: red">&lt;xlink&gt;&lt;/xlink&gt;</span> from hbase-0.98.x to hbase-1.0.0.
          
          </p></div><div class="section" title="1.2.3.&nbsp;Upgrading to 1.0 from 0.94"><div class="titlepage"><div><div><h3 class="title"><a name="upgrade1.0.from.0.94"></a>1.2.3.&nbsp;Upgrading to 1.0 from 0.94</h3></div></div></div><p>You cannot rolling upgrade from 0.94.x to 1.x.x.  You must stop your cluster,
            install the 1.x.x software, run the migration described at <a class="xref" href="#executing.the.0.96.upgrade" title="1.5.1.&nbsp;Executing the 0.96 Upgrade">Section&nbsp;1.5.1, &#8220;Executing the 0.96 Upgrade&#8221;</a>
            (substituting 1.x.x. wherever we make mention of 0.96.x in the section below),
            and then restart.  Be sure to upgrade your zookeeper if it is a version less than the required 3.4.x.
          </p></div></div><div class="section" title="1.3.&nbsp;Upgrading from 0.96.x to 0.98.x"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="upgrade0.98"></a>1.3.&nbsp;Upgrading from 0.96.x to 0.98.x</h2></div></div></div><p>A rolling upgrade from 0.96.x to 0.98.x works. The two versions are not binary
            compatible.</p><p>Additional steps are required to take advantage of some of the new features of 0.98.x,
            including cell visibility labels, cell ACLs, and transparent server side encryption. See
            the <a class="xref" href="#">???</a> chapter of this guide for more information. Significant
            performance improvements include a change to the write ahead log threading model that
            provides higher transaction throughput under high load, reverse scanners, MapReduce over
            snapshot files, and striped compaction.</p><p>Clients and servers can run with 0.98.x and 0.96.x versions. However, applications may
            need to be recompiled due to changes in the Java API.</p></div><div class="section" title="1.4.&nbsp;Upgrading from 0.94.x to 0.98.x"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="d2494e153"></a>1.4.&nbsp;Upgrading from 0.94.x to 0.98.x</h2></div></div></div><p> A rolling upgrade from 0.94.x directly to 0.98.x does not work. The upgrade path
            follows the same procedures as <a class="xref" href="#upgrade0.96" title="1.5.&nbsp;Upgrading from 0.94.x to 0.96.x">Section&nbsp;1.5, &#8220;Upgrading from 0.94.x to 0.96.x&#8221;</a>. Additional steps are required to use some of the new
            features of 0.98.x. See <a class="xref" href="#upgrade0.98" title="1.3.&nbsp;Upgrading from 0.96.x to 0.98.x">Section&nbsp;1.3, &#8220;Upgrading from 0.96.x to 0.98.x&#8221;</a> for an abbreviated list of these features. </p></div><div class="section" title="1.5.&nbsp;Upgrading from 0.94.x to 0.96.x"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="upgrade0.96"></a>1.5.&nbsp;Upgrading from 0.94.x to 0.96.x</h2></div><div><h3 class="subtitle">The "Singularity"</h3></div></div></div><div class="note" title="HBase 0.96.x was EOL'd, September 1st, 2014" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title">HBase 0.96.x was EOL'd, September 1st, 2014</h3><p>
            Do not deploy 0.96.x  Deploy a 0.98.x at least.
            See <a class="link" href="https://issues.apache.org/jira/browse/HBASE-11642" target="_top">EOL 0.96</a>.
        </p></div><p>You will have to stop your old 0.94.x cluster completely to upgrade. If you are
            replicating between clusters, both clusters will have to go down to upgrade. Make sure
            it is a clean shutdown. The less WAL files around, the faster the upgrade will run (the
            upgrade will split any log files it finds in the filesystem as part of the upgrade
            process). All clients must be upgraded to 0.96 too. </p><p>The API has changed. You will need to recompile your code against 0.96 and you may
            need to adjust applications to go against new APIs (TODO: List of changes). </p><div class="section" title="1.5.1.&nbsp;Executing the 0.96 Upgrade"><div class="titlepage"><div><div><h3 class="title"><a name="executing.the.0.96.upgrade"></a>1.5.1.&nbsp;Executing the 0.96 Upgrade</h3></div></div></div><div class="note" title="HDFS and ZooKeeper must be up!" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title">HDFS and ZooKeeper must be up!</h3><p>HDFS and ZooKeeper should be up and running during the upgrade process.</p></div><p>hbase-0.96.0 comes with an upgrade script. Run
                </p><pre class="programlisting">$ bin/hbase upgrade</pre><p> to see its usage. The script
                has two main modes: -check, and -execute. </p><div class="section" title="1.5.1.1.&nbsp;check"><div class="titlepage"><div><div><h4 class="title"><a name="d2494e192"></a>1.5.1.1.&nbsp;check</h4></div></div></div><p>The <span class="emphasis"><em>check</em></span> step is run against a running 0.94 cluster. Run
                    it from a downloaded 0.96.x binary. The <span class="emphasis"><em>check</em></span> step is
                    looking for the presence of <code class="filename">HFileV1</code> files. These are
                    unsupported in hbase-0.96.0. To purge them -- have them rewritten as HFileV2 --
                    you must run a compaction. </p><p>The <span class="emphasis"><em>check</em></span> step prints stats at the end of its run (grep
                    for &#8220;Result:&#8221; in the log) printing absolute path of the tables it scanned, any
                    HFileV1 files found, the regions containing said files (the regions we need to
                    major compact to purge the HFileV1s), and any corrupted files if any found. A
                    corrupt file is unreadable, and so is undefined (neither HFileV1 nor HFileV2). </p><p>To run the check step, run <span class="command"><strong>$ bin/hbase upgrade -check</strong></span>. Here
                    is sample output:</p><pre class="screen">
Tables Processed:
hdfs://localhost:41020/myHBase/.META.
hdfs://localhost:41020/myHBase/usertable
hdfs://localhost:41020/myHBase/TestTable
hdfs://localhost:41020/myHBase/t

Count of HFileV1: 2
HFileV1:
hdfs://localhost:41020/myHBase/usertable    /fa02dac1f38d03577bd0f7e666f12812/family/249450144068442524
hdfs://localhost:41020/myHBase/usertable    /ecdd3eaee2d2fcf8184ac025555bb2af/family/249450144068442512

Count of corrupted files: 1
Corrupted Files:
hdfs://localhost:41020/myHBase/usertable/fa02dac1f38d03577bd0f7e666f12812/family/1
Count of Regions with HFileV1: 2
Regions to Major Compact:
hdfs://localhost:41020/myHBase/usertable/fa02dac1f38d03577bd0f7e666f12812
hdfs://localhost:41020/myHBase/usertable/ecdd3eaee2d2fcf8184ac025555bb2af

There are some HFileV1, or corrupt files (files with incorrect major version)
                </pre><p>In the above sample output, there are two HFileV1 in two regions, and one
                    corrupt file. Corrupt files should probably be removed. The regions that have
                    HFileV1s need to be major compacted. To major compact, start up the hbase shell
                    and review how to compact an individual region. After the major compaction is
                    done, rerun the check step and the HFileV1s shoudl be gone, replaced by HFileV2
                    instances. </p><p>By default, the check step scans the hbase root directory (defined as
                    hbase.rootdir in the configuration). To scan a specific directory only, pass the
                        <span class="emphasis"><em>-dir</em></span> option.</p><pre class="screen">$ bin/hbase upgrade -check -dir /myHBase/testTable</pre><p>The above command would detect HFileV1s in the /myHBase/testTable directory. </p><p> Once the check step reports all the HFileV1 files have been rewritten, it is
                    safe to proceed with the upgrade. </p></div><div class="section" title="1.5.1.2.&nbsp;execute"><div class="titlepage"><div><div><h4 class="title"><a name="d2494e231"></a>1.5.1.2.&nbsp;execute</h4></div></div></div><p>After the check step shows the cluster is free of HFileV1, it is safe to
                    proceed with the upgrade. Next is the <span class="emphasis"><em>execute</em></span> step. You
                    must <span class="emphasis"><em>SHUTDOWN YOUR 0.94.x CLUSTER</em></span> before you can run the
                        <span class="emphasis"><em>execute</em></span> step. The execute step will not run if it
                    detects running HBase masters or regionservers. </p><div class="note" title="Note" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title">Note</h3><p>HDFS and ZooKeeper should be up and running during the upgrade
                            process. If zookeeper is managed by HBase, then you can start zookeeper
                            so it is available to the upgrade by running <span class="command"><strong>$
                                ./hbase/bin/hbase-daemon.sh start zookeeper</strong></span>
                        </p></div><p>
                </p><p> The <span class="emphasis"><em>execute</em></span> upgrade step is made of three substeps. </p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem"><p>Namespaces: HBase 0.96.0 has support for namespaces. The upgrade needs
                            to reorder directories in the filesystem for namespaces to work.</p></li><li class="listitem"><p>ZNodes: All znodes are purged so that new ones can be written in their
                            place using a new protobuf'ed format and a few are migrated in place:
                            e.g. replication and table state znodes</p></li><li class="listitem"><p>WAL Log Splitting: If the 0.94.x cluster shutdown was not clean, we'll
                            split WAL logs as part of migration before we startup on 0.96.0. This
                            WAL splitting runs slower than the native distributed WAL splitting
                            because it is all inside the single upgrade process (so try and get a
                            clean shutdown of the 0.94.0 cluster if you can). </p></li></ul></div><p> To run the <span class="emphasis"><em>execute</em></span> step, make sure that first you have
                    copied hbase-0.96.0 binaries everywhere under servers and under clients. Make
                    sure the 0.94.0 cluster is down. Then do as follows:</p><pre class="screen">$ bin/hbase upgrade -execute</pre><p>Here is some sample output.</p><pre class="programlisting">
Starting Namespace upgrade
Created version file at hdfs://localhost:41020/myHBase with version=7
Migrating table testTable to hdfs://localhost:41020/myHBase/.data/default/testTable
&#8230;..
Created version file at hdfs://localhost:41020/myHBase with version=8
Successfully completed NameSpace upgrade.
Starting Znode upgrade
&#8230;.
Successfully completed Znode upgrade

Starting Log splitting
&#8230;
Successfully completed Log splitting
         </pre><p> If the output from the execute step looks good, stop the zookeeper instance
                    you started to do the upgrade:
                    </p><pre class="programlisting">$ ./hbase/bin/hbase-daemon.sh stop zookeeper</pre><p>
                    Now start up hbase-0.96.0. </p></div><div class="section" title="1.5.1.3.&nbsp;Troubleshooting"><div class="titlepage"><div><div><h4 class="title"><a name="s096.migration.troubleshooting"></a>1.5.1.3.&nbsp;Troubleshooting</h4></div></div></div><div class="section" title="1.5.1.3.1.&nbsp;Old Client connecting to 0.96 cluster"><div class="titlepage"><div><div><h5 class="title"><a name="s096.migration.troubleshooting.old.client"></a>1.5.1.3.1.&nbsp;Old Client connecting to 0.96 cluster</h5></div></div></div><p>It will fail with an exception like the below. Upgrade.</p><pre class="screen">17:22:15  Exception in thread "main" java.lang.IllegalArgumentException: Not a host:port pair: PBUF
17:22:15  *
17:22:15   api-compat-8.ent.cloudera.com &#65533;&#65533;  &#65533;&#65533;&#65533;(
17:22:15    at org.apache.hadoop.hbase.util.Addressing.parseHostname(Addressing.java:60)
17:22:15    at org.apache.hadoop.hbase.ServerName.&amp;init&gt;(ServerName.java:101)
17:22:15    at org.apache.hadoop.hbase.ServerName.parseVersionedServerName(ServerName.java:283)
17:22:15    at org.apache.hadoop.hbase.MasterAddressTracker.bytesToServerName(MasterAddressTracker.java:77)
17:22:15    at org.apache.hadoop.hbase.MasterAddressTracker.getMasterAddress(MasterAddressTracker.java:61)
17:22:15    at org.apache.hadoop.hbase.client.HConnectionManager$HConnectionImplementation.getMaster(HConnectionManager.java:703)
17:22:15    at org.apache.hadoop.hbase.client.HBaseAdmin.&amp;init&gt;(HBaseAdmin.java:126)
17:22:15    at Client_4_3_0.setup(Client_4_3_0.java:716)
17:22:15    at Client_4_3_0.main(Client_4_3_0.java:63)</pre></div></div><div class="section" title="1.5.1.4.&nbsp;Upgrading META to use Protocol Buffers (Protobuf)"><div class="titlepage"><div><div><h4 class="title"><a name="d2494e293"></a>1.5.1.4.&nbsp;Upgrading <code class="code">META</code> to use Protocol Buffers (Protobuf)</h4></div></div></div><p>When you upgrade from versions prior to 0.96, <code class="code">META</code> needs to be
                    converted to use protocol buffers. This is controlled by the configuration
                    option <code class="option">hbase.MetaMigrationConvertingToPB</code>, which is set to
                        <code class="literal">true</code> by default. Therefore, by default, no action is
                    required on your part.</p><p>The migration is a one-time event. However, every time your cluster starts,
                        <code class="code">META</code> is scanned to ensure that it does not need to be
                    converted. If you have a very large number of regions, this scan can take a long
                    time. Starting in 0.98.5, you can set
                        <code class="option">hbase.MetaMigrationConvertingToPB</code> to
                        <code class="literal">false</code> in <code class="filename">hbase-site.xml</code>, to disable
                    this start-up scan. This should be considered an expert-level setting.</p></div></div></div><div class="section" title="1.6.&nbsp;Upgrading from 0.92.x to 0.94.x"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="upgrade0.94"></a>1.6.&nbsp;Upgrading from 0.92.x to 0.94.x</h2></div></div></div><p>We used to think that 0.92 and 0.94 were interface compatible and that you can do a
            rolling upgrade between these versions but then we figured that <a class="link" href="https://issues.apache.org/jira/browse/HBASE-5357" target="_top">HBASE-5357 Use builder
                pattern in HColumnDescriptor</a> changed method signatures so rather than return
            void they instead return HColumnDescriptor. This will throw</p><pre class="screen">java.lang.NoSuchMethodError: org.apache.hadoop.hbase.HColumnDescriptor.setMaxVersions(I)V</pre><p>.... so 0.92 and 0.94 are NOT compatible. You cannot do a rolling upgrade between them.</p></div><div class="section" title="1.7.&nbsp;Upgrading from 0.90.x to 0.92.x"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="upgrade0.92"></a>1.7.&nbsp;Upgrading from 0.90.x to 0.92.x</h2></div><div><h3 class="subtitle">Upgrade Guide</h3></div></div></div><p>You will find that 0.92.0 runs a little differently to 0.90.x releases. Here are a few
            things to watch out for upgrading from 0.90.x to 0.92.0. </p><div class="note" title="tl;dr" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title">tl;dr</h3><p> If you've not patience, here are the important things to know upgrading. </p><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem"><p>Once you upgrade, you can&#8217;t go back.</p></li><li class="listitem"><p> MSLAB is on by default. Watch that heap usage if you have a lot of
                            regions.</p></li><li class="listitem"><p> Distributed Log Splitting is on by default. It should make region server
                            failover faster. </p></li><li class="listitem"><p> There&#8217;s a separate tarball for security. </p></li><li class="listitem"><p> If -XX:MaxDirectMemorySize is set in your hbase-env.sh, it&#8217;s going to
                            enable the experimental off-heap cache (You may not want this). </p></li></ol></div><p>
            </p></div><div class="section" title="1.7.1.&nbsp;You can&#8217;t go back!"><div class="titlepage"><div><div><h3 class="title"><a name="d2494e365"></a>1.7.1.&nbsp;You can&#8217;t go back! </h3></div></div></div><p>To move to 0.92.0, all you need to do is shutdown your cluster, replace your hbase
                0.90.x with hbase 0.92.0 binaries (be sure you clear out all 0.90.x instances) and
                restart (You cannot do a rolling restart from 0.90.x to 0.92.x -- you must restart).
                On startup, the <code class="varname">.META.</code> table content is rewritten removing the
                table schema from the <code class="varname">info:regioninfo</code> column. Also, any flushes
                done post first startup will write out data in the new 0.92.0 file format, <a class="link" href="http://hbase.apache.org/book.html#hfilev2" target="_top">HFile V2</a>. This
                means you cannot go back to 0.90.x once you&#8217;ve started HBase 0.92.0 over your HBase
                data directory. </p></div><div class="section" title="1.7.2.&nbsp;MSLAB is ON by default"><div class="titlepage"><div><div><h3 class="title"><a name="d2494e379"></a>1.7.2.&nbsp;MSLAB is ON by default </h3></div></div></div><p>In 0.92.0, the <a class="link" href="http://hbase.apache.org/book.html#hbase.hregion.memstore.mslab.enabled" target="_top">hbase.hregion.memstore.mslab.enabled</a>
                flag is set to true (See <a class="xref" href="#">???</a>). In 0.90.x it was <code class="constant">false</code>. When it is
                enabled, memstores will step allocate memory in MSLAB 2MB chunks even if the
                memstore has zero or just a few small elements. This is fine usually but if you had
                lots of regions per regionserver in a 0.90.x cluster (and MSLAB was off), you may
                find yourself OOME'ing on upgrade because the <code class="code">thousands of regions * number of
                    column families * 2MB MSLAB (at a minimum)</code> puts your heap over the top.
                Set <code class="varname">hbase.hregion.memstore.mslab.enabled</code> to
                    <code class="constant">false</code> or set the MSLAB size down from 2MB by setting
                    <code class="varname">hbase.hregion.memstore.mslab.chunksize</code> to something less.
            </p></div><div class="section" title="1.7.3.&nbsp;Distributed Log Splitting is on by default"><div class="titlepage"><div><div><h3 class="title"><a name="dls"></a>1.7.3.&nbsp;Distributed Log Splitting is on by default </h3></div></div></div><p>Previous, WAL logs on crash were split by the Master alone. In 0.92.0, log
                splitting is done by the cluster (See See &#8220;HBASE-1364 [performance] Distributed
                splitting of regionserver commit logs&#8221; or see the blog post
                <a class="link" href="http://blog.cloudera.com/blog/2012/07/hbase-log-splitting/" target="_top">Apache HBase Log Splitting</a>).
                This should cut down significantly on the
                amount of time it takes splitting logs and getting regions back online again.
            </p></div><div class="section" title="1.7.4.&nbsp;Memory accounting is different now"><div class="titlepage"><div><div><h3 class="title"><a name="d2494e412"></a>1.7.4.&nbsp;Memory accounting is different now </h3></div></div></div><p>In 0.92.0, <a class="xref" href="#">???</a> indices and bloom filters take up residence in the same LRU
                used caching blocks that come from the filesystem. In 0.90.x, the HFile v1 indices
                lived outside of the LRU so they took up space even if the index was on a &#8216;cold&#8217;
                file, one that wasn&#8217;t being actively used. With the indices now in the LRU, you may
                find you have less space for block caching. Adjust your block cache accordingly. See
                the <a class="xref" href="#">???</a> for more detail. The block size default size has been
                changed in 0.92.0 from 0.2 (20 percent of heap) to 0.25. </p></div><div class="section" title="1.7.5.&nbsp;On the Hadoop version to use"><div class="titlepage"><div><div><h3 class="title"><a name="d2494e421"></a>1.7.5.&nbsp;On the Hadoop version to use </h3></div></div></div><p>Run 0.92.0 on Hadoop 1.0.x (or CDH3u3 when it ships). The performance benefits are
                worth making the move. Otherwise, our Hadoop prescription is as it has been; you
                need an Hadoop that supports a working sync. See <a class="xref" href="#">???</a>. </p><p>If running on Hadoop 1.0.x (or CDH3u3), enable local read. See <a class="link" href="http://files.meetup.com/1350427/hug_ebay_jdcryans.pdf" target="_top">Practical
                    Caching</a> presentation for ruminations on the performance benefits &#8216;going
                local&#8217; (and for how to enable local reads). </p></div><div class="section" title="1.7.6.&nbsp;HBase 0.92.0 ships with ZooKeeper 3.4.2"><div class="titlepage"><div><div><h3 class="title"><a name="d2494e433"></a>1.7.6.&nbsp;HBase 0.92.0 ships with ZooKeeper 3.4.2 </h3></div></div></div><p>If you can, upgrade your zookeeper. If you can&#8217;t, 3.4.2 clients should work
                against 3.3.X ensembles (HBase makes use of 3.4.2 API). </p></div><div class="section" title="1.7.7.&nbsp;Online alter is off by default"><div class="titlepage"><div><div><h3 class="title"><a name="d2494e438"></a>1.7.7.&nbsp;Online alter is off by default </h3></div></div></div><p>In 0.92.0, we&#8217;ve added an experimental online schema alter facility (See <a class="xref" href="#">???</a>). Its off by default. Enable it
                at your own risk. Online alter and splitting tables do not play well together so be
                sure your cluster quiescent using this feature (for now). </p></div><div class="section" title="1.7.8.&nbsp;WebUI"><div class="titlepage"><div><div><h3 class="title"><a name="d2494e445"></a>1.7.8.&nbsp;WebUI </h3></div></div></div><p>The webui has had a few additions made in 0.92.0. It now shows a list of the
                regions currently transitioning, recent compactions/flushes, and a process list of
                running processes (usually empty if all is well and requests are being handled
                promptly). Other additions including requests by region, a debugging servlet dump,
                etc. </p></div><div class="section" title="1.7.9.&nbsp;Security tarball"><div class="titlepage"><div><div><h3 class="title"><a name="d2494e450"></a>1.7.9.&nbsp;Security tarball </h3></div></div></div><p>We now ship with two tarballs; secure and insecure HBase. Documentation on how to
                setup a secure HBase is on the way. </p></div><div class="section" title="1.7.10.&nbsp;Changes in HBase replication"><div class="titlepage"><div><div><h3 class="title"><a name="d2494e455"></a>1.7.10.&nbsp;Changes in HBase replication </h3></div></div></div><p>0.92.0 adds two new features: multi-slave and multi-master replication. The way to
                enable this is the same as adding a new peer, so in order to have multi-master you
                would just run add_peer for each cluster that acts as a master to the other slave
                clusters. Collisions are handled at the timestamp level which may or may not be what
                you want, this needs to be evaluated on a per use case basis. Replication is still
                experimental in 0.92 and is disabled by default, run it at your own risk. </p></div><div class="section" title="1.7.11.&nbsp;RegionServer now aborts if OOME"><div class="titlepage"><div><div><h3 class="title"><a name="d2494e460"></a>1.7.11.&nbsp;RegionServer now aborts if OOME </h3></div></div></div><p>If an OOME, we now have the JVM kill -9 the regionserver process so it goes down
                fast. Previous, a RegionServer might stick around after incurring an OOME limping
                along in some wounded state. To disable this facility, and recommend you leave it in
                place, you&#8217;d need to edit the bin/hbase file. Look for the addition of the
                -XX:OnOutOfMemoryError="kill -9 %p" arguments (See [HBASE-4769] - &#8216;Abort
                RegionServer Immediately on OOME&#8217;) </p></div><div class="section" title="1.7.12.&nbsp;HFile V2 and the &#8220;Bigger, Fewer&#8221; Tendency"><div class="titlepage"><div><div><h3 class="title"><a name="d2494e465"></a>1.7.12.&nbsp;HFile V2 and the &#8220;Bigger, Fewer&#8221; Tendency </h3></div></div></div><p>0.92.0 stores data in a new format, <a class="xref" href="#">???</a>. As HBase runs, it will move all your data from HFile v1 to
                HFile v2 format. This auto-migration will run in the background as flushes and
                compactions run. HFile V2 allows HBase run with larger regions/files. In fact, we
                encourage that all HBasers going forward tend toward Facebook axiom #1, run with
                larger, fewer regions. If you have lots of regions now -- more than 100s per host --
                you should look into setting your region size up after you move to 0.92.0 (In
                0.92.0, default size is now 1G, up from 256M), and then running online merge tool
                (See &#8220;HBASE-1621 merge tool should work on online cluster, but disabled table&#8221;).
            </p></div></div><div class="section" title="1.8.&nbsp;Upgrading to HBase 0.90.x from 0.20.x or 0.89.x"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="upgrade0.90"></a>1.8.&nbsp;Upgrading to HBase 0.90.x from 0.20.x or 0.89.x</h2></div></div></div><p>This version of 0.90.x HBase can be started on data written by HBase 0.20.x or HBase
            0.89.x. There is no need of a migration step. HBase 0.89.x and 0.90.x does write out the
            name of region directories differently -- it names them with a md5 hash of the region
            name rather than a jenkins hash -- so this means that once started, there is no going
            back to HBase 0.20.x. </p><p> Be sure to remove the <code class="filename">hbase-default.xml</code> from your
                <code class="filename">conf</code> directory on upgrade. A 0.20.x version of this file will
            have sub-optimal configurations for 0.90.x HBase. The
                <code class="filename">hbase-default.xml</code> file is now bundled into the HBase jar and
            read from there. If you would like to review the content of this file, see it in the src
            tree at <code class="filename">src/main/resources/hbase-default.xml</code> or see <a class="xref" href="#">???</a>. </p><p> Finally, if upgrading from 0.20.x, check your <code class="varname">.META.</code> schema in the
            shell. In the past we would recommend that users run with a 16kb
                <code class="varname">MEMSTORE_FLUSHSIZE</code>. Run <code class="code">hbase&gt; scan '-ROOT-'</code> in the
            shell. This will output the current <code class="varname">.META.</code> schema. Check
                <code class="varname">MEMSTORE_FLUSHSIZE</code> size. Is it 16kb (16384)? If so, you will need
            to change this (The 'normal'/default value is 64MB (67108864)). Run the script
                <code class="filename">bin/set_meta_memstore_size.rb</code>. This will make the necessary
            edit to your <code class="varname">.META.</code> schema. Failure to run this change will make for
            a slow cluster. See <a class="link" href="https://issues.apache.org/jira/browse/HBASE-3499" target="_top">HBASE-3499
                        Users upgrading to 0.90.0 need to have their .META. table updated with the
                        right MEMSTORE_SIZE</a>
                </p></div></div><div id="disqus_thread"></div><script type="text/javascript">
    /* * * DON'T EDIT BELOW THIS LINE * * */
    (function() {
        var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
        dsq.src = 'http://' + disqus_shortname + '.disqus.com/embed.js';
        (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
    })();
</script><noscript>Please enable JavaScript to view the <a href="http://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript><a href="http://disqus.com" class="dsq-brlink">comments powered by <span class="logo-disqus">Disqus</span></a></body></html>