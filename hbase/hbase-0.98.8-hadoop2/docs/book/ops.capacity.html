<html><head>
      <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
   <title>17.9.&nbsp;Capacity Planning and Region Sizing</title><link rel="stylesheet" type="text/css" href="../css/freebsd_docbook.css"><meta name="generator" content="DocBook XSL-NS Stylesheets V1.76.1"><link rel="home" href="book.html" title="The Apache HBase&#153; Reference Guide"><link rel="up" href="ops_mgt.html" title="Chapter&nbsp;17.&nbsp;Apache HBase Operational Management"><link rel="prev" href="ops.snapshots.html" title="17.8.&nbsp;HBase Snapshots"><link rel="next" href="table.rename.html" title="17.10.&nbsp;Table Rename"></head><body bgcolor="white" text="black" link="#0000FF" vlink="#840084" alink="#0000FF"><div class="navheader"><table width="100%" summary="Navigation header"><tr><th colspan="3" align="center">17.9.&nbsp;Capacity Planning and Region Sizing</th></tr><tr><td width="20%" align="left"><a accesskey="p" href="ops.snapshots.html">Prev</a>&nbsp;</td><th width="60%" align="center">Chapter&nbsp;17.&nbsp;Apache HBase Operational Management</th><td width="20%" align="right">&nbsp;<a accesskey="n" href="table.rename.html">Next</a></td></tr></table><hr></div><script type="text/javascript">
    var disqus_shortname = 'hbase'; // required: replace example with your forum shortname
    var disqus_url = 'http://hbase.apache.org/book/ops.capacity.html';
    </script><div class="section" title="17.9.&nbsp;Capacity Planning and Region Sizing"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="ops.capacity"></a>17.9.&nbsp;Capacity Planning and Region Sizing</h2></div></div></div><p>There are several considerations when planning the capacity for an HBase cluster and
      performing the initial configuration. Start with a solid understanding of how HBase handles
      data internally.</p><div class="section" title="17.9.1.&nbsp;Node count and hardware/VM configuration"><div class="titlepage"><div><div><h3 class="title"><a name="ops.capacity.nodes"></a>17.9.1.&nbsp;Node count and hardware/VM configuration</h3></div></div></div><div class="section" title="17.9.1.1.&nbsp;Physical data size"><div class="titlepage"><div><div><h4 class="title"><a name="ops.capacity.nodes.datasize"></a>17.9.1.1.&nbsp;Physical data size</h4></div></div></div><p>Physical data size on disk is distinct from logical size of your data and is affected
          by the following: </p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem"><p>Increased by HBase overhead</p><div class="itemizedlist"><ul class="itemizedlist" type="circle"><li class="listitem"><p>See <a class="xref" href="regions.arch.html#keyvalue" title="9.7.7.6.&nbsp;KeyValue">Section&nbsp;9.7.7.6, &#8220;KeyValue&#8221;</a> and <a class="xref" href="rowkey.design.html#keysize" title="6.3.3.&nbsp;Try to minimize row and column sizes">Section&nbsp;6.3.3, &#8220;Try to minimize row and column sizes&#8221;</a>. At least 24 bytes per key-value (cell), can be more. Small
                  keys/values means more relative overhead.</p></li><li class="listitem"><p>KeyValue instances are aggregated into blocks, which are indexed. Indexes also
                  have to be stored. Blocksize is configurable on a per-ColumnFamily basis. See <a class="xref" href="regions.arch.html" title="9.7.&nbsp;Regions">Section&nbsp;9.7, &#8220;Regions&#8221;</a>.</p></li></ul></div></li><li class="listitem"><p>Decreased by <a class="xref" href="compression.html" title="Appendix&nbsp;E.&nbsp;Compression and Data Block Encoding In HBase">compression</a> and data block encoding, depending on data. See
              also <a class="link" href="http://search-hadoop.com/m/lL12B1PFVhp1" target="_top">this thread</a>. You might
              want to test what compression and encoding (if any) make sense for your data.</p></li><li class="listitem"><p>Increased by size of region server <a class="xref" href="regionserver.arch.html#wal" title="9.6.5.&nbsp;Write Ahead Log (WAL)">WAL</a> (usually fixed and negligible - less than half of RS
              memory size, per RS).</p></li><li class="listitem"><p>Increased by HDFS replication - usually x3.</p></li></ul></div><p>Aside from the disk space necessary to store the data, one RS may not be able to serve
          arbitrarily large amounts of data due to some practical limits on region count and size
          (see <a class="xref" href="ops.capacity.html#ops.capacity.regions" title="17.9.2.&nbsp;Determining region count and size">below</a>).</p></div><div class="section" title="17.9.1.2.&nbsp;Read/Write throughput"><div class="titlepage"><div><div><h4 class="title"><a name="ops.capacity.nodes.throughput"></a>17.9.1.2.&nbsp;Read/Write throughput</h4></div></div></div><p>Number of nodes can also be driven by required thoughput for reads and/or writes. The
          throughput one can get per node depends a lot on data (esp. key/value sizes) and request
          patterns, as well as node and system configuration. Planning should be done for peak load
          if it is likely that the load would be the main driver of the increase of the node count.
          PerformanceEvaluation and <a class="xref" href="apg.html#ycsb">YCSB</a> tools can be used to test single node or a test
          cluster.</p><p>For write, usually 5-15Mb/s per RS can be expected, since every region server has only
          one active WAL. There's no good estimate for reads, as it depends vastly on data,
          requests, and cache hit rate. <a class="xref" href="perf.casestudy.html" title="14.14.&nbsp;Case Studies">Section&nbsp;14.14, &#8220;Case Studies&#8221;</a> might be helpful.</p></div><div class="section" title="17.9.1.3.&nbsp;JVM GC limitations"><div class="titlepage"><div><div><h4 class="title"><a name="ops.capacity.nodes.gc"></a>17.9.1.3.&nbsp;JVM GC limitations</h4></div></div></div><p>RS cannot currently utilize very large heap due to cost of GC. There's also no good
          way of running multiple RS-es per server (other than running several VMs per machine).
          Thus, ~20-24Gb or less memory dedicated to one RS is recommended. GC tuning is required
          for large heap sizes. See <a class="xref" href="jvm.html#gcpause" title="14.3.1.1.&nbsp;Long GC pauses">Section&nbsp;14.3.1.1, &#8220;Long GC pauses&#8221;</a>, <a class="xref" href="trouble.log.html#trouble.log.gc" title="15.2.3.&nbsp;JVM Garbage Collection Logs">Section&nbsp;15.2.3, &#8220;JVM Garbage Collection Logs&#8221;</a> and elsewhere (TODO: where?)</p></div></div><div class="section" title="17.9.2.&nbsp;Determining region count and size"><div class="titlepage"><div><div><h3 class="title"><a name="ops.capacity.regions"></a>17.9.2.&nbsp;Determining region count and size</h3></div></div></div><p>Generally less regions makes for a smoother running cluster (you can always manually
        split the big regions later (if necessary) to spread the data, or request load, over the
        cluster); 20-200 regions per RS is a reasonable range. The number of regions cannot be
        configured directly (unless you go for fully <a class="xref" href="important_configurations.html#disable.splitting" title="2.6.2.7.&nbsp;Managed Splitting">manual splitting</a>); adjust the region size to achieve the target
        region size given table size.</p><p>When configuring regions for multiple tables, note that most region settings can be set
        on a per-table basis via <a class="link" href="http://hbase.apache.org/apidocs/org/apache/hadoop/hbase/HTableDescriptor.html" target="_top">HTableDescriptor</a>,
        as well as shell commands. These settings will override the ones in
          <code class="varname">hbase-site.xml</code>. That is useful if your tables have different
        workloads/use cases.</p><p>Also note that in the discussion of region sizes here, <span class="bold"><strong>HDFS replication factor is not (and should not be) taken into account, whereas
          other factors <a class="xref" href="ops.capacity.html#ops.capacity.nodes.datasize" title="17.9.1.1.&nbsp;Physical data size">above</a> should be.</strong></span> So, if your data is compressed and
        replicated 3 ways by HDFS, "9 Gb region" means 9 Gb of compressed data. HDFS replication
        factor only affects your disk usage and is invisible to most HBase code.</p><div class="section" title="17.9.2.1.&nbsp;Viewing the Current Number of Regions"><div class="titlepage"><div><div><h4 class="title"><a name="d2875e19314"></a>17.9.2.1.&nbsp;Viewing the Current Number of Regions</h4></div></div></div><p>You can view the current number of regions for a given table using the HMaster UI. In
          the <span class="guilabel">Tables</span> section, the number of online regions for each table is
          listed in the <span class="guilabel">Online Regions</span> column. This total only includes the
          in-memory state and does not include disabled or offline regions. If you do not want to
          use the HMaster UI, you can determine the number of regions by counting the number of
          subdirectories of the /hbase/&lt;table&gt;/ subdirectories in HDFS, or by running the
            <span class="command"><strong>bin/hbase hbck</strong></span> command. Each of these methods may return a slightly
          different number, depending on the status of each region.</p></div><div class="section" title="17.9.2.2.&nbsp;Number of regions per RS - upper bound"><div class="titlepage"><div><div><h4 class="title"><a name="ops.capacity.regions.count"></a>17.9.2.2.&nbsp;Number of regions per RS - upper bound</h4></div></div></div><p>In production scenarios, where you have a lot of data, you are normally concerned with
          the maximum number of regions you can have per server. <a class="xref" href="regions.arch.html#too_many_regions" title="9.7.1.1.&nbsp;Why cannot I have too many regions?">Section&nbsp;9.7.1.1, &#8220;Why cannot I have too many regions?&#8221;</a>
          has technical discussion on the subject. Basically, the maximum number of regions is
          mostly determined by memstore memory usage. Each region has its own memstores; these grow
          up to a configurable size; usually in 128-256 MB range, see <a class="xref" href="config.files.html#hbase.hregion.memstore.flush.size" title="hbase.hregion.memstore.flush.size"><code class="varname">hbase.hregion.memstore.flush.size</code></a>. One memstore exists per column family (so
          there's only one per region if there's one CF in the table). The RS dedicates some
          fraction of total memory to its memstores (see <a class="xref" href="">???</a>). If this memory is exceeded (too
          much memstore usage), it can cause undesirable consequences such as unresponsive server or
          compaction storms. A good starting point for the number of regions per RS (assuming one
          table) is:</p><pre class="programlisting">((RS memory) * (total memstore fraction)) / ((memstore size)*(# column families))</pre><p>This formula is pseudo-code. Here are two formulas using the actual tunable
          parameters, first for HBase 0.98+ and second for HBase 0.94.x.</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem"><p>HBase 0.98.x:<code class="code">((RS Xmx) * hbase.regionserver.global.memstore.size) /
                (hbase.hregion.memstore.flush.size * (# column families))</code></p></li><li class="listitem"><p>HBase 0.94.x:<code class="code">((RS Xmx) * hbase.regionserver.global.memstore.upperLimit) /
                (hbase.hregion.memstore.flush.size * (# column families))</code></p></li></ul></div><p>If a given RegionServer has 16 GB of RAM, with default settings, the formula works out
          to 16384*0.4/128 ~ 51 regions per RS is a starting point. The formula can be extended to
          multiple tables; if they all have the same configuration, just use the total number of
          families.</p><p>This number can be adjusted; the formula above assumes all your regions are filled at
          approximately the same rate. If only a fraction of your regions are going to be actively
          written to, you can divide the result by that fraction to get a larger region count. Then,
          even if all regions are written to, all region memstores are not filled evenly, and
          eventually jitter appears even if they are (due to limited number of concurrent flushes).
          Thus, one can have as many as 2-3 times more regions than the starting point; however,
          increased numbers carry increased risk.</p><p>For write-heavy workload, memstore fraction can be increased in configuration at the
          expense of block cache; this will also allow one to have more regions.</p></div><div class="section" title="17.9.2.3.&nbsp;Number of regions per RS - lower bound"><div class="titlepage"><div><div><h4 class="title"><a name="ops.capacity.regions.mincount"></a>17.9.2.3.&nbsp;Number of regions per RS - lower bound</h4></div></div></div><p>HBase scales by having regions across many servers. Thus if you have 2 regions for
          16GB data, on a 20 node machine your data will be concentrated on just a few machines -
          nearly the entire cluster will be idle. This really can't be stressed enough, since a
          common problem is loading 200MB data into HBase and then wondering why your awesome 10
          node cluster isn't doing anything.</p><p>On the other hand, if you have a very large amount of data, you may also want to go
          for a larger number of regions to avoid having regions that are too large.</p></div><div class="section" title="17.9.2.4.&nbsp;Maximum region size"><div class="titlepage"><div><div><h4 class="title"><a name="ops.capacity.regions.size"></a>17.9.2.4.&nbsp;Maximum region size</h4></div></div></div><p>For large tables in production scenarios, maximum region size is mostly limited by
          compactions - very large compactions, esp. major, can degrade cluster performance.
          Currently, the recommended maximum region size is 10-20Gb, and 5-10Gb is optimal. For
          older 0.90.x codebase, the upper-bound of regionsize is about 4Gb, with a default of
          256Mb.</p><p>The size at which the region is split into two is generally configured via <a class="xref" href="config.files.html#hbase.hregion.max.filesize" title="hbase.hregion.max.filesize"><code class="varname">hbase.hregion.max.filesize</code></a>; for details, see <a class="xref" href="regions.arch.html#arch.region.splits" title="9.7.4.&nbsp;Region Splits">Section&nbsp;9.7.4, &#8220;Region Splits&#8221;</a>.</p><p>If you cannot estimate the size of your tables well, when starting off, it's probably
          best to stick to the default region size, perhaps going smaller for hot tables (or
          manually split hot regions to spread the load over the cluster), or go with larger region
          sizes if your cell sizes tend to be largish (100k and up).</p><p>In HBase 0.98, experimental stripe compactions feature was added that would allow for
          larger regions, especially for log data. See <a class="xref" href="regions.arch.html#ops.stripe" title="9.7.7.7.3.&nbsp;Experimental: Stripe Compactions">Section&nbsp;9.7.7.7.3, &#8220;Experimental: Stripe Compactions&#8221;</a>.</p></div><div class="section" title="17.9.2.5.&nbsp;Total data size per region server"><div class="titlepage"><div><div><h4 class="title"><a name="ops.capacity.regions.total"></a>17.9.2.5.&nbsp;Total data size per region server</h4></div></div></div><p>According to above numbers for region size and number of regions per region server, in
          an optimistic estimate 10 GB x 100 regions per RS will give up to 1TB served per region
          server, which is in line with some of the reported multi-PB use cases. However, it is
          important to think about the data vs cache size ratio at the RS level. With 1TB of data
          per server and 10 GB block cache, only 1% of the data will be cached, which may barely
          cover all block indices.</p></div></div><div class="section" title="17.9.3.&nbsp;Initial configuration and tuning"><div class="titlepage"><div><div><h3 class="title"><a name="ops.capacity.config"></a>17.9.3.&nbsp;Initial configuration and tuning</h3></div></div></div><p>First, see <a class="xref" href="important_configurations.html" title="2.6.&nbsp;The Important Configurations">Section&nbsp;2.6, &#8220;The Important Configurations&#8221;</a>. Note that some configurations, more than others,
        depend on specific scenarios. Pay special attention to:</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem"><p><a class="xref" href="config.files.html#hbase.regionserver.handler.count" title="hbase.regionserver.handler.count"><code class="varname">hbase.regionserver.handler.count</code></a> - request handler thread count, vital
            for high-throughput workloads.</p></li><li class="listitem"><p><a class="xref" href="important_configurations.html#config.wals" title="2.6.2.6.&nbsp;Configuring the size and number of WAL files">Section&nbsp;2.6.2.6, &#8220;Configuring the size and number of WAL files&#8221;</a> - the blocking number of WAL files depends on your memstore
            configuration and should be set accordingly to prevent potential blocking when doing
            high volume of writes.</p></li></ul></div><p>Then, there are some considerations when setting up your cluster and tables.</p><div class="section" title="17.9.3.1.&nbsp;Compactions"><div class="titlepage"><div><div><h4 class="title"><a name="ops.capacity.config.compactions"></a>17.9.3.1.&nbsp;Compactions</h4></div></div></div><p>Depending on read/write volume and latency requirements, optimal compaction settings
          may be different. See <a class="xref" href="regions.arch.html#compaction" title="9.7.7.7.&nbsp;Compaction">Section&nbsp;9.7.7.7, &#8220;Compaction&#8221;</a> for some details.</p><p>When provisioning for large data sizes, however, it's good to keep in mind that
          compactions can affect write throughput. Thus, for write-intensive workloads, you may opt
          for less frequent compactions and more store files per regions. Minimum number of files
          for compactions (<code class="varname">hbase.hstore.compaction.min</code>) can be set to higher
          value; <a class="xref" href="config.files.html#hbase.hstore.blockingStoreFiles" title="hbase.hstore.blockingStoreFiles"><code class="varname">hbase.hstore.blockingStoreFiles</code></a> should also be increased, as more files
          might accumulate in such case. You may also consider manually managing compactions: <a class="xref" href="important_configurations.html#managed.compactions" title="2.6.2.8.&nbsp;Managed Compactions">Section&nbsp;2.6.2.8, &#8220;Managed Compactions&#8221;</a></p></div><div class="section" title="17.9.3.2.&nbsp;Pre-splitting the table"><div class="titlepage"><div><div><h4 class="title"><a name="ops.capacity.config.presplit"></a>17.9.3.2.&nbsp;Pre-splitting the table</h4></div></div></div><p>Based on the target number of the regions per RS (see <a class="xref" href="ops.capacity.html#ops.capacity.regions.count" title="17.9.2.2.&nbsp;Number of regions per RS - upper bound">above</a>) and number of RSes, one can pre-split the table at
          creation time. This would both avoid some costly splitting as the table starts to fill up,
          and ensure that the table starts out already distributed across many servers.</p><p>If the table is expected to grow large enough to justify that, at least one region per
          RS should be created. It is not recommended to split immediately into the full target
          number of regions (e.g. 50 * number of RSes), but a low intermediate value can be chosen.
          For multiple tables, it is recommended to be conservative with presplitting (e.g.
          pre-split 1 region per RS at most), especially if you don't know how much each table will
          grow. If you split too much, you may end up with too many regions, with some tables having
          too many small regions.</p><p>For pre-splitting howto, see <a class="xref" href="regions.arch.html#manual_region_splitting_decisions" title="9.7.5.&nbsp;Manual Region Splitting">Section&nbsp;9.7.5, &#8220;Manual Region Splitting&#8221;</a> and
            <a class="xref" href="perf.writing.html#precreate.regions" title="14.8.2.&nbsp; Table Creation: Pre-Creating Regions">Section&nbsp;14.8.2, &#8220; Table Creation: Pre-Creating Regions &#8221;</a>.</p></div></div></div><div id="disqus_thread"></div><script type="text/javascript">
    /* * * DON'T EDIT BELOW THIS LINE * * */
    (function() {
        var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
        dsq.src = 'http://' + disqus_shortname + '.disqus.com/embed.js';
        (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
    })();
</script><noscript>Please enable JavaScript to view the <a href="http://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript><a href="http://disqus.com" class="dsq-brlink">comments powered by <span class="logo-disqus">Disqus</span></a><div class="navfooter"><hr><table width="100%" summary="Navigation footer"><tr><td width="40%" align="left"><a accesskey="p" href="ops.snapshots.html">Prev</a>&nbsp;</td><td width="20%" align="center"><a accesskey="u" href="ops_mgt.html">Up</a></td><td width="40%" align="right">&nbsp;<a accesskey="n" href="table.rename.html">Next</a></td></tr><tr><td width="40%" align="left" valign="top">17.8.&nbsp;HBase Snapshots&nbsp;</td><td width="20%" align="center"><a accesskey="h" href="book.html">Home</a></td><td width="40%" align="right" valign="top">&nbsp;17.10.&nbsp;Table Rename</td></tr></table></div></body></html>