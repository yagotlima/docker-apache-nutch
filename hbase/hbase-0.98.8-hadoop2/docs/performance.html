<html><head>
      <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
   <title>Chapter&nbsp;1.&nbsp;Apache HBase Performance Tuning</title><link rel="stylesheet" type="text/css" href="css/freebsd_docbook.css"><meta name="generator" content="DocBook XSL-NS Stylesheets V1.76.1"></head><body bgcolor="white" text="black" link="#0000FF" vlink="#840084" alink="#0000FF"><script type="text/javascript">
    var disqus_shortname = 'hbase'; // required: replace example with your forum shortname
    var disqus_url = 'http://hbase.apache.org/book/performance.html';
    </script><div class="chapter" title="Chapter&nbsp;1.&nbsp;Apache HBase Performance Tuning"><div class="titlepage"><div><div><h2 class="title"><a name="performance"></a>Chapter&nbsp;1.&nbsp;Apache HBase Performance Tuning</h2></div></div></div><div class="toc"><p><b>Table of Contents</b></p><dl><dt><span class="section"><a href="#perf.os">1.1. Operating System</a></span></dt><dd><dl><dt><span class="section"><a href="#perf.os.ram">1.1.1. Memory</a></span></dt><dt><span class="section"><a href="#perf.os.64">1.1.2. 64-bit</a></span></dt><dt><span class="section"><a href="#perf.os.swap">1.1.3. Swapping</a></span></dt></dl></dd><dt><span class="section"><a href="#perf.network">1.2. Network</a></span></dt><dd><dl><dt><span class="section"><a href="#perf.network.1switch">1.2.1. Single Switch</a></span></dt><dt><span class="section"><a href="#perf.network.2switch">1.2.2. Multiple Switches</a></span></dt><dt><span class="section"><a href="#perf.network.multirack">1.2.3. Multiple Racks</a></span></dt><dt><span class="section"><a href="#perf.network.ints">1.2.4. Network Interfaces</a></span></dt></dl></dd><dt><span class="section"><a href="#jvm">1.3. Java</a></span></dt><dd><dl><dt><span class="section"><a href="#gc">1.3.1. The Garbage Collector and Apache HBase</a></span></dt></dl></dd><dt><span class="section"><a href="#perf.configurations">1.4. HBase Configurations</a></span></dt><dd><dl><dt><span class="section"><a href="#perf.compactions.and.splits">1.4.1. Managing Compactions</a></span></dt><dt><span class="section"><a href="#perf.handlers">1.4.2. <code class="varname">hbase.regionserver.handler.count</code></a></span></dt><dt><span class="section"><a href="#perf.hfile.block.cache.size">1.4.3. <code class="varname">hfile.block.cache.size</code></a></span></dt><dt><span class="section"><a href="#blockcache.prefetch">1.4.4. Prefetch Option for Blockcache</a></span></dt><dt><span class="section"><a href="#perf.rs.memstore.size">1.4.5. <code class="varname">hbase.regionserver.global.memstore.size</code></a></span></dt><dt><span class="section"><a href="#perf.rs.memstore.size.lower.limit">1.4.6. <code class="varname">hbase.regionserver.global.memstore.size.lower.limit</code></a></span></dt><dt><span class="section"><a href="#perf.hstore.blockingstorefiles">1.4.7. <code class="varname">hbase.hstore.blockingStoreFiles</code></a></span></dt><dt><span class="section"><a href="#perf.hregion.memstore.block.multiplier">1.4.8. <code class="varname">hbase.hregion.memstore.block.multiplier</code></a></span></dt><dt><span class="section"><a href="#hbase.regionserver.checksum.verify">1.4.9. <code class="varname">hbase.regionserver.checksum.verify</code></a></span></dt><dt><span class="section"><a href="#d9542e242">1.4.10. Tuning <code class="code">callQueue</code> Options</a></span></dt></dl></dd><dt><span class="section"><a href="#perf.zookeeper">1.5. ZooKeeper</a></span></dt><dt><span class="section"><a href="#perf.schema">1.6. Schema Design</a></span></dt><dd><dl><dt><span class="section"><a href="#perf.number.of.cfs">1.6.1. Number of Column Families</a></span></dt><dt><span class="section"><a href="#perf.schema.keys">1.6.2. Key and Attribute Lengths</a></span></dt><dt><span class="section"><a href="#schema.regionsize">1.6.3. Table RegionSize</a></span></dt><dt><span class="section"><a href="#schema.bloom">1.6.4. Bloom Filters</a></span></dt><dt><span class="section"><a href="#schema.cf.blocksize">1.6.5. ColumnFamily BlockSize</a></span></dt><dt><span class="section"><a href="#cf.in.memory">1.6.6. In-Memory ColumnFamilies</a></span></dt><dt><span class="section"><a href="#perf.compression">1.6.7. Compression</a></span></dt></dl></dd><dt><span class="section"><a href="#perf.general">1.7. HBase General Patterns</a></span></dt><dd><dl><dt><span class="section"><a href="#perf.general.constants">1.7.1. Constants</a></span></dt></dl></dd><dt><span class="section"><a href="#perf.writing">1.8. Writing to HBase</a></span></dt><dd><dl><dt><span class="section"><a href="#perf.batch.loading">1.8.1. Batch Loading</a></span></dt><dt><span class="section"><a href="#precreate.regions">1.8.2.  Table Creation: Pre-Creating Regions </a></span></dt><dt><span class="section"><a href="#def.log.flush">1.8.3.  Table Creation: Deferred Log Flush </a></span></dt><dt><span class="section"><a href="#perf.hbase.client.autoflush">1.8.4. HBase Client: AutoFlush</a></span></dt><dt><span class="section"><a href="#perf.hbase.client.putwal">1.8.5. HBase Client: Turn off WAL on Puts</a></span></dt><dt><span class="section"><a href="#perf.hbase.client.regiongroup">1.8.6. HBase Client: Group Puts by RegionServer</a></span></dt><dt><span class="section"><a href="#perf.hbase.write.mr.reducer">1.8.7. MapReduce: Skip The Reducer</a></span></dt><dt><span class="section"><a href="#perf.one.region">1.8.8. Anti-Pattern: One Hot Region</a></span></dt></dl></dd><dt><span class="section"><a href="#perf.reading">1.9. Reading from HBase</a></span></dt><dd><dl><dt><span class="section"><a href="#perf.hbase.client.caching">1.9.1. Scan Caching</a></span></dt><dt><span class="section"><a href="#perf.hbase.client.selection">1.9.2. Scan Attribute Selection</a></span></dt><dt><span class="section"><a href="#perf.hbase.client.seek">1.9.3. Avoid scan seeks</a></span></dt><dt><span class="section"><a href="#perf.hbase.mr.input">1.9.4. MapReduce - Input Splits</a></span></dt><dt><span class="section"><a href="#perf.hbase.client.scannerclose">1.9.5. Close ResultScanners</a></span></dt><dt><span class="section"><a href="#perf.hbase.client.blockcache">1.9.6. Block Cache</a></span></dt><dt><span class="section"><a href="#perf.hbase.client.rowkeyonly">1.9.7. Optimal Loading of Row Keys</a></span></dt><dt><span class="section"><a href="#perf.hbase.read.dist">1.9.8. Concurrency: Monitor Data Spread</a></span></dt><dt><span class="section"><a href="#blooms">1.9.9. Bloom Filters</a></span></dt><dt><span class="section"><a href="#d9542e1135">1.9.10. Hedged Reads</a></span></dt></dl></dd><dt><span class="section"><a href="#perf.deleting">1.10. Deleting from HBase</a></span></dt><dd><dl><dt><span class="section"><a href="#perf.deleting.queue">1.10.1. Using HBase Tables as Queues</a></span></dt><dt><span class="section"><a href="#perf.deleting.rpc">1.10.2. Delete RPC Behavior</a></span></dt></dl></dd><dt><span class="section"><a href="#perf.hdfs">1.11. HDFS</a></span></dt><dd><dl><dt><span class="section"><a href="#perf.hdfs.curr">1.11.1. Current Issues With Low-Latency Reads</a></span></dt><dt><span class="section"><a href="#perf.hdfs.configs.localread">1.11.2. Leveraging local data</a></span></dt><dt><span class="section"><a href="#perf.hdfs.comp">1.11.3. Performance Comparisons of HBase vs. HDFS</a></span></dt></dl></dd><dt><span class="section"><a href="#perf.ec2">1.12. Amazon EC2</a></span></dt><dt><span class="section"><a href="#perf.hbase.mr.cluster">1.13. Collocating HBase and MapReduce</a></span></dt><dt><span class="section"><a href="#perf.casestudy">1.14. Case Studies</a></span></dt></dl></div><div class="section" title="1.1.&nbsp;Operating System"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="perf.os"></a>1.1.&nbsp;Operating System</h2></div></div></div><div class="section" title="1.1.1.&nbsp;Memory"><div class="titlepage"><div><div><h3 class="title"><a name="perf.os.ram"></a>1.1.1.&nbsp;Memory</h3></div></div></div><p>RAM, RAM, RAM. Don't starve HBase.</p></div><div class="section" title="1.1.2.&nbsp;64-bit"><div class="titlepage"><div><div><h3 class="title"><a name="perf.os.64"></a>1.1.2.&nbsp;64-bit</h3></div></div></div><p>Use a 64-bit platform (and 64-bit JVM).</p></div><div class="section" title="1.1.3.&nbsp;Swapping"><div class="titlepage"><div><div><h3 class="title"><a name="perf.os.swap"></a>1.1.3.&nbsp;Swapping</h3></div></div></div><p>Watch out for swapping. Set swappiness to 0.</p></div></div><div class="section" title="1.2.&nbsp;Network"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="perf.network"></a>1.2.&nbsp;Network</h2></div></div></div><p> Perhaps the most important factor in avoiding network issues degrading Hadoop and HBase
      performance is the switching hardware that is used, decisions made early in the scope of the
      project can cause major problems when you double or triple the size of your cluster (or more). </p><p> Important items to consider: </p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem"><p>Switching capacity of the device</p></li><li class="listitem"><p>Number of systems connected</p></li><li class="listitem"><p>Uplink capacity</p></li></ul></div><p>
    </p><div class="section" title="1.2.1.&nbsp;Single Switch"><div class="titlepage"><div><div><h3 class="title"><a name="perf.network.1switch"></a>1.2.1.&nbsp;Single Switch</h3></div></div></div><p>The single most important factor in this configuration is that the switching capacity of
        the hardware is capable of handling the traffic which can be generated by all systems
        connected to the switch. Some lower priced commodity hardware can have a slower switching
        capacity than could be utilized by a full switch. </p></div><div class="section" title="1.2.2.&nbsp;Multiple Switches"><div class="titlepage"><div><div><h3 class="title"><a name="perf.network.2switch"></a>1.2.2.&nbsp;Multiple Switches</h3></div></div></div><p>Multiple switches are a potential pitfall in the architecture. The most common
        configuration of lower priced hardware is a simple 1Gbps uplink from one switch to another.
        This often overlooked pinch point can easily become a bottleneck for cluster communication.
        Especially with MapReduce jobs that are both reading and writing a lot of data the
        communication across this uplink could be saturated. </p><p>Mitigation of this issue is fairly simple and can be accomplished in multiple ways: </p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem"><p>Use appropriate hardware for the scale of the cluster which you're attempting to
            build.</p></li><li class="listitem"><p>Use larger single switch configurations i.e. single 48 port as opposed to 2x 24
            port</p></li><li class="listitem"><p>Configure port trunking for uplinks to utilize multiple interfaces to increase cross
            switch bandwidth.</p></li></ul></div></div><div class="section" title="1.2.3.&nbsp;Multiple Racks"><div class="titlepage"><div><div><h3 class="title"><a name="perf.network.multirack"></a>1.2.3.&nbsp;Multiple Racks</h3></div></div></div><p>Multiple rack configurations carry the same potential issues as multiple switches, and
        can suffer performance degradation from two main areas: </p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem"><p>Poor switch capacity performance</p></li><li class="listitem"><p>Insufficient uplink to another rack</p></li></ul></div><p>If the the switches in your rack have appropriate switching capacity to handle all the
        hosts at full speed, the next most likely issue will be caused by homing more of your
        cluster across racks. The easiest way to avoid issues when spanning multiple racks is to use
        port trunking to create a bonded uplink to other racks. The downside of this method however,
        is in the overhead of ports that could potentially be used. An example of this is, creating
        an 8Gbps port channel from rack A to rack B, using 8 of your 24 ports to communicate between
        racks gives you a poor ROI, using too few however can mean you're not getting the most out
        of your cluster. </p><p>Using 10Gbe links between racks will greatly increase performance, and assuming your
        switches support a 10Gbe uplink or allow for an expansion card will allow you to save your
        ports for machines as opposed to uplinks. </p></div><div class="section" title="1.2.4.&nbsp;Network Interfaces"><div class="titlepage"><div><div><h3 class="title"><a name="perf.network.ints"></a>1.2.4.&nbsp;Network Interfaces</h3></div></div></div><p>Are all the network interfaces functioning correctly? Are you sure? See the
        Troubleshooting Case Study in <a class="xref" href="#">???</a>. </p></div></div><div class="section" title="1.3.&nbsp;Java"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="jvm"></a>1.3.&nbsp;Java</h2></div></div></div><div class="section" title="1.3.1.&nbsp;The Garbage Collector and Apache HBase"><div class="titlepage"><div><div><h3 class="title"><a name="gc"></a>1.3.1.&nbsp;The Garbage Collector and Apache HBase</h3></div></div></div><div class="section" title="1.3.1.1.&nbsp;Long GC pauses"><div class="titlepage"><div><div><h4 class="title"><a name="gcpause"></a>1.3.1.1.&nbsp;Long GC pauses</h4></div></div></div><p><a name="mslab"></a>In his presentation, <a class="link" href="http://www.slideshare.net/cloudera/hbase-hug-presentation" target="_top">Avoiding Full GCs
            with MemStore-Local Allocation Buffers</a>, Todd Lipcon describes two cases of
          stop-the-world garbage collections common in HBase, especially during loading; CMS failure
          modes and old generation heap fragmentation brought. To address the first, start the CMS
          earlier than default by adding <code class="code">-XX:CMSInitiatingOccupancyFraction</code> and setting
          it down from defaults. Start at 60 or 70 percent (The lower you bring down the threshold,
          the more GCing is done, the more CPU used). To address the second fragmentation issue,
          Todd added an experimental facility, <a class="indexterm" name="d9542e104"></a>, that
          must be explicitly enabled in Apache HBase 0.90.x (Its defaulted to be on in Apache 0.92.x
          HBase). See <code class="code">hbase.hregion.memstore.mslab.enabled</code> to true in your
            <code class="classname">Configuration</code>. See the cited slides for background and detail.
          The latest jvms do better regards fragmentation so make sure you are running a recent
          release. Read down in the message, <a class="link" href="http://osdir.com/ml/hotspot-gc-use/2011-11/msg00002.html" target="_top">Identifying
            concurrent mode failures caused by fragmentation</a>. Be aware that when enabled,
          each MemStore instance will occupy at least an MSLAB instance of memory. If you have
          thousands of regions or lots of regions each with many column families, this allocation of
          MSLAB may be responsible for a good portion of your heap allocation and in an extreme case
          cause you to OOME. Disable MSLAB in this case, or lower the amount of memory it uses or
          float less regions per server. </p><p>If you have a write-heavy workload, check out <a class="link" href="https://issues.apache.org/jira/browse/HBASE-8163" target="_top">HBASE-8163
            MemStoreChunkPool: An improvement for JAVA GC when using MSLAB</a>. It describes
          configurations to lower the amount of young GC during write-heavy loadings. If you do not
          have HBASE-8163 installed, and you are trying to improve your young GC times, one trick to
          consider -- courtesy of our Liang Xie -- is to set the GC config
            <code class="varname">-XX:PretenureSizeThreshold</code> in <code class="filename">hbase-env.sh</code> to be
          just smaller than the size of <code class="varname">hbase.hregion.memstore.mslab.chunksize</code> so
          MSLAB allocations happen in the tenured space directly rather than first in the young gen.
          You'd do this because these MSLAB allocations are going to likely make it to the old gen
          anyways and rather than pay the price of a copies between s0 and s1 in eden space followed
          by the copy up from young to old gen after the MSLABs have achieved sufficient tenure,
          save a bit of YGC churn and allocate in the old gen directly. </p><p>For more information about GC logs, see <a class="xref" href="#">???</a>. </p><p>Consider also enabling the offheap Block Cache.  This has been shown to mitigate
        GC pause times.  See <a class="xref" href="#">???</a></p></div></div></div><div class="section" title="1.4.&nbsp;HBase Configurations"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="perf.configurations"></a>1.4.&nbsp;HBase Configurations</h2></div></div></div><p>See <a class="xref" href="#">???</a>.</p><div class="section" title="1.4.1.&nbsp;Managing Compactions"><div class="titlepage"><div><div><h3 class="title"><a name="perf.compactions.and.splits"></a>1.4.1.&nbsp;Managing Compactions</h3></div></div></div><p>For larger systems, managing <a class="link" href="#">compactions and splits</a> may be
      something you want to consider.</p></div><div class="section" title="1.4.2.&nbsp;hbase.regionserver.handler.count"><div class="titlepage"><div><div><h3 class="title"><a name="perf.handlers"></a>1.4.2.&nbsp;<code class="varname">hbase.regionserver.handler.count</code></h3></div></div></div><p>See <a class="xref" href="#">???</a>.
	    </p></div><div class="section" title="1.4.3.&nbsp;hfile.block.cache.size"><div class="titlepage"><div><div><h3 class="title"><a name="perf.hfile.block.cache.size"></a>1.4.3.&nbsp;<code class="varname">hfile.block.cache.size</code></h3></div></div></div><p>See <a class="xref" href="#">???</a>.
        A memory setting for the RegionServer process.
        </p></div><div class="section" title="1.4.4.&nbsp;Prefetch Option for Blockcache"><div class="titlepage"><div><div><h3 class="title"><a name="blockcache.prefetch"></a>1.4.4.&nbsp;Prefetch Option for Blockcache</h3></div></div></div><p><a class="link" href="https://issues.apache.org/jira/browse/HBASE-9857" target="_top">HBASE-9857</a>
        adds a new option to prefetch HFile contents when opening the blockcache, if a columnfamily
        or regionserver property is set. This option is available for HBase 0.98.3 and later. The
        purpose is to warm the blockcache as rapidly as possible after the cache is opened, using
        in-memory table data, and not counting the prefetching as cache misses. This is great for
        fast reads, but is not a good idea if the data to be preloaded will not fit into the
        blockcache. It is useful for tuning the IO impact of prefetching versus the time before all
        data blocks are in cache. </p><p>To enable prefetching on a given column family, you can use HBase Shell or use the
        API.</p><div class="example"><a name="d9542e178"></a><p class="title"><b>Example&nbsp;1.1.&nbsp;Enable Prefetch Using HBase Shell</b></p><div class="example-contents"><pre class="screen">hbase&gt; create 'MyTable', { NAME =&gt; 'myCF', PREFETCH_BLOCKS_ON_OPEN =&gt; 'true' }</pre></div></div><br class="example-break"><div class="example"><a name="d9542e183"></a><p class="title"><b>Example&nbsp;1.2.&nbsp;Enable Prefetch Using the API</b></p><div class="example-contents"><pre class="programlisting">
// ...
HTableDescriptor tableDesc = new HTableDescriptor("myTable");
HColumnDescriptor cfDesc = new HColumnDescriptor("myCF");
cfDesc.setPrefetchBlocksOnOpen(true);
tableDesc.addFamily(cfDesc);
// ...        
        </pre></div></div><br class="example-break"><p>See the API documentation for <a class="link" href="https://hbase.apache.org/apidocs/org/apache/hadoop/hbase/io/hfile/CacheConfig.html" target="_top">CacheConfig</a>.</p></div><div class="section" title="1.4.5.&nbsp;hbase.regionserver.global.memstore.size"><div class="titlepage"><div><div><h3 class="title"><a name="perf.rs.memstore.size"></a>1.4.5.&nbsp;<code class="varname">hbase.regionserver.global.memstore.size</code></h3></div></div></div><p>See <a class="xref" href="#">???</a>.
        This memory setting is often adjusted for the RegionServer process depending on needs.
        </p></div><div class="section" title="1.4.6.&nbsp;hbase.regionserver.global.memstore.size.lower.limit"><div class="titlepage"><div><div><h3 class="title"><a name="perf.rs.memstore.size.lower.limit"></a>1.4.6.&nbsp;<code class="varname">hbase.regionserver.global.memstore.size.lower.limit</code></h3></div></div></div><p>See <a class="xref" href="#">???</a>.
        This memory setting is often adjusted for the RegionServer process depending on needs.
        </p></div><div class="section" title="1.4.7.&nbsp;hbase.hstore.blockingStoreFiles"><div class="titlepage"><div><div><h3 class="title"><a name="perf.hstore.blockingstorefiles"></a>1.4.7.&nbsp;<code class="varname">hbase.hstore.blockingStoreFiles</code></h3></div></div></div><p>See <a class="xref" href="#">???</a>.
        If there is blocking in the RegionServer logs, increasing this can help.
        </p></div><div class="section" title="1.4.8.&nbsp;hbase.hregion.memstore.block.multiplier"><div class="titlepage"><div><div><h3 class="title"><a name="perf.hregion.memstore.block.multiplier"></a>1.4.8.&nbsp;<code class="varname">hbase.hregion.memstore.block.multiplier</code></h3></div></div></div><p>See <a class="xref" href="#">???</a>.
        If there is enough RAM, increasing this can help.
        </p></div><div class="section" title="1.4.9.&nbsp;hbase.regionserver.checksum.verify"><div class="titlepage"><div><div><h3 class="title"><a name="hbase.regionserver.checksum.verify"></a>1.4.9.&nbsp;<code class="varname">hbase.regionserver.checksum.verify</code></h3></div></div></div><p>Have HBase write the checksum into the datablock and save
        having to do the checksum seek whenever you read.</p><p>See <a class="xref" href="#hbase.regionserver.checksum.verify" title="1.4.9.&nbsp;hbase.regionserver.checksum.verify">Section&nbsp;1.4.9, &#8220;<code class="varname">hbase.regionserver.checksum.verify</code>&#8221;</a>,
        <a class="xref" href="#">???</a> and <a class="xref" href="#">???</a>
        For more information see the
        release note on <a class="link" href="https://issues.apache.org/jira/browse/HBASE-5074" target="_top">HBASE-5074 support checksums in HBase block cache</a>.
        </p></div><div class="section" title="1.4.10.&nbsp;Tuning callQueue Options"><div class="titlepage"><div><div><h3 class="title"><a name="d9542e242"></a>1.4.10.&nbsp;Tuning <code class="code">callQueue</code> Options</h3></div></div></div><p><a class="link" href="https://issues.apache.org/jira/browse/HBASE-11355" target="_top">HBASE-11355</a>
        introduces several callQueue tuning mechanisms which can increase performance. See the JIRA
        for some benchmarking information.</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem"><p>To increase the number of callqueues, set
              <code class="option">hbase.ipc.server.num.callqueue</code> to a value greater than
              <code class="literal">1</code>.</p></li><li class="listitem"><p>To split the callqueue into separate read and write queues, set
              <code class="code">hbase.ipc.server.callqueue.read.ratio</code> to a value between
              <code class="literal">0</code> and <code class="literal">1</code>. This factor weights the queues toward
            writes (if below .5) or reads (if above .5). Another way to say this is that the factor
            determines what percentage of the split queues are used for reads. The following
            examples illustrate some of the possibilities. Note that you always have at least one
            write queue, no matter what setting you use.</p><div class="itemizedlist"><ul class="itemizedlist" type="circle"><li class="listitem"><p>The default value of <code class="literal">0</code> does not split the queue.</p></li><li class="listitem"><p>A value of <code class="literal">.3</code> uses 30% of the queues for reading and 60% for
                writing. Given a value of <code class="literal">10</code> for
                  <code class="option">hbase.ipc.server.num.callqueue</code>, 3 queues would be used for reads
                and 7 for writes.</p></li><li class="listitem"><p>A value of <code class="literal">.5</code> uses the same number of read queues and write
                queues. Given a value of <code class="literal">10</code> for
                <code class="option">hbase.ipc.server.num.callqueue</code>, 5 queues would be used for reads
                and 5 for writes.</p></li><li class="listitem"><p>A value of <code class="literal">.6</code> uses 60% of the queues for reading and 30% for
                reading. Given a value of <code class="literal">10</code> for
                <code class="option">hbase.ipc.server.num.callqueue</code>, 7 queues would be used for reads
                and 3 for writes.</p></li><li class="listitem"><p>A value of <code class="literal">1.0</code> uses one queue to process write requests, and
                all other queues process read requests. A value higher than <code class="literal">1.0</code>
                has the same effect as a value of <code class="literal">1.0</code>. Given a value of
                  <code class="literal">10</code> for <code class="option">hbase.ipc.server.num.callqueue</code>, 9
                queues would be used for reads and 1 for writes.</p></li></ul></div></li><li class="listitem"><p>You can also split the read queues so that separate queues are used for short reads
            (from Get operations) and long reads (from Scan operations), by setting the
              <code class="option">hbase.ipc.server.callqueue.scan.ratio</code> option. This option is a factor
            between 0 and 1, which determine the ratio of read queues used for Gets and Scans. More
            queues are used for Gets if the value is below <code class="literal">.5</code> and more are used
            for scans if the value is above <code class="literal">.5</code>. No matter what setting you use,
            at least one read queue is used for Get operations.</p><div class="itemizedlist"><ul class="itemizedlist" type="circle"><li class="listitem"><p>A value of <code class="literal">0</code> does not split the read queue.</p></li><li class="listitem"><p>A value of <code class="literal">.3</code> uses 60% of the read queues for Gets and 30%
                for Scans. Given a value of <code class="literal">20</code> for
                  <code class="option">hbase.ipc.server.num.callqueue</code> and a value of <code class="literal">.5
                </code> for <code class="option">hbase.ipc.server.callqueue.read.ratio</code>, 10 queues
                would be used for reads, out of those 10, 7 would be used for Gets and 3 for
                Scans.</p></li><li class="listitem"><p>A value of <code class="literal">.5</code> uses half the read queues for Gets and half for
                Scans. Given a value of <code class="literal">20</code> for
                  <code class="option">hbase.ipc.server.num.callqueue</code> and a value of <code class="literal">.5
                </code> for <code class="option">hbase.ipc.server.callqueue.read.ratio</code>, 10 queues
                would be used for reads, out of those 10, 5 would be used for Gets and 5 for
                Scans.</p></li><li class="listitem"><p>A value of <code class="literal">.6</code> uses 30% of the read queues for Gets and 60%
                for Scans. Given a value of <code class="literal">20</code> for
                  <code class="option">hbase.ipc.server.num.callqueue</code> and a value of <code class="literal">.5
                </code> for <code class="option">hbase.ipc.server.callqueue.read.ratio</code>, 10 queues
                would be used for reads, out of those 10, 3 would be used for Gets and 7 for
                Scans.</p></li><li class="listitem"><p>A value of <code class="literal">1.0</code> uses all but one of the read queues for Scans.
                Given a value of <code class="literal">20</code> for
                  <code class="option">hbase.ipc.server.num.callqueue</code> and a value of <code class="literal">.5
                </code> for <code class="option">hbase.ipc.server.callqueue.read.ratio</code>, 10 queues
                would be used for reads, out of those 10, 1 would be used for Gets and 9 for
                Scans.</p></li></ul></div></li><li class="listitem"><p>You can use the new option
              <code class="option">hbase.ipc.server.callqueue.handler.factor</code> to programmatically tune
            the number of queues:</p><div class="itemizedlist"><ul class="itemizedlist" type="circle"><li class="listitem"><p>A value of <code class="literal">0</code> uses a single shared queue between all the
                handlers.</p></li><li class="listitem"><p>A value of <code class="literal">1</code> uses a separate queue for each handler.</p></li><li class="listitem"><p>A value between <code class="literal">0</code> and <code class="literal">1</code> tunes the number
                of queues against the number of handlers. For instance, a value of
                  <code class="literal">.5</code> shares one queue between each two handlers.</p></li></ul></div><p>Having more queues, such as in a situation where you have one queue per handler,
            reduces contention when adding a task to a queue or selecting it from a queue. The
            trade-off is that if you have some queues with long-running tasks, a handler may end up
            waiting to execute from that queue rather than processing another queue which has
            waiting tasks.</p></li></ul></div><p>For these values to take effect on a given Region Server, the Region Server must be
        restarted. These parameters are intended for testing purposes and should be used
        carefully.</p></div></div><div class="section" title="1.5.&nbsp;ZooKeeper"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="perf.zookeeper"></a>1.5.&nbsp;ZooKeeper</h2></div></div></div><p>See <a class="xref" href="#">???</a> for information on configuring ZooKeeper, and see the part about
      having a dedicated disk. </p></div><div class="section" title="1.6.&nbsp;Schema Design"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="perf.schema"></a>1.6.&nbsp;Schema Design</h2></div></div></div><div class="section" title="1.6.1.&nbsp;Number of Column Families"><div class="titlepage"><div><div><h3 class="title"><a name="perf.number.of.cfs"></a>1.6.1.&nbsp;Number of Column Families</h3></div></div></div><p>See <a class="xref" href="#">???</a>.</p></div><div class="section" title="1.6.2.&nbsp;Key and Attribute Lengths"><div class="titlepage"><div><div><h3 class="title"><a name="perf.schema.keys"></a>1.6.2.&nbsp;Key and Attribute Lengths</h3></div></div></div><p>See <a class="xref" href="#">???</a>. See also <a class="xref" href="#perf.compression.however" title="1.6.7.1.&nbsp;However...">Section&nbsp;1.6.7.1, &#8220;However...&#8221;</a> for compression caveats.</p></div><div class="section" title="1.6.3.&nbsp;Table RegionSize"><div class="titlepage"><div><div><h3 class="title"><a name="schema.regionsize"></a>1.6.3.&nbsp;Table RegionSize</h3></div></div></div><p>The regionsize can be set on a per-table basis via <code class="code">setFileSize</code> on <a class="link" href="http://hbase.apache.org/apidocs/org/apache/hadoop/hbase/HTableDescriptor.html" target="_top">HTableDescriptor</a>
        in the event where certain tables require different regionsizes than the configured default
        regionsize. </p><p>See <a class="xref" href="#">???</a> for more information. </p></div><div class="section" title="1.6.4.&nbsp;Bloom Filters"><div class="titlepage"><div><div><h3 class="title"><a name="schema.bloom"></a>1.6.4.&nbsp;Bloom Filters</h3></div></div></div><p>A Bloom filter, named for its creator, Burton Howard Bloom, is a data structure which is
        designed to predict whether a given element is a member of a set of data. A positive result
        from a Bloom filter is not always accurate, but a negative result is guaranteed to be
        accurate. Bloom filters are designed to be "accurate enough" for sets of data which are so
        large that conventional hashing mechanisms would be impractical. For more information about
        Bloom filters in general, refer to <a class="link" href="http://en.wikipedia.org/wiki/Bloom_filter" target="_top">http://en.wikipedia.org/wiki/Bloom_filter</a>.</p><p>In terms of HBase, Bloom filters provide a lightweight in-memory structure to reduce the
        number of disk reads for a given Get operation (Bloom filters do not work with Scans) to only the StoreFiles likely to
        contain the desired Row. The potential performance gain increases with the number of
        parallel reads. </p><p>The Bloom filters themselves are stored in the metadata of each HFile and never need to
        be updated. When an HFile is opened because a region is deployed to a RegionServer, the
        Bloom filter is loaded into memory. </p><p>HBase includes some tuning mechanisms for folding the Bloom filter to reduce the size
        and keep the false positive rate within a desired range.</p><p>Bloom filters were introduced in <a class="link" href="https://issues.apache.org/jira/browse/HBASE-1200" target="_top">HBASE-1200</a>. Since
        HBase 0.96, row-based Bloom filters are enabled by default. (<a class="link" href="https://issues.apache.org/jira/browse/HBASE-8450" target="_top">HBASE-</a>)</p><p>For more information on Bloom filters in relation to HBase, see <a class="xref" href="#blooms" title="1.9.9.&nbsp;Bloom Filters">Section&nbsp;1.9.9, &#8220;Bloom Filters&#8221;</a> for more information, or the following Quora discussion: <a class="link" href="http://www.quora.com/How-are-bloom-filters-used-in-HBase" target="_top">How are bloom
          filters used in HBase?</a>. </p><div class="section" title="1.6.4.1.&nbsp;When To Use Bloom Filters"><div class="titlepage"><div><div><h4 class="title"><a name="bloom.filters.when"></a>1.6.4.1.&nbsp;When To Use Bloom Filters</h4></div></div></div><p>Since HBase 0.96, row-based Bloom filters are enabled by default. You may choose to
          disable them or to change some tables to use row+column Bloom filters, depending on the
          characteristics of your data and how it is loaded into HBase.</p><p>To determine whether Bloom filters could have a positive impact, check the value of
          <code class="code">blockCacheHitRatio</code> in the RegionServer metrics. If Bloom filters are enabled, the value of
          <code class="code">blockCacheHitRatio</code> should increase, because the Bloom filter is filtering out blocks that
          are definitely not needed. </p><p>You can choose to enable Bloom filters for a row or for a row+column combination. If
          you generally scan entire rows, the row+column combination will not provide any benefit. A
          row-based Bloom filter can operate on a row+column Get, but not the other way around.
          However, if you have a large number of column-level Puts, such that a row may be present
          in every StoreFile, a row-based filter will always return a positive result and provide no
          benefit. Unless you have one column per row, row+column Bloom filters require more space,
          in order to store more keys. Bloom filters work best when the size of each data entry is
          at least a few kilobytes in size. </p><p>Overhead will be reduced when your data is stored in a few larger StoreFiles, to avoid
          extra disk IO during low-level scans to find a specific row. </p><p>Bloom filters need to be rebuilt upon deletion, so may not be appropriate in
          environments with a large number of deletions.</p></div><div class="section" title="1.6.4.2.&nbsp;Enabling Bloom Filters"><div class="titlepage"><div><div><h4 class="title"><a name="d9542e549"></a>1.6.4.2.&nbsp;Enabling Bloom Filters</h4></div></div></div><p>Bloom filters are enabled on a Column Family. You can do this by using the
          setBloomFilterType method of HColumnDescriptor or using the HBase API. Valid values are
            <code class="literal">NONE</code> (the default), <code class="literal">ROW</code>, or
            <code class="literal">ROWCOL</code>. See <a class="xref" href="#bloom.filters.when" title="1.6.4.1.&nbsp;When To Use Bloom Filters">Section&nbsp;1.6.4.1, &#8220;When To Use Bloom Filters&#8221;</a> for more information on <code class="literal">ROW</code> versus
            <code class="literal">ROWCOL</code>. See also the API documentation for <a class="link" href="http://hbase.apache.org/apidocs/org/apache/hadoop/hbase/HColumnDescriptor.html" target="_top">HColumnDescriptor</a>.</p><p>The following example creates a table and enables a ROWCOL Bloom filter on the
            <code class="literal">colfam1</code> column family.</p><pre class="screen">
hbase&gt; <strong class="userinput"><code>create 'mytable',{NAME =&gt; 'colfam1', BLOOMFILTER =&gt; 'ROWCOL'}</code></strong>          
        </pre></div><div class="section" title="1.6.4.3.&nbsp;Configuring Server-Wide Behavior of Bloom Filters"><div class="titlepage"><div><div><h4 class="title"><a name="d9542e584"></a>1.6.4.3.&nbsp;Configuring Server-Wide Behavior of Bloom Filters</h4></div></div></div><p>You can configure the following settings in the <code class="filename">hbase-site.xml</code>.
        </p><div class="informaltable"><table border="1"><colgroup><col><col><col></colgroup><thead><tr><th>Parameter</th><th>Default</th><th>Description</th></tr></thead><tbody><tr><td><p><code class="code">io.hfile.bloom.enabled</code></p></td><td><p><code class="literal">yes</code></p></td><td><p>Set to <code class="literal">no</code> to kill bloom filters server-wide if
                    something goes wrong</p></td></tr><tr><td><p><code class="code">io.hfile.bloom.error.rate</code></p></td><td><p><code class="literal">.01</code></p></td><td><p>The average false positive rate for bloom filters. Folding is used to
                  maintain the false positive rate. Expressed as a decimal representation of a
                  percentage.</p></td></tr><tr><td><p><code class="code">io.hfile.bloom.max.fold</code></p></td><td><p><code class="literal">7</code></p></td><td><p>The guaranteed maximum fold rate. Changing this setting should not be
                  necessary and is not recommended.</p></td></tr><tr><td><p><code class="code">io.storefile.bloom.max.keys</code></p></td><td><p><code class="literal">128000000</code></p></td><td><p>For default (single-block) Bloom filters, this specifies the maximum
                    number of keys.</p></td></tr><tr><td><p><code class="code">io.storefile.delete.family.bloom.enabled</code></p></td><td><p><code class="literal">true</code></p></td><td><p>Master switch to enable Delete Family Bloom filters and store them in
                  the StoreFile.</p></td></tr><tr><td><p><code class="code">io.storefile.bloom.block.size</code></p></td><td><p><code class="literal">65536</code></p></td><td><p>Target Bloom block size. Bloom filter blocks of approximately this size
                    are interleaved with data blocks.</p></td></tr><tr><td><p><code class="code">hfile.block.bloom.cacheonwrite</code></p></td><td><p><code class="literal">false</code></p></td><td><p>Enables cache-on-write for inline blocks of a compound Bloom filter.</p></td></tr></tbody></table></div></div></div><div class="section" title="1.6.5.&nbsp;ColumnFamily BlockSize"><div class="titlepage"><div><div><h3 class="title"><a name="schema.cf.blocksize"></a>1.6.5.&nbsp;ColumnFamily BlockSize</h3></div></div></div><p>The blocksize can be configured for each ColumnFamily in a table, and this defaults to
        64k. Larger cell values require larger blocksizes. There is an inverse relationship between
        blocksize and the resulting StoreFile indexes (i.e., if the blocksize is doubled then the
        resulting indexes should be roughly halved). </p><p>See <a class="link" href="http://hbase.apache.org/apidocs/org/apache/hadoop/hbase/HColumnDescriptor.html" target="_top">HColumnDescriptor</a>
        and <a class="xref" href="#">???</a>for more information. </p></div><div class="section" title="1.6.6.&nbsp;In-Memory ColumnFamilies"><div class="titlepage"><div><div><h3 class="title"><a name="cf.in.memory"></a>1.6.6.&nbsp;In-Memory ColumnFamilies</h3></div></div></div><p>ColumnFamilies can optionally be defined as in-memory. Data is still persisted to disk,
        just like any other ColumnFamily. In-memory blocks have the highest priority in the <a class="xref" href="#">???</a>, but it is not a guarantee that the entire table will be in
        memory. </p><p>See <a class="link" href="http://hbase.apache.org/apidocs/org/apache/hadoop/hbase/HColumnDescriptor.html" target="_top">HColumnDescriptor</a>
        for more information. </p></div><div class="section" title="1.6.7.&nbsp;Compression"><div class="titlepage"><div><div><h3 class="title"><a name="perf.compression"></a>1.6.7.&nbsp;Compression</h3></div></div></div><p>Production systems should use compression with their ColumnFamily definitions. See <a class="xref" href="#">???</a> for more information. </p><div class="section" title="1.6.7.1.&nbsp;However..."><div class="titlepage"><div><div><h4 class="title"><a name="perf.compression.however"></a>1.6.7.1.&nbsp;However...</h4></div></div></div><p>Compression deflates data <span class="emphasis"><em>on disk</em></span>. When it's in-memory (e.g., in
          the MemStore) or on the wire (e.g., transferring between RegionServer and Client) it's
          inflated. So while using ColumnFamily compression is a best practice, but it's not going
          to completely eliminate the impact of over-sized Keys, over-sized ColumnFamily names, or
          over-sized Column names. </p><p>See <a class="xref" href="#">???</a> on for schema design tips, and <a class="xref" href="#">???</a> for more information on HBase stores data internally. </p></div></div></div><div class="section" title="1.7.&nbsp;HBase General Patterns"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="perf.general"></a>1.7.&nbsp;HBase General Patterns</h2></div></div></div><div class="section" title="1.7.1.&nbsp;Constants"><div class="titlepage"><div><div><h3 class="title"><a name="perf.general.constants"></a>1.7.1.&nbsp;Constants</h3></div></div></div><p>When people get started with HBase they have a tendency to write code that looks like
        this:</p><pre class="programlisting">
Get get = new Get(rowkey);
Result r = htable.get(get);
byte[] b = r.getValue(Bytes.toBytes("cf"), Bytes.toBytes("attr"));  // returns current version of value
      </pre><p>But especially when inside loops (and MapReduce jobs), converting the columnFamily and
        column-names to byte-arrays repeatedly is surprisingly expensive. It's better to use
        constants for the byte-arrays, like this:</p><pre class="programlisting">
public static final byte[] CF = "cf".getBytes();
public static final byte[] ATTR = "attr".getBytes();
...
Get get = new Get(rowkey);
Result r = htable.get(get);
byte[] b = r.getValue(CF, ATTR);  // returns current version of value
      </pre></div></div><div class="section" title="1.8.&nbsp;Writing to HBase"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="perf.writing"></a>1.8.&nbsp;Writing to HBase</h2></div></div></div><div class="section" title="1.8.1.&nbsp;Batch Loading"><div class="titlepage"><div><div><h3 class="title"><a name="perf.batch.loading"></a>1.8.1.&nbsp;Batch Loading</h3></div></div></div><p>Use the bulk load tool if you can. See <a class="xref" href="#">???</a>. Otherwise, pay attention to the below. </p></div><div class="section" title="1.8.2.&nbsp; Table Creation: Pre-Creating Regions"><div class="titlepage"><div><div><h3 class="title"><a name="precreate.regions"></a>1.8.2.&nbsp; Table Creation: Pre-Creating Regions </h3></div></div></div><p> Tables in HBase are initially created with one region by default. For bulk imports,
        this means that all clients will write to the same region until it is large enough to split
        and become distributed across the cluster. A useful pattern to speed up the bulk import
        process is to pre-create empty regions. Be somewhat conservative in this, because too-many
        regions can actually degrade performance. </p><p>There are two different approaches to pre-creating splits. The first approach is to rely
        on the default <code class="code">HBaseAdmin</code> strategy (which is implemented in
          <code class="code">Bytes.split</code>)... </p><pre class="programlisting">
byte[] startKey = ...;   	// your lowest key
byte[] endKey = ...;   		// your highest key
int numberOfRegions = ...;	// # of regions to create
admin.createTable(table, startKey, endKey, numberOfRegions);
      </pre><p>And the other approach is to define the splits yourself... </p><pre class="programlisting">
byte[][] splits = ...;   // create your own splits
admin.createTable(table, splits);
</pre><p> See <a class="xref" href="#">???</a> for issues related to understanding your
        keyspace and pre-creating regions. See <a class="xref" href="#">???</a>
        for discussion on manually pre-splitting regions.</p></div><div class="section" title="1.8.3.&nbsp; Table Creation: Deferred Log Flush"><div class="titlepage"><div><div><h3 class="title"><a name="def.log.flush"></a>1.8.3.&nbsp; Table Creation: Deferred Log Flush </h3></div></div></div><p> The default behavior for Puts using the Write Ahead Log (WAL) is that
          <code class="classname">HLog</code> edits will be written immediately. If deferred log flush is
        used, WAL edits are kept in memory until the flush period. The benefit is aggregated and
        asynchronous <code class="classname">HLog</code>- writes, but the potential downside is that if the
        RegionServer goes down the yet-to-be-flushed edits are lost. This is safer, however, than
        not using WAL at all with Puts. </p><p> Deferred log flush can be configured on tables via <a class="link" href="http://hbase.apache.org/apidocs/org/apache/hadoop/hbase/HTableDescriptor.html" target="_top">HTableDescriptor</a>.
        The default value of <code class="varname">hbase.regionserver.optionallogflushinterval</code> is
        1000ms. </p></div><div class="section" title="1.8.4.&nbsp;HBase Client: AutoFlush"><div class="titlepage"><div><div><h3 class="title"><a name="perf.hbase.client.autoflush"></a>1.8.4.&nbsp;HBase Client: AutoFlush</h3></div></div></div><p>When performing a lot of Puts, make sure that setAutoFlush is set to false on your <a class="link" href="http://hbase.apache.org/apidocs/org/apache/hadoop/hbase/client/HTable.html" target="_top">HTable</a>
        instance. Otherwise, the Puts will be sent one at a time to the RegionServer. Puts added via
          <code class="code"> htable.add(Put)</code> and <code class="code"> htable.add( &lt;List&gt; Put)</code> wind up in
        the same write buffer. If <code class="code">autoFlush = false</code>, these messages are not sent until
        the write-buffer is filled. To explicitly flush the messages, call
          <code class="methodname">flushCommits</code>. Calling <code class="methodname">close</code> on the
          <code class="classname">HTable</code> instance will invoke
        <code class="methodname">flushCommits</code>.</p></div><div class="section" title="1.8.5.&nbsp;HBase Client: Turn off WAL on Puts"><div class="titlepage"><div><div><h3 class="title"><a name="perf.hbase.client.putwal"></a>1.8.5.&nbsp;HBase Client: Turn off WAL on Puts</h3></div></div></div><p>A frequent request is to disable the WAL to increase performance of Puts. This is only
        appropriate for bulk loads, as it puts your data at risk by removing the protection of the
        WAL in the event of a region server crash. Bulk loads can be re-run in the event of a crash,
        with little risk of data loss.</p><div class="warning" title="Warning" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title">Warning</h3><p>If you disable the WAL for anything other than bulk loads, your data is at
          risk.</p></div><p>In general, it is best to use WAL for Puts, and where loading throughput is a concern to
        use <a class="link" href="#perf.batch.loading" title="1.8.1.&nbsp;Batch Loading">bulk loading</a> techniques instead. For normal
        Puts, you are not likely to see a performance improvement which would outweigh the risk. To
        disable the WAL, see <a class="xref" href="#">???</a>.</p></div><div class="section" title="1.8.6.&nbsp;HBase Client: Group Puts by RegionServer"><div class="titlepage"><div><div><h3 class="title"><a name="perf.hbase.client.regiongroup"></a>1.8.6.&nbsp;HBase Client: Group Puts by RegionServer</h3></div></div></div><p>In addition to using the writeBuffer, grouping <code class="classname">Put</code>s by
        RegionServer can reduce the number of client RPC calls per writeBuffer flush. There is a
        utility <code class="classname">HTableUtil</code> currently on TRUNK that does this, but you can
        either copy that or implement your own version for those still on 0.90.x or earlier. </p></div><div class="section" title="1.8.7.&nbsp;MapReduce: Skip The Reducer"><div class="titlepage"><div><div><h3 class="title"><a name="perf.hbase.write.mr.reducer"></a>1.8.7.&nbsp;MapReduce: Skip The Reducer</h3></div></div></div><p>When writing a lot of data to an HBase table from a MR job (e.g., with <a class="link" href="http://hbase.apache.org/apidocs/org/apache/hadoop/hbase/mapreduce/TableOutputFormat.html" target="_top">TableOutputFormat</a>),
        and specifically where Puts are being emitted from the Mapper, skip the Reducer step. When a
        Reducer step is used, all of the output (Puts) from the Mapper will get spooled to disk,
        then sorted/shuffled to other Reducers that will most likely be off-node. It's far more
        efficient to just write directly to HBase. </p><p>For summary jobs where HBase is used as a source and a sink, then writes will be coming
        from the Reducer step (e.g., summarize values then write out result). This is a different
        processing problem than from the the above case. </p></div><div class="section" title="1.8.8.&nbsp;Anti-Pattern: One Hot Region"><div class="titlepage"><div><div><h3 class="title"><a name="perf.one.region"></a>1.8.8.&nbsp;Anti-Pattern: One Hot Region</h3></div></div></div><p>If all your data is being written to one region at a time, then re-read the section on
        processing <a class="link" href="#">timeseries</a> data.</p><p>Also, if you are pre-splitting regions and all your data is <span class="emphasis"><em>still</em></span>
        winding up in a single region even though your keys aren't monotonically increasing, confirm
        that your keyspace actually works with the split strategy. There are a variety of reasons
        that regions may appear "well split" but won't work with your data. As the HBase client
        communicates directly with the RegionServers, this can be obtained via <a class="link" href="http://hbase.apache.org/apidocs/org/apache/hadoop/hbase/client/HTable.html#getRegionLocation%28byte[]%29" target="_top">HTable.getRegionLocation</a>. </p><p>See <a class="xref" href="#precreate.regions" title="1.8.2.&nbsp; Table Creation: Pre-Creating Regions">Section&nbsp;1.8.2, &#8220; Table Creation: Pre-Creating Regions &#8221;</a>, as well as <a class="xref" href="#perf.configurations" title="1.4.&nbsp;HBase Configurations">Section&nbsp;1.4, &#8220;HBase Configurations&#8221;</a>
      </p></div></div><div class="section" title="1.9.&nbsp;Reading from HBase"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="perf.reading"></a>1.9.&nbsp;Reading from HBase</h2></div></div></div><p>The mailing list can help if you are having performance issues. For example, here is a
      good general thread on what to look at addressing read-time issues: <a class="link" href="http://search-hadoop.com/m/qOo2yyHtCC1" target="_top">HBase Random Read latency &gt;
      100ms</a></p><div class="section" title="1.9.1.&nbsp;Scan Caching"><div class="titlepage"><div><div><h3 class="title"><a name="perf.hbase.client.caching"></a>1.9.1.&nbsp;Scan Caching</h3></div></div></div><p>If HBase is used as an input source for a MapReduce job, for example, make sure that the
        input <a class="link" href="http://hbase.apache.org/apidocs/org/apache/hadoop/hbase/client/Scan.html" target="_top">Scan</a>
        instance to the MapReduce job has <code class="methodname">setCaching</code> set to something
        greater than the default (which is 1). Using the default value means that the map-task will
        make call back to the region-server for every record processed. Setting this value to 500,
        for example, will transfer 500 rows at a time to the client to be processed. There is a
        cost/benefit to have the cache value be large because it costs more in memory for both
        client and RegionServer, so bigger isn't always better.</p><div class="section" title="1.9.1.1.&nbsp;Scan Caching in MapReduce Jobs"><div class="titlepage"><div><div><h4 class="title"><a name="perf.hbase.client.caching.mr"></a>1.9.1.1.&nbsp;Scan Caching in MapReduce Jobs</h4></div></div></div><p>Scan settings in MapReduce jobs deserve special attention. Timeouts can result (e.g.,
          UnknownScannerException) in Map tasks if it takes longer to process a batch of records
          before the client goes back to the RegionServer for the next set of data. This problem can
          occur because there is non-trivial processing occuring per row. If you process rows
          quickly, set caching higher. If you process rows more slowly (e.g., lots of
          transformations per row, writes), then set caching lower. </p><p>Timeouts can also happen in a non-MapReduce use case (i.e., single threaded HBase
          client doing a Scan), but the processing that is often performed in MapReduce jobs tends
          to exacerbate this issue. </p></div></div><div class="section" title="1.9.2.&nbsp;Scan Attribute Selection"><div class="titlepage"><div><div><h3 class="title"><a name="perf.hbase.client.selection"></a>1.9.2.&nbsp;Scan Attribute Selection</h3></div></div></div><p>Whenever a Scan is used to process large numbers of rows (and especially when used as a
        MapReduce source), be aware of which attributes are selected. If <code class="code">scan.addFamily</code>
        is called then <span class="emphasis"><em>all</em></span> of the attributes in the specified ColumnFamily will
        be returned to the client. If only a small number of the available attributes are to be
        processed, then only those attributes should be specified in the input scan because
        attribute over-selection is a non-trivial performance penalty over large datasets. </p></div><div class="section" title="1.9.3.&nbsp;Avoid scan seeks"><div class="titlepage"><div><div><h3 class="title"><a name="perf.hbase.client.seek"></a>1.9.3.&nbsp;Avoid scan seeks</h3></div></div></div><p>When columns are selected explicitly with <code class="code">scan.addColumn</code>, HBase will
        schedule seek operations to seek between the selected columns. When rows have few columns
        and each column has only a few versions this can be inefficient. A seek operation is
        generally slower if does not seek at least past 5-10 columns/versions or 512-1024
        bytes.</p><p>In order to opportunistically look ahead a few columns/versions to see if the next
        column/version can be found that way before a seek operation is scheduled, a new attribute
          <code class="code">Scan.HINT_LOOKAHEAD</code> can be set the on Scan object. The following code
        instructs the RegionServer to attempt two iterations of next before a seek is
        scheduled:</p><pre class="programlisting">
Scan scan = new Scan();
scan.addColumn(...);
scan.setAttribute(Scan.HINT_LOOKAHEAD, Bytes.toBytes(2));
table.getScanner(scan);
      </pre></div><div class="section" title="1.9.4.&nbsp;MapReduce - Input Splits"><div class="titlepage"><div><div><h3 class="title"><a name="perf.hbase.mr.input"></a>1.9.4.&nbsp;MapReduce - Input Splits</h3></div></div></div><p>For MapReduce jobs that use HBase tables as a source, if there a pattern where the
        "slow" map tasks seem to have the same Input Split (i.e., the RegionServer serving the
        data), see the Troubleshooting Case Study in <a class="xref" href="#">???</a>. </p></div><div class="section" title="1.9.5.&nbsp;Close ResultScanners"><div class="titlepage"><div><div><h3 class="title"><a name="perf.hbase.client.scannerclose"></a>1.9.5.&nbsp;Close ResultScanners</h3></div></div></div><p>This isn't so much about improving performance but rather <span class="emphasis"><em>avoiding</em></span>
        performance problems. If you forget to close <a class="link" href="http://hbase.apache.org/apidocs/org/apache/hadoop/hbase/client/ResultScanner.html" target="_top">ResultScanners</a>
        you can cause problems on the RegionServers. Always have ResultScanner processing enclosed
        in try/catch blocks...</p><pre class="programlisting">
Scan scan = new Scan();
// set attrs...
ResultScanner rs = htable.getScanner(scan);
try {
  for (Result r = rs.next(); r != null; r = rs.next()) {
  // process result...
} finally {
  rs.close();  // always close the ResultScanner!
}
htable.close();
      </pre></div><div class="section" title="1.9.6.&nbsp;Block Cache"><div class="titlepage"><div><div><h3 class="title"><a name="perf.hbase.client.blockcache"></a>1.9.6.&nbsp;Block Cache</h3></div></div></div><p><a class="link" href="http://hbase.apache.org/apidocs/org/apache/hadoop/hbase/client/Scan.html" target="_top">Scan</a>
        instances can be set to use the block cache in the RegionServer via the
          <code class="methodname">setCacheBlocks</code> method. For input Scans to MapReduce jobs, this
        should be <code class="varname">false</code>. For frequently accessed rows, it is advisable to use the
        block cache.</p><p>Cache more data by moving your Block Cache offheap.  See <a class="xref" href="#">???</a></p></div><div class="section" title="1.9.7.&nbsp;Optimal Loading of Row Keys"><div class="titlepage"><div><div><h3 class="title"><a name="perf.hbase.client.rowkeyonly"></a>1.9.7.&nbsp;Optimal Loading of Row Keys</h3></div></div></div><p>When performing a table <a class="link" href="http://hbase.apache.org/apidocs/org/apache/hadoop/hbase/client/Scan.html" target="_top">scan</a>
        where only the row keys are needed (no families, qualifiers, values or timestamps), add a
        FilterList with a <code class="varname">MUST_PASS_ALL</code> operator to the scanner using
          <code class="methodname">setFilter</code>. The filter list should include both a <a class="link" href="http://hbase.apache.org/apidocs/org/apache/hadoop/hbase/filter/FirstKeyOnlyFilter.html" target="_top">FirstKeyOnlyFilter</a>
        and a <a class="link" href="http://hbase.apache.org/apidocs/org/apache/hadoop/hbase/filter/KeyOnlyFilter.html" target="_top">KeyOnlyFilter</a>.
        Using this filter combination will result in a worst case scenario of a RegionServer reading
        a single value from disk and minimal network traffic to the client for a single row. </p></div><div class="section" title="1.9.8.&nbsp;Concurrency: Monitor Data Spread"><div class="titlepage"><div><div><h3 class="title"><a name="perf.hbase.read.dist"></a>1.9.8.&nbsp;Concurrency: Monitor Data Spread</h3></div></div></div><p>When performing a high number of concurrent reads, monitor the data spread of the target
        tables. If the target table(s) have too few regions then the reads could likely be served
        from too few nodes. </p><p>See <a class="xref" href="#precreate.regions" title="1.8.2.&nbsp; Table Creation: Pre-Creating Regions">Section&nbsp;1.8.2, &#8220; Table Creation: Pre-Creating Regions &#8221;</a>, as well as <a class="xref" href="#perf.configurations" title="1.4.&nbsp;HBase Configurations">Section&nbsp;1.4, &#8220;HBase Configurations&#8221;</a>
      </p></div><div class="section" title="1.9.9.&nbsp;Bloom Filters"><div class="titlepage"><div><div><h3 class="title"><a name="blooms"></a>1.9.9.&nbsp;Bloom Filters</h3></div></div></div><p>Enabling Bloom Filters can save your having to go to disk and can help improve read
        latencies.</p><p><a class="link" href="http://en.wikipedia.org/wiki/Bloom_filter" target="_top">Bloom filters</a> were developed
        over in <a class="link" href="https://issues.apache.org/jira/browse/HBASE-1200" target="_top">HBase-1200 Add
          bloomfilters</a>. For description of the development process -- why static blooms rather than dynamic
            -- and for an overview of the unique properties that pertain to blooms in HBase, as well
            as possible future directions, see the <span class="emphasis"><em>Development Process</em></span> section
            of the document <a class="link" href="https://issues.apache.org/jira/secure/attachment/12444007/Bloom_Filters_in_HBase.pdf" target="_top">BloomFilters
              in HBase</a> attached to <a class="link" href="https://issues.apache.org/jira/browse/HBASE-1200" target="_top">HBase-1200</a>.  The bloom filters described here are actually version two of blooms in HBase. In
            versions up to 0.19.x, HBase had a dynamic bloom option based on work done by the <a class="link" href="http://www.one-lab.org" target="_top">European Commission One-Lab Project 034819</a>.
            The core of the HBase bloom work was later pulled up into Hadoop to implement
            org.apache.hadoop.io.BloomMapFile. Version 1 of HBase blooms never worked that well.
            Version 2 is a rewrite from scratch though again it starts with the one-lab work.</p><p>See also <a class="xref" href="#schema.bloom" title="1.6.4.&nbsp;Bloom Filters">Section&nbsp;1.6.4, &#8220;Bloom Filters&#8221;</a>. </p><div class="section" title="1.9.9.1.&nbsp;Bloom StoreFile footprint"><div class="titlepage"><div><div><h4 class="title"><a name="bloom_footprint"></a>1.9.9.1.&nbsp;Bloom StoreFile footprint</h4></div></div></div><p>Bloom filters add an entry to the <code class="classname">StoreFile</code> general
            <code class="classname">FileInfo</code> data structure and then two extra entries to the
            <code class="classname">StoreFile</code> metadata section.</p><div class="section" title="1.9.9.1.1.&nbsp;BloomFilter in the StoreFile FileInfo data structure"><div class="titlepage"><div><div><h5 class="title"><a name="d9542e1053"></a>1.9.9.1.1.&nbsp;BloomFilter in the <code class="classname">StoreFile</code>
            <code class="classname">FileInfo</code> data structure</h5></div></div></div><p><code class="classname">FileInfo</code> has a <code class="varname">BLOOM_FILTER_TYPE</code> entry
            which is set to <code class="varname">NONE</code>, <code class="varname">ROW</code> or
              <code class="varname">ROWCOL.</code></p></div><div class="section" title="1.9.9.1.2.&nbsp;BloomFilter entries in StoreFile metadata"><div class="titlepage"><div><div><h5 class="title"><a name="d9542e1077"></a>1.9.9.1.2.&nbsp;BloomFilter entries in <code class="classname">StoreFile</code> metadata</h5></div></div></div><p><code class="varname">BLOOM_FILTER_META</code> holds Bloom Size, Hash Function used, etc. Its
            small in size and is cached on <code class="classname">StoreFile.Reader</code> load</p><p><code class="varname">BLOOM_FILTER_DATA</code> is the actual bloomfilter data. Obtained
            on-demand. Stored in the LRU cache, if it is enabled (Its enabled by default).</p></div></div><div class="section" title="1.9.9.2.&nbsp;Bloom Filter Configuration"><div class="titlepage"><div><div><h4 class="title"><a name="config.bloom"></a>1.9.9.2.&nbsp;Bloom Filter Configuration</h4></div></div></div><div class="section" title="1.9.9.2.1.&nbsp;io.hfile.bloom.enabled global kill switch"><div class="titlepage"><div><div><h5 class="title"><a name="d9542e1097"></a>1.9.9.2.1.&nbsp;<code class="varname">io.hfile.bloom.enabled</code> global kill switch</h5></div></div></div><p><code class="code">io.hfile.bloom.enabled</code> in <code class="classname">Configuration</code> serves
            as the kill switch in case something goes wrong. Default =
            <code class="varname">true</code>.</p></div><div class="section" title="1.9.9.2.2.&nbsp;io.hfile.bloom.error.rate"><div class="titlepage"><div><div><h5 class="title"><a name="d9542e1112"></a>1.9.9.2.2.&nbsp;<code class="varname">io.hfile.bloom.error.rate</code></h5></div></div></div><p><code class="varname">io.hfile.bloom.error.rate</code> = average false positive rate. Default
            = 1%. Decrease rate by &frac12; (e.g. to .5%) == +1 bit per bloom entry.</p></div><div class="section" title="1.9.9.2.3.&nbsp;io.hfile.bloom.max.fold"><div class="titlepage"><div><div><h5 class="title"><a name="d9542e1120"></a>1.9.9.2.3.&nbsp;<code class="varname">io.hfile.bloom.max.fold</code></h5></div></div></div><p><code class="varname">io.hfile.bloom.max.fold</code> = guaranteed minimum fold rate. Most
            people should leave this alone. Default = 7, or can collapse to at least 1/128th of
            original size. See the <span class="emphasis"><em>Development Process</em></span> section of the document <a class="link" href="https://issues.apache.org/jira/secure/attachment/12444007/Bloom_Filters_in_HBase.pdf" target="_top">BloomFilters
              in HBase</a> for more on what this option means.</p></div></div></div><div class="section" title="1.9.10.&nbsp;Hedged Reads"><div class="titlepage"><div><div><h3 class="title"><a name="d9542e1135"></a>1.9.10.&nbsp;Hedged Reads</h3></div></div></div><p>Hedged reads are a feature of HDFS, introduced in <a class="link" href="https://issues.apache.org/jira/browse/HDFS-5776" target="_top">HDFS-5776</a>. Normally, a
        single thread is spawned for each read request. However, if hedged reads are enabled, the
        client waits some configurable amount of time, and if the read does not return, the client
        spawns a second read request, against a different block replica of the same data. Whichever
        read returns first is used, and the other read request is discarded. Hedged reads can be
        helpful for times where a rare slow read is caused by a transient error such as a failing
        disk or flaky network connection.</p><p> Because a HBase RegionServer is a HDFS client, you can enable hedged reads in HBase, by
        adding the following properties to the RegionServer's hbase-site.xml and tuning the values
        to suit your environment.</p><div class="itemizedlist" title="Configuration for Hedged Reads"><p class="title"><b>Configuration for Hedged Reads</b></p><ul class="itemizedlist" type="disc"><li class="listitem"><p><code class="code">dfs.client.hedged.read.threadpool.size</code> - the number of threads
            dedicated to servicing hedged reads. If this is set to 0 (the default), hedged reads are
            disabled.</p></li><li class="listitem"><p><code class="code">dfs.client.hedged.read.threshold.millis</code> - the number of milliseconds to
            wait before spawning a second read thread.</p></li></ul></div><div class="example"><a name="d9542e1158"></a><p class="title"><b>Example&nbsp;1.3.&nbsp;Hedged Reads Configuration Example</b></p><div class="example-contents"><pre class="screen">&lt;property&gt;
  &lt;name&gt;dfs.client.hedged.read.threadpool.size&lt;/name&gt;
  &lt;value&gt;20&lt;/value&gt;  &lt;!-- 20 threads --&gt;
&lt;/property&gt;
&lt;property&gt;
  &lt;name&gt;dfs.client.hedged.read.threshold.millis&lt;/name&gt;
  &lt;value&gt;10&lt;/value&gt;  &lt;!-- 10 milliseconds --&gt;
&lt;/property&gt;</pre></div></div><br class="example-break"><p>Use the following metrics to tune the settings for hedged reads on
        your cluster. See <a class="xref" href="#">???</a>  for more information.</p><div class="itemizedlist" title="Metrics for Hedged Reads"><p class="title"><b>Metrics for Hedged Reads</b></p><ul class="itemizedlist" type="disc"><li class="listitem"><p>hedgedReadOps - the number of times hedged read threads have been triggered. This
            could indicate that read requests are often slow, or that hedged reads are triggered too
            quickly.</p></li><li class="listitem"><p>hedgeReadOpsWin - the number of times the hedged read thread was faster than the
            original thread. This could indicate that a given RegionServer is having trouble
            servicing requests.</p></li></ul></div></div></div><div class="section" title="1.10.&nbsp;Deleting from HBase"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="perf.deleting"></a>1.10.&nbsp;Deleting from HBase</h2></div></div></div><div class="section" title="1.10.1.&nbsp;Using HBase Tables as Queues"><div class="titlepage"><div><div><h3 class="title"><a name="perf.deleting.queue"></a>1.10.1.&nbsp;Using HBase Tables as Queues</h3></div></div></div><p>HBase tables are sometimes used as queues. In this case, special care must be taken to
        regularly perform major compactions on tables used in this manner. As is documented in <a class="xref" href="#">???</a>, marking rows as deleted creates additional StoreFiles which then
        need to be processed on reads. Tombstones only get cleaned up with major compactions. </p><p>See also <a class="xref" href="#">???</a> and <a class="link" href="http://hbase.apache.org/apidocs/org/apache/hadoop/hbase/client/HBaseAdmin.html#majorCompact%28java.lang.String%29" target="_top">HBaseAdmin.majorCompact</a>.
      </p></div><div class="section" title="1.10.2.&nbsp;Delete RPC Behavior"><div class="titlepage"><div><div><h3 class="title"><a name="perf.deleting.rpc"></a>1.10.2.&nbsp;Delete RPC Behavior</h3></div></div></div><p>Be aware that <code class="code">htable.delete(Delete)</code> doesn't use the writeBuffer. It will
        execute an RegionServer RPC with each invocation. For a large number of deletes, consider
          <code class="code">htable.delete(List)</code>. </p><p>See <a class="link" href="http://hbase.apache.org/apidocs/org/apache/hadoop/hbase/client/HTable.html#delete%28org.apache.hadoop.hbase.client.Delete%29" target="_top">http://hbase.apache.org/apidocs/org/apache/hadoop/hbase/client/HTable.html#delete%28org.apache.hadoop.hbase.client.Delete%29</a>
      </p></div></div><div class="section" title="1.11.&nbsp;HDFS"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="perf.hdfs"></a>1.11.&nbsp;HDFS</h2></div></div></div><p>Because HBase runs on <a class="xref" href="#">???</a> it is important to understand how it works and how it affects HBase. </p><div class="section" title="1.11.1.&nbsp;Current Issues With Low-Latency Reads"><div class="titlepage"><div><div><h3 class="title"><a name="perf.hdfs.curr"></a>1.11.1.&nbsp;Current Issues With Low-Latency Reads</h3></div></div></div><p>The original use-case for HDFS was batch processing. As such, there low-latency reads
        were historically not a priority. With the increased adoption of Apache HBase this is
        changing, and several improvements are already in development. See the <a class="link" href="https://issues.apache.org/jira/browse/HDFS-1599" target="_top">Umbrella Jira Ticket for HDFS
          Improvements for HBase</a>. </p></div><div class="section" title="1.11.2.&nbsp;Leveraging local data"><div class="titlepage"><div><div><h3 class="title"><a name="perf.hdfs.configs.localread"></a>1.11.2.&nbsp;Leveraging local data</h3></div></div></div><p>Since Hadoop 1.0.0 (also 0.22.1, 0.23.1, CDH3u3 and HDP 1.0) via <a class="link" href="https://issues.apache.org/jira/browse/HDFS-2246" target="_top">HDFS-2246</a>, it is
        possible for the DFSClient to take a "short circuit" and read directly from the disk instead
        of going through the DataNode when the data is local. What this means for HBase is that the
        RegionServers can read directly off their machine's disks instead of having to open a socket
        to talk to the DataNode, the former being generally much faster. See JD's <a class="link" href="http://files.meetup.com/1350427/hug_ebay_jdcryans.pdf" target="_top">Performance
              Talk</a>. Also see <a class="link" href="http://search-hadoop.com/m/zV6dKrLCVh1" target="_top">HBase, mail # dev - read short
          circuit</a> thread for more discussion around short circuit reads. </p><p>To enable "short circuit" reads, it will depend on your version of Hadoop. The original
        shortcircuit read patch was much improved upon in Hadoop 2 in <a class="link" href="https://issues.apache.org/jira/browse/HDFS-347" target="_top">HDFS-347</a>. See <a class="link" href="http://blog.cloudera.com/blog/2013/08/how-improved-short-circuit-local-reads-bring-better-performance-and-security-to-hadoop/" target="_top">http://blog.cloudera.com/blog/2013/08/how-improved-short-circuit-local-reads-bring-better-performance-and-security-to-hadoop/</a>
        for details on the difference between the old and new implementations. See <a class="link" href="http://archive.cloudera.com/cdh4/cdh/4/hadoop/hadoop-project-dist/hadoop-hdfs/ShortCircuitLocalReads.html" target="_top">Hadoop
          shortcircuit reads configuration page</a> for how to enable the latter, better version
        of shortcircuit. For example, here is a minimal config. enabling short-circuit reads added
        to <code class="filename">hbase-site.xml</code>: </p><pre class="programlisting">&lt;property&gt;
  &lt;name&gt;dfs.client.read.shortcircuit&lt;/name&gt;
  &lt;value&gt;true&lt;/value&gt;
  &lt;description&gt;
    This configuration parameter turns on short-circuit local reads.
  &lt;/description&gt;
&lt;/property&gt;
&lt;property&gt;
  &lt;name&gt;dfs.domain.socket.path&lt;/name&gt;
  &lt;value&gt;/home/stack/sockets/short_circuit_read_socket_PORT&lt;/value&gt;
  &lt;description&gt;
    Optional.  This is a path to a UNIX domain socket that will be used for
    communication between the DataNode and local HDFS clients.
    If the string "_PORT" is present in this path, it will be replaced by the
    TCP port of the DataNode.
  &lt;/description&gt;
&lt;/property&gt;</pre><p>Be careful about permissions for the directory that hosts the shared domain socket;
        dfsclient will complain if open to other than the hbase user. </p><p>If you are running on an old Hadoop, one that is without <a class="link" href="https://issues.apache.org/jira/browse/HDFS-347" target="_top">HDFS-347</a> but that has
          <a class="link" href="https://issues.apache.org/jira/browse/HDFS-2246" target="_top">HDFS-2246</a>, you
        must set two configurations. First, the hdfs-site.xml needs to be amended. Set the property
          <code class="varname">dfs.block.local-path-access.user</code> to be the <span class="emphasis"><em>only</em></span>
        user that can use the shortcut. This has to be the user that started HBase. Then in
        hbase-site.xml, set <code class="varname">dfs.client.read.shortcircuit</code> to be
          <code class="varname">true</code>
      </p><p> Services -- at least the HBase RegionServers -- will need to be restarted in order to
        pick up the new configurations. </p><div class="note" title="dfs.client.read.shortcircuit.buffer.size" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a name="dfs.client.read.shortcircuit.buffer.size"></a>dfs.client.read.shortcircuit.buffer.size</h3><p>The default for this value is too high when running on a highly trafficed HBase. In
          HBase, if this value has not been set, we set it down from the default of 1M to 128k
          (Since HBase 0.98.0 and 0.96.1). See <a class="link" href="https://issues.apache.org/jira/browse/HBASE-8143" target="_top">HBASE-8143 HBase on Hadoop
            2 with local short circuit reads (ssr) causes OOM</a>). The Hadoop DFSClient in HBase
          will allocate a direct byte buffer of this size for <span class="emphasis"><em>each</em></span> block it has
          open; given HBase keeps its HDFS files open all the time, this can add up quickly.</p></div></div><div class="section" title="1.11.3.&nbsp;Performance Comparisons of HBase vs. HDFS"><div class="titlepage"><div><div><h3 class="title"><a name="perf.hdfs.comp"></a>1.11.3.&nbsp;Performance Comparisons of HBase vs. HDFS</h3></div></div></div><p>A fairly common question on the dist-list is why HBase isn't as performant as HDFS files
        in a batch context (e.g., as a MapReduce source or sink). The short answer is that HBase is
        doing a lot more than HDFS (e.g., reading the KeyValues, returning the most current row or
        specified timestamps, etc.), and as such HBase is 4-5 times slower than HDFS in this
        processing context. There is room for improvement and this gap will, over time, be reduced,
        but HDFS will always be faster in this use-case. </p></div></div><div class="section" title="1.12.&nbsp;Amazon EC2"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="perf.ec2"></a>1.12.&nbsp;Amazon EC2</h2></div></div></div><p>Performance questions are common on Amazon EC2 environments because it is a shared
      environment. You will not see the same throughput as a dedicated server. In terms of running
      tests on EC2, run them several times for the same reason (i.e., it's a shared environment and
      you don't know what else is happening on the server). </p><p>If you are running on EC2 and post performance questions on the dist-list, please state
      this fact up-front that because EC2 issues are practically a separate class of performance
      issues. </p></div><div class="section" title="1.13.&nbsp;Collocating HBase and MapReduce"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="perf.hbase.mr.cluster"></a>1.13.&nbsp;Collocating HBase and MapReduce</h2></div></div></div><p>It is often recommended to have different clusters for HBase and MapReduce. A better
      qualification of this is: don't collocate a HBase that serves live requests with a heavy MR
      workload. OLTP and OLAP-optimized systems have conflicting requirements and one will lose to
      the other, usually the former. For example, short latency-sensitive disk reads will have to
      wait in line behind longer reads that are trying to squeeze out as much throughput as
      possible. MR jobs that write to HBase will also generate flushes and compactions, which will
      in turn invalidate blocks in the <a class="xref" href="#">???</a>. </p><p>If you need to process the data from your live HBase cluster in MR, you can ship the
      deltas with <a class="xref" href="#">???</a> or use replication to get the new data in real time on the OLAP
      cluster. In the worst case, if you really need to collocate both, set MR to use less Map and
      Reduce slots than you'd normally configure, possibly just one. </p><p>When HBase is used for OLAP operations, it's preferable to set it up in a hardened way
      like configuring the ZooKeeper session timeout higher and giving more memory to the MemStores
      (the argument being that the Block Cache won't be used much since the workloads are usually
      long scans). </p></div><div class="section" title="1.14.&nbsp;Case Studies"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="perf.casestudy"></a>1.14.&nbsp;Case Studies</h2></div></div></div><p>For Performance and Troubleshooting Case Studies, see <a class="xref" href="#">???</a>. </p></div></div><div id="disqus_thread"></div><script type="text/javascript">
    /* * * DON'T EDIT BELOW THIS LINE * * */
    (function() {
        var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
        dsq.src = 'http://' + disqus_shortname + '.disqus.com/embed.js';
        (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
    })();
</script><noscript>Please enable JavaScript to view the <a href="http://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript><a href="http://disqus.com" class="dsq-brlink">comments powered by <span class="logo-disqus">Disqus</span></a></body></html>