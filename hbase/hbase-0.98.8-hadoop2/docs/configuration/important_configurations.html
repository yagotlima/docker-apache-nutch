<html><head>
      <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
   <title>1.6.&nbsp;The Important Configurations</title><link rel="stylesheet" type="text/css" href="../css/freebsd_docbook.css"><meta name="generator" content="DocBook XSL-NS Stylesheets V1.76.1"><link rel="home" href="configuration.html" title="Chapter&nbsp;1.&nbsp;Apache HBase Configuration"><link rel="up" href="configuration.html" title="Chapter&nbsp;1.&nbsp;Apache HBase Configuration"><link rel="prev" href="example_config.html" title="1.5.&nbsp;Example Configurations"><link rel="next" href="dyn_config.html" title="1.7.&nbsp;Dynamic Configuration"></head><body bgcolor="white" text="black" link="#0000FF" vlink="#840084" alink="#0000FF"><div class="navheader"><table width="100%" summary="Navigation header"><tr><th colspan="3" align="center">1.6.&nbsp;The Important Configurations</th></tr><tr><td width="20%" align="left"><a accesskey="p" href="example_config.html">Prev</a>&nbsp;</td><th width="60%" align="center">&nbsp;</th><td width="20%" align="right">&nbsp;<a accesskey="n" href="dyn_config.html">Next</a></td></tr></table><hr></div><script type="text/javascript">
    var disqus_shortname = 'hbase'; // required: replace example with your forum shortname
    var disqus_url = 'http://hbase.apache.org/book/important_configurations.html';
    </script><div class="section" title="1.6.&nbsp;The Important Configurations"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="important_configurations"></a>1.6.&nbsp;The Important Configurations</h2></div></div></div><p>Below we list what the <span class="emphasis"><em>important</em></span> Configurations. We've divided this
      section into required configuration and worth-a-look recommended configs. </p><div class="section" title="1.6.1.&nbsp;Required Configurations"><div class="titlepage"><div><div><h3 class="title"><a name="required_configuration"></a>1.6.1.&nbsp;Required Configurations</h3></div></div></div><p>Review the <a class="xref" href="configuration.html#os" title="Operating System Utilities">Operating System Utilities</a> and <a class="xref" href="configuration.html#hadoop" title="1.1.1.&nbsp;Hadoop">Section&nbsp;1.1.1, &#8220;Hadoop&#8221;</a> sections. </p><div class="section" title="1.6.1.1.&nbsp;Big Cluster Configurations"><div class="titlepage"><div><div><h4 class="title"><a name="big.cluster.config"></a>1.6.1.1.&nbsp;Big Cluster Configurations</h4></div></div></div><p>If a cluster with a lot of regions, it is possible if an eager beaver regionserver
          checks in soon after master start while all the rest in the cluster are laggardly, this
          first server to checkin will be assigned all regions. If lots of regions, this first
          server could buckle under the load. To prevent the above scenario happening up the
            <code class="varname">hbase.master.wait.on.regionservers.mintostart</code> from its default value
          of 1. See <a class="link" href="https://issues.apache.org/jira/browse/HBASE-6389" target="_top">HBASE-6389 Modify the
            conditions to ensure that Master waits for sufficient number of Region Servers before
            starting region assignments</a> for more detail. </p></div><div class="section" title="1.6.1.2.&nbsp;If a backup Master, making primary Master fail fast"><div class="titlepage"><div><div><h4 class="title"><a name="backup.master.fail.fast"></a>1.6.1.2.&nbsp;If a backup Master, making primary Master fail fast</h4></div></div></div><p>If the primary Master loses its connection with ZooKeeper, it will fall into a loop
          where it keeps trying to reconnect. Disable this functionality if you are running more
          than one Master: i.e. a backup Master. Failing to do so, the dying Master may continue to
          receive RPCs though another Master has assumed the role of primary. See the configuration <a class="xref" href="config.files.html#fail.fast.expired.active.master" title="fail.fast.expired.active.master"><code class="varname">fail.fast.expired.active.master</code></a>. </p></div></div><div class="section" title="1.6.2.&nbsp;Recommended Configurations"><div class="titlepage"><div><div><h3 class="title"><a name="recommended_configurations"></a>1.6.2.&nbsp;Recommended Configurations</h3></div></div></div><div class="section" title="1.6.2.1.&nbsp;ZooKeeper Configuration"><div class="titlepage"><div><div><h4 class="title"><a name="recommended_configurations.zk"></a>1.6.2.1.&nbsp;ZooKeeper Configuration</h4></div></div></div><div class="section" title="1.6.2.1.1.&nbsp;zookeeper.session.timeout"><div class="titlepage"><div><div><h5 class="title"><a name="sect.zookeeper.session.timeout"></a>1.6.2.1.1.&nbsp;<code class="varname">zookeeper.session.timeout</code></h5></div></div></div><p>The default timeout is three minutes (specified in milliseconds). This means that if
            a server crashes, it will be three minutes before the Master notices the crash and
            starts recovery. You might like to tune the timeout down to a minute or even less so the
            Master notices failures the sooner. Before changing this value, be sure you have your
            JVM garbage collection configuration under control otherwise, a long garbage collection
            that lasts beyond the ZooKeeper session timeout will take out your RegionServer (You
            might be fine with this -- you probably want recovery to start on the server if a
            RegionServer has been in GC for a long period of time).</p><p>To change this configuration, edit <code class="filename">hbase-site.xml</code>, copy the
            changed file around the cluster and restart.</p><p>We set this value high to save our having to field noob questions up on the mailing
            lists asking why a RegionServer went down during a massive import. The usual cause is
            that their JVM is untuned and they are running into long GC pauses. Our thinking is that
            while users are getting familiar with HBase, we'd save them having to know all of its
            intricacies. Later when they've built some confidence, then they can play with
            configuration such as this. </p></div><div class="section" title="1.6.2.1.2.&nbsp;Number of ZooKeeper Instances"><div class="titlepage"><div><div><h5 class="title"><a name="zookeeper.instances"></a>1.6.2.1.2.&nbsp;Number of ZooKeeper Instances</h5></div></div></div><p>See <a class="xref" href="">???</a>. </p></div></div><div class="section" title="1.6.2.2.&nbsp;HDFS Configurations"><div class="titlepage"><div><div><h4 class="title"><a name="recommended.configurations.hdfs"></a>1.6.2.2.&nbsp;HDFS Configurations</h4></div></div></div><div class="section" title="1.6.2.2.1.&nbsp;dfs.datanode.failed.volumes.tolerated"><div class="titlepage"><div><div><h5 class="title"><a name="dfs.datanode.failed.volumes.tolerated"></a>1.6.2.2.1.&nbsp;dfs.datanode.failed.volumes.tolerated</h5></div></div></div><p>This is the "...number of volumes that are allowed to fail before a datanode stops
            offering service. By default any volume failure will cause a datanode to shutdown" from
            the <code class="filename">hdfs-default.xml</code> description. If you have &gt; three or four
            disks, you might want to set this to 1 or if you have many disks, two or more. </p></div></div><div class="section" title="1.6.2.3.&nbsp;hbase.regionserver.handler.count"><div class="titlepage"><div><div><h4 class="title"><a name="hbase.regionserver.handler.count-description"></a>1.6.2.3.&nbsp;<code class="varname">hbase.regionserver.handler.count</code></h4></div></div></div><p> This setting defines the number of threads that are kept open to answer incoming
          requests to user tables. The rule of thumb is to keep this number low when the payload per
          request approaches the MB (big puts, scans using a large cache) and high when the payload
          is small (gets, small puts, ICVs, deletes). The total size of the queries in progress is
          limited by the setting "hbase.ipc.server.max.callqueue.size". </p><p> It is safe to set that number to the maximum number of incoming clients if their
          payload is small, the typical example being a cluster that serves a website since puts
          aren't typically buffered and most of the operations are gets. </p><p> The reason why it is dangerous to keep this setting high is that the aggregate size
          of all the puts that are currently happening in a region server may impose too much
          pressure on its memory, or even trigger an OutOfMemoryError. A region server running on
          low memory will trigger its JVM's garbage collector to run more frequently up to a point
          where GC pauses become noticeable (the reason being that all the memory used to keep all
          the requests' payloads cannot be trashed, no matter how hard the garbage collector tries).
          After some time, the overall cluster throughput is affected since every request that hits
          that region server will take longer, which exacerbates the problem even more. </p><p>You can get a sense of whether you have too little or too many handlers by <a class="xref" href="">???</a> on an individual RegionServer then tailing its logs (Queued
          requests consume memory). </p></div><div class="section" title="1.6.2.4.&nbsp;Configuration for large memory machines"><div class="titlepage"><div><div><h4 class="title"><a name="big_memory"></a>1.6.2.4.&nbsp;Configuration for large memory machines</h4></div></div></div><p> HBase ships with a reasonable, conservative configuration that will work on nearly
          all machine types that people might want to test with. If you have larger machines --
          HBase has 8G and larger heap -- you might the following configuration options helpful.
          TODO. </p></div><div class="section" title="1.6.2.5.&nbsp;Compression"><div class="titlepage"><div><div><h4 class="title"><a name="config.compression"></a>1.6.2.5.&nbsp;Compression</h4></div></div></div><p>You should consider enabling ColumnFamily compression. There are several options that
          are near-frictionless and in most all cases boost performance by reducing the size of
          StoreFiles and thus reducing I/O. </p><p>See <a class="xref" href="">???</a> for more information.</p></div><div class="section" title="1.6.2.6.&nbsp;Configuring the size and number of WAL files"><div class="titlepage"><div><div><h4 class="title"><a name="config.wals"></a>1.6.2.6.&nbsp;Configuring the size and number of WAL files</h4></div></div></div><p>HBase uses <a class="xref" href="">???</a> to recover the memstore data that has not been flushed to disk in case
          of an RS failure. These WAL files should be configured to be slightly smaller than HDFS
          block (by default, HDFS block is 64Mb and WAL file is ~60Mb).</p><p>HBase also has a limit on number of WAL files, designed to ensure there's never too
          much data that needs to be replayed during recovery. This limit needs to be set according
          to memstore configuration, so that all the necessary data would fit. It is recommended to
          allocated enough WAL files to store at least that much data (when all memstores are close
          to full). For example, with 16Gb RS heap, default memstore settings (0.4), and default WAL
          file size (~60Mb), 16Gb*0.4/60, the starting point for WAL file count is ~109. However, as
          all memstores are not expected to be full all the time, less WAL files can be
          allocated.</p></div><div class="section" title="1.6.2.7.&nbsp;Managed Splitting"><div class="titlepage"><div><div><h4 class="title"><a name="disable.splitting"></a>1.6.2.7.&nbsp;Managed Splitting</h4></div></div></div><p>HBase generally handles splitting your regions, based upon the settings in your
            <code class="filename">hbase-default.xml</code> and <code class="filename">hbase-site.xml</code>
          configuration files. Important settings include
            <code class="varname">hbase.regionserver.region.split.policy</code>,
            <code class="varname">hbase.hregion.max.filesize</code>,
            <code class="varname">hbase.regionserver.regionSplitLimit</code>. A simplistic view of splitting
          is that when a region grows to <code class="varname">hbase.hregion.max.filesize</code>, it is split.
          For most use patterns, most of the time, you should use automatic splitting. See <a class="xref" href="">???</a> for more information about manual region
          splitting.</p><p>Instead of allowing HBase to split your regions automatically, you can choose to
          manage the splitting yourself. This feature was added in HBase 0.90.0. Manually managing
          splits works if you know your keyspace well, otherwise let HBase figure where to split for you.
          Manual splitting can mitigate region creation and movement under load. It also makes it so
          region boundaries are known and invariant (if you disable region splitting). If you use manual
          splits, it is easier doing staggered, time-based major compactions spread out your network IO
          load.</p><p title="Disable Automatic Splitting"><b>Disable Automatic Splitting.&nbsp;</b>To disable automatic splitting, set <code class="varname">hbase.hregion.max.filesize</code> to
            a very large value, such as <code class="literal">100 GB</code> It is not recommended to set it to
            its absolute maximum value of <code class="literal">Long.MAX_VALUE</code>.</p><div class="note" title="Automatic Splitting Is Recommended" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title">Automatic Splitting Is Recommended</h3><p>If you disable automatic splits to diagnose a problem or during a period of fast
            data growth, it is recommended to re-enable them when your situation becomes more
            stable. The potential benefits of managing region splits yourself are not
            undisputed.</p></div><p title="Determine the Optimal Number of Pre-Split Regions"><b>Determine the Optimal Number of Pre-Split Regions.&nbsp;</b>The optimal number of pre-split regions depends on your application and environment.
            A good rule of thumb is to start with 10 pre-split regions per server and watch as data
            grows over time. It is better to err on the side of too few regions and perform rolling
            splits later. The optimal number of regions depends upon the largest StoreFile in your
            region. The size of the largest StoreFile will increase with time if the amount of data
            grows. The goal is for the largest region to be just large enough that the compaction
            selection algorithm only compacts it during a timed major compaction. Otherwise, the
            cluster can be prone to compaction storms where a large number of regions under
            compaction at the same time. It is important to understand that the data growth causes
            compaction storms, and not the manual split decision.</p><p>If the regions are split into too many large regions, you can increase the major
          compaction interval by configuring <code class="varname">HConstants.MAJOR_COMPACTION_PERIOD</code>.
          HBase 0.90 introduced <code class="classname">org.apache.hadoop.hbase.util.RegionSplitter</code>,
          which provides a network-IO-safe rolling split of all regions.</p></div><div class="section" title="1.6.2.8.&nbsp;Managed Compactions"><div class="titlepage"><div><div><h4 class="title"><a name="managed.compactions"></a>1.6.2.8.&nbsp;Managed Compactions</h4></div></div></div><p>By default, major compactions are scheduled to run once in a 7-day period. Prior to HBase 0.96.x, major
          compactions were scheduled to happen once per day by default.</p><p>If you need to control exactly when and how often major compaction runs, you can
          disable managed major compactions. See the entry for
            <code class="varname">hbase.hregion.majorcompaction</code> in the <a class="xref" href="">???</a> table for details.</p><div class="warning" title="Do Not Disable Major Compactions" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title">Do Not Disable Major Compactions</h3><p>Major compactions are absolutely necessary for StoreFile clean-up. Do not disable
            them altogether. You can run major compactions manually via the HBase shell or via the <a class="link" href="http://hbase.apache.org/apidocs/org/apache/hadoop/hbase/client/HBaseAdmin.html#majorCompact%28java.lang.String%29" target="_top">HBaseAdmin
              API</a>.</p></div><p>For more information about compactions and the compaction file selection process, see <a class="xref" href="">???</a></p></div><div class="section" title="1.6.2.9.&nbsp;Speculative Execution"><div class="titlepage"><div><div><h4 class="title"><a name="spec.ex"></a>1.6.2.9.&nbsp;Speculative Execution</h4></div></div></div><p>Speculative Execution of MapReduce tasks is on by default, and for HBase clusters it
          is generally advised to turn off Speculative Execution at a system-level unless you need
          it for a specific case, where it can be configured per-job. Set the properties
            <code class="varname">mapreduce.map.speculative</code> and
            <code class="varname">mapreduce.reduce.speculative</code> to false. </p></div></div><div class="section" title="1.6.3.&nbsp;Other Configurations"><div class="titlepage"><div><div><h3 class="title"><a name="other_configuration"></a>1.6.3.&nbsp;Other Configurations</h3></div></div></div><div class="section" title="1.6.3.1.&nbsp;Balancer"><div class="titlepage"><div><div><h4 class="title"><a name="balancer_config"></a>1.6.3.1.&nbsp;Balancer</h4></div></div></div><p>The balancer is a periodic operation which is run on the master to redistribute regions on the cluster.  It is configured via
           <code class="varname">hbase.balancer.period</code> and defaults to 300000 (5 minutes). </p><p>See <a class="xref" href="">???</a> for more information on the LoadBalancer.
           </p></div><div class="section" title="1.6.3.2.&nbsp;Disabling Blockcache"><div class="titlepage"><div><div><h4 class="title"><a name="disabling.blockcache"></a>1.6.3.2.&nbsp;Disabling Blockcache</h4></div></div></div><p>Do not turn off block cache (You'd do it by setting <code class="varname">hbase.block.cache.size</code> to zero).
          Currently we do not do well if you do this because the regionserver will spend all its time loading hfile
          indices over and over again.  If your working set it such that block cache does you no good, at least
          size the block cache such that hfile indices will stay up in the cache (you can get a rough idea
          on the size you need by surveying regionserver UIs; you'll see index block size accounted near the
          top of the webpage).</p></div><div class="section" title="1.6.3.3.&nbsp;Nagle's or the small package problem"><div class="titlepage"><div><div><h4 class="title"><a name="nagles"></a>1.6.3.3.&nbsp;<a class="link" href="http://en.wikipedia.org/wiki/Nagle's_algorithm" target="_top">Nagle's</a> or the small package problem</h4></div></div></div><p>If a big 40ms or so occasional delay is seen in operations against HBase,
      try the Nagles' setting.  For example, see the user mailing list thread,
      <a class="link" href="http://search-hadoop.com/m/pduLg2fydtE/Inconsistent+scan+performance+with+caching+set+&amp;subj=Re+Inconsistent+scan+performance+with+caching+set+to+1" target="_top">Inconsistent scan performance with caching set to 1</a>
      and the issue cited therein where setting notcpdelay improved scan speeds.  You might also
      see the graphs on the tail of <a class="link" href="https://issues.apache.org/jira/browse/HBASE-7008" target="_top">HBASE-7008 Set scanner caching to a better default</a>
      where our Lars Hofhansl tries various data sizes w/ Nagle's on and off measuring the effect.</p></div><div class="section" title="1.6.3.4.&nbsp;Better Mean Time to Recover (MTTR)"><div class="titlepage"><div><div><h4 class="title"><a name="mttr"></a>1.6.3.4.&nbsp;Better Mean Time to Recover (MTTR)</h4></div></div></div><p>This section is about configurations that will make servers come back faster after a fail.
          See the Deveraj Das an Nicolas Liochon blog post
          <a class="link" href="http://hortonworks.com/blog/introduction-to-hbase-mean-time-to-recover-mttr/" target="_top">Introduction to HBase Mean Time to Recover (MTTR)</a>
          for a brief introduction.</p><p>The issue <a class="link" href="https://issues.apache.org/jira/browse/HBASE-8389" target="_top">HBASE-8354 forces Namenode into loop with lease recovery requests</a>
          is messy but has a bunch of good discussion toward the end on low timeouts and how to effect faster recovery including citation of fixes
          added to HDFS.  Read the Varun Sharma comments.  The below suggested configurations are Varun's suggestions distilled and tested.  Make sure you are
          running on a late-version HDFS so you have the fixes he refers too and himself adds to HDFS that help HBase MTTR
          (e.g. HDFS-3703, HDFS-3712, and HDFS-4791 -- hadoop 2 for sure has them and late hadoop 1 has some).
          Set the following in the RegionServer.</p><pre class="programlisting">
&lt;property&gt;
&lt;property&gt;
    &lt;name&gt;hbase.lease.recovery.dfs.timeout&lt;/name&gt;
    &lt;value&gt;23000&lt;/value&gt;
    &lt;description&gt;How much time we allow elapse between calls to recover lease.
    Should be larger than the dfs timeout.&lt;/description&gt;
&lt;/property&gt;
&lt;property&gt;
    &lt;name&gt;dfs.client.socket-timeout&lt;/name&gt;
    &lt;value&gt;10000&lt;/value&gt;
    &lt;description&gt;Down the DFS timeout from 60 to 10 seconds.&lt;/description&gt;
&lt;/property&gt;
</pre><p>And on the namenode/datanode side, set the following to enable 'staleness' introduced
          in HDFS-3703, HDFS-3912. </p><pre class="programlisting">
&lt;property&gt;
    &lt;name&gt;dfs.client.socket-timeout&lt;/name&gt;
    &lt;value&gt;10000&lt;/value&gt;
    &lt;description&gt;Down the DFS timeout from 60 to 10 seconds.&lt;/description&gt;
&lt;/property&gt;
&lt;property&gt;
    &lt;name&gt;dfs.datanode.socket.write.timeout&lt;/name&gt;
    &lt;value&gt;10000&lt;/value&gt;
    &lt;description&gt;Down the DFS timeout from 8 * 60 to 10 seconds.&lt;/description&gt;
&lt;/property&gt;
&lt;property&gt;
    &lt;name&gt;ipc.client.connect.timeout&lt;/name&gt;
    &lt;value&gt;3000&lt;/value&gt;
    &lt;description&gt;Down from 60 seconds to 3.&lt;/description&gt;
&lt;/property&gt;
&lt;property&gt;
    &lt;name&gt;ipc.client.connect.max.retries.on.timeouts&lt;/name&gt;
    &lt;value&gt;2&lt;/value&gt;
    &lt;description&gt;Down from 45 seconds to 3 (2 == 3 retries).&lt;/description&gt;
&lt;/property&gt;
&lt;property&gt;
    &lt;name&gt;dfs.namenode.avoid.read.stale.datanode&lt;/name&gt;
    &lt;value&gt;true&lt;/value&gt;
    &lt;description&gt;Enable stale state in hdfs&lt;/description&gt;
&lt;/property&gt;
&lt;property&gt;
    &lt;name&gt;dfs.namenode.stale.datanode.interval&lt;/name&gt;
    &lt;value&gt;20000&lt;/value&gt;
    &lt;description&gt;Down from default 30 seconds&lt;/description&gt;
&lt;/property&gt;
&lt;property&gt;
    &lt;name&gt;dfs.namenode.avoid.write.stale.datanode&lt;/name&gt;
    &lt;value&gt;true&lt;/value&gt;
    &lt;description&gt;Enable stale state in hdfs&lt;/description&gt;
&lt;/property&gt;
</pre></div><div class="section" title="1.6.3.5.&nbsp;JMX"><div class="titlepage"><div><div><h4 class="title"><a name="JMX_config"></a>1.6.3.5.&nbsp;JMX</h4></div></div></div><p>JMX(Java Management Extensions) provides built-in instrumentation that enables you
          to monitor and manage the Java VM. To enable monitoring and management from remote
          systems, you need to set system property com.sun.management.jmxremote.port(the port
          number through which you want to enable JMX RMI connections) when you start the Java VM.
          See <a class="link" href="http://docs.oracle.com/javase/6/docs/technotes/guides/management/agent.html" target="_top">
          official document</a> for more information. Historically, besides above port mentioned,
          JMX opens 2 additional random TCP listening ports, which could lead to port conflict
          problem.(See <a class="link" href="https://issues.apache.org/jira/browse/HBASE-10289" target="_top">HBASE-10289</a>
          for details)
        </p><p>As an alternative, You can use the coprocessor-based JMX implementation provided
          by HBase. To enable it in 0.99 or above, add below property in
          <code class="filename">hbase-site.xml</code>:
        </p><pre class="programlisting">
&lt;property&gt;
    &lt;name&gt;hbase.coprocessor.regionserver.classes&lt;/name&gt;
    &lt;value&gt;org.apache.hadoop.hbase.JMXListener&lt;/value&gt;
&lt;/property&gt;
</pre><p>
          NOTE: DO NOT set com.sun.management.jmxremote.port for Java VM at the same time.
        </p><p>Currently it supports Master and RegionServer Java VM. The reason why you only
          configure coprocessor for 'regionserver' is that, starting from HBase 0.99,
          a Master IS also a RegionServer. (See <a class="link" href="https://issues.apache.org/jira/browse/HBASE-10569" target="_top">HBASE-10569</a>
          for more information.)
          By default, the JMX listens on TCP port 10102, you can further configure the port
          using below properties:

        </p><pre class="programlisting">
&lt;property&gt;
    &lt;name&gt;regionserver.rmi.registry.port&lt;/name&gt;
    &lt;value&gt;61130&lt;/value&gt;
&lt;/property&gt;
&lt;property&gt;
    &lt;name&gt;regionserver.rmi.connector.port&lt;/name&gt;
    &lt;value&gt;61140&lt;/value&gt;
&lt;/property&gt;
</pre><p>
          The registry port can be shared with connector port in most cases, so you only
          need to configure regionserver.rmi.registry.port. However if you want to use SSL
          communication, the 2 ports must be configured to different values.
        </p><p>By default the password authentication and SSL communication is disabled.
          To enable password authentication, you need to update <code class="filename">hbase-env.sh</code>
          like below:
      </p><pre class="screen">
export HBASE_JMX_BASE="-Dcom.sun.management.jmxremote.authenticate=true                  \
                       -Dcom.sun.management.jmxremote.password.file=your_password_file   \
                       -Dcom.sun.management.jmxremote.access.file=your_access_file"

export HBASE_MASTER_OPTS="$HBASE_MASTER_OPTS $HBASE_JMX_BASE "
export HBASE_REGIONSERVER_OPTS="$HBASE_REGIONSERVER_OPTS $HBASE_JMX_BASE "
      </pre><p>
          See example password/access file under $JRE_HOME/lib/management.
        </p><p>To enable SSL communication with password authentication, follow below steps:
      </p><pre class="screen">
#1. generate a key pair, stored in myKeyStore
keytool -genkey -alias jconsole -keystore myKeyStore

#2. export it to file jconsole.cert
keytool -export -alias jconsole -keystore myKeyStore -file jconsole.cert

#3. copy jconsole.cert to jconsole client machine, import it to jconsoleKeyStore
keytool -import -alias jconsole -keystore jconsoleKeyStore -file jconsole.cert
      </pre><p>
          And then update <code class="filename">hbase-env.sh</code> like below:
      </p><pre class="screen">
export HBASE_JMX_BASE="-Dcom.sun.management.jmxremote.ssl=true                         \
                       -Djavax.net.ssl.keyStore=/home/tianq/myKeyStore                 \
                       -Djavax.net.ssl.keyStorePassword=your_password_in_step_1       \
                       -Dcom.sun.management.jmxremote.authenticate=true                \
                       -Dcom.sun.management.jmxremote.password.file=your_password file \
                       -Dcom.sun.management.jmxremote.access.file=your_access_file"

export HBASE_MASTER_OPTS="$HBASE_MASTER_OPTS $HBASE_JMX_BASE "
export HBASE_REGIONSERVER_OPTS="$HBASE_REGIONSERVER_OPTS $HBASE_JMX_BASE "
      </pre><p>

          Finally start jconsole on client using the key store:
      </p><pre class="screen">
jconsole -J-Djavax.net.ssl.trustStore=/home/tianq/jconsoleKeyStore
      </pre><p>
        </p><p>NOTE: for HBase 0.98, To enable the HBase JMX implementation on Master, you also
          need to add below property in <code class="filename">hbase-site.xml</code>:
        </p><pre class="programlisting">
&lt;property&gt;
    &lt;name&gt;hbase.coprocessor.master.classes&lt;/name&gt;
    &lt;value&gt;org.apache.hadoop.hbase.JMXListener&lt;/value&gt;
&lt;/property&gt;
</pre><p>
          The corresponding properties for port configuration are master.rmi.registry.port
          (by default 10101) and master.rmi.connector.port(by default the same as registry.port)
        </p></div></div></div><div id="disqus_thread"></div><script type="text/javascript">
    /* * * DON'T EDIT BELOW THIS LINE * * */
    (function() {
        var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
        dsq.src = 'http://' + disqus_shortname + '.disqus.com/embed.js';
        (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
    })();
</script><noscript>Please enable JavaScript to view the <a href="http://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript><a href="http://disqus.com" class="dsq-brlink">comments powered by <span class="logo-disqus">Disqus</span></a><div class="navfooter"><hr><table width="100%" summary="Navigation footer"><tr><td width="40%" align="left"><a accesskey="p" href="example_config.html">Prev</a>&nbsp;</td><td width="20%" align="center">&nbsp;</td><td width="40%" align="right">&nbsp;<a accesskey="n" href="dyn_config.html">Next</a></td></tr><tr><td width="40%" align="left" valign="top">1.5.&nbsp;Example Configurations&nbsp;</td><td width="20%" align="center"><a accesskey="h" href="configuration.html">Home</a></td><td width="40%" align="right" valign="top">&nbsp;1.7.&nbsp;Dynamic Configuration</td></tr></table></div></body></html>